X nd y are loaded for temporal feature extraction
X shape is (76950, 10000)
y shape is (76950,)
  0%|          | 0/76950 [00:00<?, ?it/s]  0%|          | 90/76950 [00:00<01:26, 884.13it/s]  0%|          | 189/76950 [00:00<01:21, 941.76it/s]  0%|          | 295/76950 [00:00<01:17, 994.63it/s]  1%|          | 415/76950 [00:00<01:11, 1071.89it/s]  1%|          | 523/76950 [00:00<01:12, 1052.09it/s]  1%|          | 629/76950 [00:00<01:13, 1037.58it/s]  1%|          | 733/76950 [00:00<01:16, 990.99it/s]   1%|          | 833/76950 [00:00<01:16, 990.05it/s]  1%|          | 955/76950 [00:00<01:11, 1059.24it/s]  1%|▏         | 1062/76950 [00:01<01:12, 1046.53it/s]  2%|▏         | 1170/76950 [00:01<01:11, 1055.44it/s]  2%|▏         | 1276/76950 [00:01<01:13, 1032.14it/s]  2%|▏         | 1394/76950 [00:01<01:10, 1073.73it/s]  2%|▏         | 1504/76950 [00:01<01:09, 1080.06it/s]  2%|▏         | 1613/76950 [00:01<01:11, 1058.00it/s]  2%|▏         | 1720/76950 [00:01<01:11, 1054.69it/s]  2%|▏         | 1830/76950 [00:01<01:10, 1064.11it/s]  3%|▎         | 1937/76950 [00:01<01:12, 1036.38it/s]  3%|▎         | 2041/76950 [00:01<01:12, 1028.13it/s]  3%|▎         | 2150/76950 [00:02<01:11, 1043.51it/s]  3%|▎         | 2266/76950 [00:02<01:09, 1075.95it/s]  3%|▎         | 2377/76950 [00:02<01:08, 1082.47it/s]  3%|▎         | 2486/76950 [00:02<01:14, 1005.29it/s]  3%|▎         | 2589/76950 [00:02<01:13, 1011.76it/s]  4%|▎         | 2699/76950 [00:02<01:11, 1036.21it/s]  4%|▎         | 2810/76950 [00:02<01:10, 1055.60it/s]  4%|▍         | 2931/76950 [00:02<01:07, 1099.97it/s]  4%|▍         | 3042/76950 [00:02<01:09, 1062.04it/s]  4%|▍         | 3149/76950 [00:03<01:09, 1059.46it/s]  4%|▍         | 3256/76950 [00:03<01:10, 1045.05it/s]  4%|▍         | 3366/76950 [00:03<01:09, 1060.09it/s]  5%|▍         | 3473/76950 [00:03<01:12, 1012.94it/s]  5%|▍         | 3575/76950 [00:03<01:15, 977.69it/s]   5%|▍         | 3674/76950 [00:03<01:15, 973.78it/s]  5%|▍         | 3782/76950 [00:03<01:13, 1002.01it/s]  5%|▌         | 3883/76950 [00:03<01:14, 982.67it/s]   5%|▌         | 4008/76950 [00:03<01:09, 1054.23it/s]  5%|▌         | 4114/76950 [00:03<01:09, 1042.90it/s]  5%|▌         | 4226/76950 [00:04<01:08, 1061.48it/s]  6%|▌         | 4334/76950 [00:04<01:08, 1064.85it/s]  6%|▌         | 4441/76950 [00:04<01:08, 1056.94it/s]  6%|▌         | 4566/76950 [00:04<01:05, 1112.63it/s]  6%|▌         | 4678/76950 [00:04<01:07, 1077.01it/s]  6%|▌         | 4787/76950 [00:04<01:07, 1074.28it/s]  6%|▋         | 4897/76950 [00:04<01:06, 1077.64it/s]  7%|▋         | 5007/76950 [00:04<01:06, 1082.86it/s]  7%|▋         | 5116/76950 [00:04<01:06, 1079.95it/s]  7%|▋         | 5225/76950 [00:05<01:08, 1042.72it/s]  7%|▋         | 5330/76950 [00:05<01:12, 982.24it/s]   7%|▋         | 5435/76950 [00:05<01:11, 1000.27it/s]  7%|▋         | 5536/76950 [00:05<01:14, 957.17it/s]   7%|▋         | 5646/76950 [00:05<01:11, 996.58it/s]  7%|▋         | 5763/76950 [00:05<01:08, 1033.34it/s]  8%|▊         | 5867/76950 [00:05<01:10, 1003.80it/s]  8%|▊         | 5989/76950 [00:05<01:06, 1064.20it/s]  8%|▊         | 6097/76950 [00:05<01:07, 1054.19it/s]  8%|▊         | 6203/76950 [00:05<01:07, 1050.81it/s]  8%|▊         | 6309/76950 [00:06<01:09, 1021.78it/s]  8%|▊         | 6417/76950 [00:06<01:07, 1037.37it/s]  8%|▊         | 6538/76950 [00:06<01:04, 1086.84it/s]  9%|▊         | 6661/76950 [00:06<01:02, 1127.02it/s]  9%|▉         | 6782/76950 [00:06<01:01, 1149.06it/s]  9%|▉         | 6898/76950 [00:06<01:04, 1080.54it/s]  9%|▉         | 7008/76950 [00:06<01:07, 1032.06it/s]  9%|▉         | 7115/76950 [00:06<01:07, 1041.54it/s]  9%|▉         | 7226/76950 [00:06<01:06, 1054.05it/s] 10%|▉         | 7332/76950 [00:07<01:10, 991.88it/s]  10%|▉         | 7437/76950 [00:07<01:08, 1007.51it/s] 10%|▉         | 7551/76950 [00:07<01:06, 1042.23it/s] 10%|▉         | 7660/76950 [00:07<01:05, 1054.10it/s] 10%|█         | 7766/76950 [00:07<01:06, 1037.78it/s] 10%|█         | 7874/76950 [00:07<01:05, 1048.84it/s] 10%|█         | 7980/76950 [00:07<01:06, 1042.21it/s] 11%|█         | 8087/76950 [00:07<01:05, 1046.42it/s] 11%|█         | 8193/76950 [00:07<01:05, 1043.52it/s] 11%|█         | 8324/76950 [00:07<01:01, 1118.81it/s] 11%|█         | 8437/76950 [00:08<01:05, 1047.90it/s] 11%|█         | 8543/76950 [00:08<01:05, 1037.04it/s] 11%|█         | 8648/76950 [00:08<01:07, 1012.23it/s] 11%|█▏        | 8756/76950 [00:08<01:06, 1031.27it/s] 12%|█▏        | 8872/76950 [00:08<01:03, 1065.03it/s] 12%|█▏        | 8983/76950 [00:08<01:03, 1074.82it/s] 12%|█▏        | 9091/76950 [00:08<01:05, 1041.03it/s] 12%|█▏        | 9199/76950 [00:08<01:04, 1050.41it/s] 12%|█▏        | 9313/76950 [00:08<01:02, 1073.94it/s] 12%|█▏        | 9425/76950 [00:09<01:02, 1086.03it/s] 12%|█▏        | 9534/76950 [00:09<01:07, 992.59it/s]  13%|█▎        | 9649/76950 [00:09<01:05, 1025.28it/s] 13%|█▎        | 9762/76950 [00:09<01:03, 1054.56it/s] 13%|█▎        | 9882/76950 [00:09<01:01, 1090.43it/s] 13%|█▎        | 9992/76950 [00:09<01:04, 1044.71it/s] 13%|█▎        | 10098/76950 [00:09<01:03, 1048.40it/s] 13%|█▎        | 10204/76950 [00:09<01:04, 1035.66it/s] 13%|█▎        | 10320/76950 [00:09<01:02, 1070.35it/s] 14%|█▎        | 10428/76950 [00:09<01:03, 1055.26it/s] 14%|█▎        | 10534/76950 [00:10<01:04, 1022.90it/s] 14%|█▍        | 10637/76950 [00:10<01:06, 997.68it/s]  14%|█▍        | 10750/76950 [00:10<01:04, 1032.73it/s] 14%|█▍        | 10854/76950 [00:10<01:03, 1034.35it/s] 14%|█▍        | 10958/76950 [00:10<01:04, 1023.62it/s] 14%|█▍        | 11061/76950 [00:10<01:05, 1011.93it/s] 15%|█▍        | 11171/76950 [00:10<01:03, 1035.21it/s] 15%|█▍        | 11275/76950 [00:10<01:04, 1020.70it/s] 15%|█▍        | 11385/76950 [00:10<01:02, 1043.80it/s] 15%|█▍        | 11496/76950 [00:11<01:01, 1059.84it/s] 15%|█▌        | 11603/76950 [00:11<01:05, 995.58it/s]  15%|█▌        | 11709/76950 [00:11<01:04, 1011.81it/s] 15%|█▌        | 11811/76950 [00:11<01:05, 987.61it/s]  15%|█▌        | 11911/76950 [00:11<01:06, 984.76it/s] 16%|█▌        | 12010/76950 [00:11<01:07, 956.07it/s] 16%|█▌        | 12118/76950 [00:11<01:05, 986.61it/s] 16%|█▌        | 12242/76950 [00:11<01:01, 1057.33it/s] 16%|█▌        | 12349/76950 [00:11<01:01, 1045.69it/s] 16%|█▌        | 12460/76950 [00:11<01:00, 1062.39it/s] 16%|█▋        | 12567/76950 [00:12<01:01, 1047.66it/s] 16%|█▋        | 12672/76950 [00:12<01:04, 1004.32it/s] 17%|█▋        | 12786/76950 [00:12<01:01, 1036.51it/s] 17%|█▋        | 12891/76950 [00:12<01:03, 1013.31it/s] 17%|█▋        | 12996/76950 [00:12<01:02, 1022.51it/s] 17%|█▋        | 13112/76950 [00:12<01:00, 1060.89it/s] 17%|█▋        | 13219/76950 [00:12<01:00, 1058.91it/s] 17%|█▋        | 13326/76950 [00:12<01:00, 1057.97it/s] 17%|█▋        | 13432/76950 [00:12<01:01, 1024.87it/s] 18%|█▊        | 13548/76950 [00:13<00:59, 1060.28it/s] 18%|█▊        | 13656/76950 [00:13<00:59, 1055.17it/s] 18%|█▊        | 13764/76950 [00:13<00:59, 1056.41it/s] 18%|█▊        | 13871/76950 [00:13<00:59, 1058.60it/s] 18%|█▊        | 13992/76950 [00:13<00:57, 1097.47it/s] 18%|█▊        | 14102/76950 [00:13<00:59, 1062.26it/s] 18%|█▊        | 14209/76950 [00:13<01:00, 1039.51it/s] 19%|█▊        | 14314/76950 [00:13<01:01, 1023.11it/s] 19%|█▊        | 14420/76950 [00:13<01:00, 1031.83it/s] 19%|█▉        | 14525/76950 [00:13<01:00, 1036.66it/s] 19%|█▉        | 14629/76950 [00:14<01:00, 1037.61it/s] 19%|█▉        | 14737/76950 [00:14<00:59, 1046.98it/s] 19%|█▉        | 14845/76950 [00:14<00:58, 1054.67it/s] 19%|█▉        | 14961/76950 [00:14<00:57, 1079.40it/s] 20%|█▉        | 15074/76950 [00:14<00:56, 1094.19it/s] 20%|█▉        | 15184/76950 [00:14<00:57, 1069.64it/s] 20%|█▉        | 15297/76950 [00:14<00:56, 1086.90it/s] 20%|██        | 15406/76950 [00:14<00:58, 1055.06it/s] 20%|██        | 15512/76950 [00:14<00:58, 1051.52it/s] 20%|██        | 15625/76950 [00:14<00:57, 1071.74it/s] 20%|██        | 15746/76950 [00:15<00:55, 1111.12it/s] 21%|██        | 15858/76950 [00:15<00:55, 1098.83it/s] 21%|██        | 15973/76950 [00:15<00:54, 1113.27it/s] 21%|██        | 16085/76950 [00:15<00:55, 1105.54it/s] 21%|██        | 16196/76950 [00:15<00:55, 1094.70it/s] 21%|██        | 16306/76950 [00:15<00:57, 1062.11it/s] 21%|██▏       | 16413/76950 [00:15<00:58, 1026.26it/s] 21%|██▏       | 16521/76950 [00:15<00:58, 1040.66it/s] 22%|██▏       | 16626/76950 [00:15<00:58, 1027.74it/s] 22%|██▏       | 16729/76950 [00:16<00:59, 1010.43it/s] 22%|██▏       | 16840/76950 [00:16<00:57, 1038.73it/s] 22%|██▏       | 16950/76950 [00:16<00:56, 1055.23it/s] 22%|██▏       | 17073/76950 [00:16<00:54, 1106.10it/s] 22%|██▏       | 17184/76950 [00:16<00:55, 1080.06it/s] 22%|██▏       | 17295/76950 [00:16<00:54, 1085.94it/s] 23%|██▎       | 17404/76950 [00:16<00:56, 1049.12it/s] 23%|██▎       | 17516/76950 [00:16<00:55, 1065.75it/s] 23%|██▎       | 17623/76950 [00:16<00:55, 1065.14it/s] 23%|██▎       | 17734/76950 [00:16<00:55, 1074.03it/s] 23%|██▎       | 17846/76950 [00:17<00:54, 1084.94it/s] 23%|██▎       | 17960/76950 [00:17<00:53, 1097.95it/s] 23%|██▎       | 18070/76950 [00:17<00:56, 1047.38it/s] 24%|██▎       | 18176/76950 [00:17<00:56, 1047.77it/s] 24%|██▍       | 18282/76950 [00:17<00:56, 1035.58it/s] 24%|██▍       | 18391/76950 [00:17<00:55, 1050.52it/s] 24%|██▍       | 18497/76950 [00:17<00:55, 1047.08it/s] 24%|██▍       | 18602/76950 [00:17<00:55, 1045.09it/s] 24%|██▍       | 18711/76950 [00:17<00:55, 1056.64it/s] 24%|██▍       | 18817/76950 [00:17<00:57, 1017.05it/s] 25%|██▍       | 18920/76950 [00:18<00:58, 991.90it/s]  25%|██▍       | 19033/76950 [00:18<00:56, 1030.91it/s] 25%|██▍       | 19137/76950 [00:18<00:56, 1025.31it/s] 25%|██▌       | 19240/76950 [00:18<00:57, 1003.43it/s] 25%|██▌       | 19342/76950 [00:18<00:57, 1004.03it/s] 25%|██▌       | 19447/76950 [00:18<00:56, 1015.06it/s] 25%|██▌       | 19549/76950 [00:18<00:57, 997.91it/s]  26%|██▌       | 19661/76950 [00:18<00:56, 1020.11it/s] 26%|██▌       | 19779/76950 [00:18<00:53, 1066.18it/s] 26%|██▌       | 19886/76950 [00:19<00:55, 1027.46it/s] 26%|██▌       | 20008/76950 [00:19<00:52, 1075.89it/s] 26%|██▌       | 20116/76950 [00:19<00:54, 1035.94it/s] 26%|██▋       | 20227/76950 [00:19<00:53, 1050.65it/s] 26%|██▋       | 20338/76950 [00:19<00:53, 1067.73it/s] 27%|██▋       | 20446/76950 [00:19<00:53, 1058.67it/s] 27%|██▋       | 20553/76950 [00:19<00:54, 1025.94it/s] 27%|██▋       | 20656/76950 [00:19<00:55, 1005.61it/s] 27%|██▋       | 20764/76950 [00:19<00:55, 1020.75it/s] 27%|██▋       | 20867/76950 [00:19<00:56, 1000.75it/s] 27%|██▋       | 20981/76950 [00:20<00:53, 1039.68it/s] 27%|██▋       | 21090/76950 [00:20<00:53, 1053.26it/s] 28%|██▊       | 21196/76950 [00:20<00:56, 990.80it/s]  28%|██▊       | 21304/76950 [00:20<00:54, 1015.91it/s] 28%|██▊       | 21407/76950 [00:20<00:56, 991.31it/s]  28%|██▊       | 21515/76950 [00:20<00:54, 1016.43it/s] 28%|██▊       | 21618/76950 [00:20<00:55, 995.53it/s]  28%|██▊       | 21718/76950 [00:20<00:56, 976.00it/s] 28%|██▊       | 21819/76950 [00:20<00:55, 984.60it/s] 28%|██▊       | 21918/76950 [00:21<00:56, 972.39it/s] 29%|██▊       | 22024/76950 [00:21<00:55, 997.04it/s] 29%|██▉       | 22124/76950 [00:21<00:55, 990.88it/s] 29%|██▉       | 22233/76950 [00:21<00:53, 1017.61it/s] 29%|██▉       | 22348/76950 [00:21<00:52, 1048.87it/s] 29%|██▉       | 22453/76950 [00:21<00:52, 1047.21it/s] 29%|██▉       | 22560/76950 [00:21<00:51, 1052.58it/s] 29%|██▉       | 22666/76950 [00:21<00:53, 1009.77it/s] 30%|██▉       | 22768/76950 [00:21<00:54, 991.17it/s]  30%|██▉       | 22874/76950 [00:21<00:53, 1007.58it/s] 30%|██▉       | 22990/76950 [00:22<00:51, 1049.75it/s] 30%|███       | 23096/76950 [00:22<00:55, 963.43it/s]  30%|███       | 23206/76950 [00:22<00:53, 999.83it/s] 30%|███       | 23309/76950 [00:22<00:53, 1006.79it/s] 30%|███       | 23418/76950 [00:22<00:52, 1020.07it/s] 31%|███       | 23522/76950 [00:22<00:52, 1024.43it/s] 31%|███       | 23625/76950 [00:22<00:54, 972.43it/s]  31%|███       | 23740/76950 [00:22<00:52, 1022.37it/s] 31%|███       | 23845/76950 [00:22<00:51, 1029.41it/s] 31%|███       | 23952/76950 [00:23<00:51, 1031.76it/s] 31%|███▏      | 24056/76950 [00:23<00:51, 1031.17it/s] 31%|███▏      | 24172/76950 [00:23<00:49, 1066.90it/s] 32%|███▏      | 24279/76950 [00:23<00:49, 1061.53it/s] 32%|███▏      | 24386/76950 [00:23<00:52, 998.93it/s]  32%|███▏      | 24493/76950 [00:23<00:51, 1017.68it/s] 32%|███▏      | 24608/76950 [00:23<00:49, 1053.76it/s] 32%|███▏      | 24726/76950 [00:23<00:47, 1089.79it/s] 32%|███▏      | 24836/76950 [00:23<00:49, 1045.42it/s] 32%|███▏      | 24942/76950 [00:23<00:49, 1044.06it/s] 33%|███▎      | 25048/76950 [00:24<00:49, 1047.44it/s] 33%|███▎      | 25154/76950 [00:24<00:50, 1024.78it/s] 33%|███▎      | 25257/76950 [00:24<00:52, 979.47it/s]  33%|███▎      | 25366/76950 [00:24<00:51, 1009.87it/s] 33%|███▎      | 25471/76950 [00:24<00:50, 1016.06it/s] 33%|███▎      | 25580/76950 [00:24<00:49, 1032.60it/s] 33%|███▎      | 25684/76950 [00:24<00:49, 1031.35it/s] 34%|███▎      | 25791/76950 [00:24<00:49, 1041.41it/s] 34%|███▎      | 25896/76950 [00:24<00:49, 1027.02it/s] 34%|███▍      | 26012/76950 [00:25<00:47, 1065.36it/s] 34%|███▍      | 26119/76950 [00:25<00:49, 1033.67it/s] 34%|███▍      | 26223/76950 [00:25<00:49, 1028.85it/s] 34%|███▍      | 26340/76950 [00:25<00:47, 1065.80it/s] 34%|███▍      | 26456/76950 [00:25<00:46, 1091.40it/s] 35%|███▍      | 26566/76950 [00:25<00:46, 1081.06it/s] 35%|███▍      | 26694/76950 [00:25<00:44, 1137.12it/s] 35%|███▍      | 26808/76950 [00:25<00:44, 1125.26it/s] 35%|███▍      | 26921/76950 [00:25<00:44, 1117.76it/s] 35%|███▌      | 27033/76950 [00:25<00:47, 1055.11it/s] 35%|███▌      | 27156/76950 [00:26<00:45, 1104.40it/s] 35%|███▌      | 27268/76950 [00:26<00:46, 1074.27it/s] 36%|███▌      | 27377/76950 [00:26<00:49, 1003.33it/s] 36%|███▌      | 27488/76950 [00:26<00:47, 1032.03it/s] 36%|███▌      | 27600/76950 [00:26<00:46, 1056.36it/s] 36%|███▌      | 27711/76950 [00:26<00:46, 1068.66it/s] 36%|███▌      | 27819/76950 [00:26<00:47, 1039.02it/s] 36%|███▋      | 27924/76950 [00:26<00:48, 1010.55it/s] 36%|███▋      | 28044/76950 [00:26<00:46, 1062.87it/s] 37%|███▋      | 28165/76950 [00:27<00:44, 1100.04it/s] 37%|███▋      | 28283/76950 [00:27<00:43, 1117.86it/s] 37%|███▋      | 28396/76950 [00:27<00:43, 1111.15it/s] 37%|███▋      | 28508/76950 [00:27<00:44, 1079.78it/s] 37%|███▋      | 28623/76950 [00:27<00:44, 1085.49it/s] 37%|███▋      | 28732/76950 [00:27<00:44, 1071.96it/s] 37%|███▋      | 28840/76950 [00:27<00:47, 1015.79it/s] 38%|███▊      | 28943/76950 [00:27<00:48, 992.27it/s]  38%|███▊      | 29044/76950 [00:27<00:48, 996.48it/s] 38%|███▊      | 29144/76950 [00:27<00:48, 993.15it/s] 38%|███▊      | 29246/76950 [00:28<00:47, 1000.11it/s] 38%|███▊      | 29351/76950 [00:28<00:47, 1012.43it/s] 38%|███▊      | 29478/76950 [00:28<00:43, 1086.35it/s] 38%|███▊      | 29587/76950 [00:28<00:46, 1022.90it/s] 39%|███▊      | 29691/76950 [00:28<00:48, 969.32it/s]  39%|███▊      | 29789/76950 [00:28<00:50, 935.24it/s] 39%|███▉      | 29914/76950 [00:28<00:46, 1021.28it/s] 39%|███▉      | 30019/76950 [00:28<00:45, 1025.82it/s] 39%|███▉      | 30125/76950 [00:28<00:45, 1033.88it/s] 39%|███▉      | 30240/76950 [00:29<00:43, 1067.23it/s] 39%|███▉      | 30354/76950 [00:29<00:42, 1087.06it/s] 40%|███▉      | 30471/76950 [00:29<00:42, 1102.19it/s] 40%|███▉      | 30582/76950 [00:29<00:42, 1096.15it/s] 40%|███▉      | 30692/76950 [00:29<00:42, 1090.34it/s] 40%|████      | 30802/76950 [00:29<00:43, 1068.99it/s] 40%|████      | 30910/76950 [00:29<00:43, 1070.51it/s] 40%|████      | 31018/76950 [00:29<00:43, 1047.22it/s] 40%|████      | 31126/76950 [00:29<00:43, 1046.73it/s] 41%|████      | 31231/76950 [00:29<00:45, 998.85it/s]  41%|████      | 31342/76950 [00:30<00:44, 1026.46it/s] 41%|████      | 31463/76950 [00:30<00:42, 1075.47it/s] 41%|████      | 31572/76950 [00:30<00:44, 1026.29it/s] 41%|████      | 31676/76950 [00:30<00:44, 1023.41it/s] 41%|████▏     | 31782/76950 [00:30<00:43, 1030.19it/s] 41%|████▏     | 31886/76950 [00:30<00:44, 1015.35it/s] 42%|████▏     | 32003/76950 [00:30<00:42, 1057.95it/s] 42%|████▏     | 32110/76950 [00:30<00:43, 1038.13it/s] 42%|████▏     | 32215/76950 [00:30<00:43, 1033.44it/s] 42%|████▏     | 32319/76950 [00:31<00:43, 1017.24it/s] 42%|████▏     | 32421/76950 [00:31<00:43, 1017.95it/s] 42%|████▏     | 32523/76950 [00:31<00:46, 963.38it/s]  42%|████▏     | 32633/76950 [00:31<00:44, 994.34it/s] 43%|████▎     | 32753/76950 [00:31<00:41, 1053.02it/s] 43%|████▎     | 32864/76950 [00:31<00:41, 1068.76it/s] 43%|████▎     | 32974/76950 [00:31<00:41, 1063.72it/s] 43%|████▎     | 33084/76950 [00:31<00:41, 1067.30it/s] 43%|████▎     | 33192/76950 [00:31<00:41, 1063.40it/s] 43%|████▎     | 33309/76950 [00:31<00:39, 1094.38it/s] 43%|████▎     | 33419/76950 [00:32<00:42, 1024.72it/s] 44%|████▎     | 33531/76950 [00:32<00:41, 1051.60it/s] 44%|████▎     | 33638/76950 [00:32<00:42, 1015.07it/s] 44%|████▍     | 33741/76950 [00:32<00:42, 1007.57it/s] 44%|████▍     | 33843/76950 [00:32<00:42, 1005.14it/s] 44%|████▍     | 33967/76950 [00:32<00:40, 1069.69it/s] 44%|████▍     | 34075/76950 [00:32<00:42, 1002.82it/s] 44%|████▍     | 34181/76950 [00:32<00:42, 1016.29it/s] 45%|████▍     | 34284/76950 [00:32<00:44, 968.03it/s]  45%|████▍     | 34383/76950 [00:33<00:43, 972.69it/s] 45%|████▍     | 34481/76950 [00:33<00:44, 946.85it/s] 45%|████▍     | 34577/76950 [00:33<00:45, 929.07it/s] 45%|████▌     | 34679/76950 [00:33<00:44, 954.66it/s] 45%|████▌     | 34778/76950 [00:33<00:43, 963.47it/s] 45%|████▌     | 34879/76950 [00:33<00:43, 971.93it/s] 45%|████▌     | 34977/76950 [00:33<00:44, 943.07it/s] 46%|████▌     | 35072/76950 [00:33<00:44, 944.39it/s] 46%|████▌     | 35174/76950 [00:33<00:43, 966.03it/s] 46%|████▌     | 35288/76950 [00:33<00:40, 1016.80it/s] 46%|████▌     | 35390/76950 [00:34<00:40, 1017.24it/s] 46%|████▌     | 35492/76950 [00:34<00:40, 1013.92it/s] 46%|████▋     | 35611/76950 [00:34<00:38, 1062.96it/s] 46%|████▋     | 35720/76950 [00:34<00:38, 1069.09it/s] 47%|████▋     | 35827/76950 [00:34<00:40, 1024.91it/s] 47%|████▋     | 35937/76950 [00:34<00:39, 1046.37it/s] 47%|████▋     | 36045/76950 [00:34<00:38, 1055.99it/s] 47%|████▋     | 36152/76950 [00:34<00:38, 1058.80it/s] 47%|████▋     | 36259/76950 [00:34<00:38, 1055.16it/s] 47%|████▋     | 36391/76950 [00:35<00:35, 1131.42it/s] 47%|████▋     | 36505/76950 [00:35<00:36, 1095.50it/s] 48%|████▊     | 36616/76950 [00:35<00:36, 1097.48it/s] 48%|████▊     | 36738/76950 [00:35<00:35, 1129.07it/s] 48%|████▊     | 36852/76950 [00:35<00:36, 1087.71it/s] 48%|████▊     | 36962/76950 [00:35<00:37, 1074.53it/s] 48%|████▊     | 37070/76950 [00:35<00:38, 1028.43it/s] 48%|████▊     | 37187/76950 [00:35<00:37, 1064.80it/s] 48%|████▊     | 37295/76950 [00:35<00:37, 1058.77it/s] 49%|████▊     | 37416/76950 [00:35<00:35, 1101.94it/s] 49%|████▉     | 37527/76950 [00:36<00:36, 1080.62it/s] 49%|████▉     | 37648/76950 [00:36<00:35, 1115.50it/s] 49%|████▉     | 37760/76950 [00:36<00:36, 1078.58it/s] 49%|████▉     | 37869/76950 [00:36<00:38, 1002.95it/s] 49%|████▉     | 37971/76950 [00:36<00:39, 981.16it/s]  49%|████▉     | 38085/76950 [00:36<00:37, 1023.26it/s] 50%|████▉     | 38196/76950 [00:36<00:36, 1047.58it/s] 50%|████▉     | 38306/76950 [00:36<00:36, 1058.08it/s] 50%|████▉     | 38413/76950 [00:36<00:36, 1059.69it/s] 50%|█████     | 38526/76950 [00:37<00:35, 1074.71it/s] 50%|█████     | 38634/76950 [00:37<00:35, 1068.63it/s] 50%|█████     | 38744/76950 [00:37<00:35, 1077.08it/s] 50%|█████     | 38852/76950 [00:37<00:35, 1074.20it/s] 51%|█████     | 38961/76950 [00:37<00:35, 1078.40it/s] 51%|█████     | 39075/76950 [00:37<00:34, 1093.00it/s] 51%|█████     | 39185/76950 [00:37<00:34, 1084.73it/s] 51%|█████     | 39296/76950 [00:37<00:34, 1089.43it/s] 51%|█████     | 39405/76950 [00:37<00:34, 1075.42it/s] 51%|█████▏    | 39513/76950 [00:37<00:34, 1069.91it/s] 51%|█████▏    | 39621/76950 [00:38<00:36, 1027.05it/s] 52%|█████▏    | 39725/76950 [00:38<00:36, 1007.82it/s] 52%|█████▏    | 39834/76950 [00:38<00:36, 1028.32it/s] 52%|█████▏    | 39943/76950 [00:38<00:35, 1045.84it/s] 52%|█████▏    | 40049/76950 [00:38<00:35, 1045.06it/s] 52%|█████▏    | 40163/76950 [00:38<00:34, 1072.54it/s] 52%|█████▏    | 40271/76950 [00:38<00:34, 1071.04it/s] 52%|█████▏    | 40392/76950 [00:38<00:32, 1109.50it/s] 53%|█████▎    | 40504/76950 [00:38<00:33, 1092.42it/s] 53%|█████▎    | 40619/76950 [00:38<00:32, 1104.38it/s] 53%|█████▎    | 40743/76950 [00:39<00:31, 1143.88it/s] 53%|█████▎    | 40858/76950 [00:39<00:32, 1117.48it/s] 53%|█████▎    | 40970/76950 [00:39<00:33, 1076.60it/s] 53%|█████▎    | 41079/76950 [00:39<00:33, 1071.94it/s] 54%|█████▎    | 41193/76950 [00:39<00:33, 1080.53it/s] 54%|█████▎    | 41310/76950 [00:39<00:32, 1102.57it/s] 54%|█████▍    | 41421/76950 [00:39<00:32, 1083.39it/s] 54%|█████▍    | 41535/76950 [00:39<00:32, 1099.43it/s] 54%|█████▍    | 41656/76950 [00:39<00:31, 1129.39it/s] 54%|█████▍    | 41770/76950 [00:40<00:32, 1073.58it/s] 54%|█████▍    | 41879/76950 [00:40<00:33, 1052.98it/s] 55%|█████▍    | 41985/76950 [00:40<00:34, 1013.69it/s] 55%|█████▍    | 42096/76950 [00:40<00:33, 1037.82it/s] 55%|█████▍    | 42201/76950 [00:40<00:35, 982.13it/s]  55%|█████▍    | 42301/76950 [00:40<00:36, 956.22it/s] 55%|█████▌    | 42398/76950 [00:40<00:36, 955.24it/s] 55%|█████▌    | 42494/76950 [00:40<00:36, 946.90it/s] 55%|█████▌    | 42612/76950 [00:40<00:34, 1009.47it/s] 56%|█████▌    | 42714/76950 [00:41<00:34, 993.09it/s]  56%|█████▌    | 42827/76950 [00:41<00:33, 1032.38it/s] 56%|█████▌    | 42931/76950 [00:41<00:33, 1028.24it/s] 56%|█████▌    | 43036/76950 [00:41<00:32, 1033.20it/s] 56%|█████▌    | 43140/76950 [00:41<00:32, 1029.32it/s] 56%|█████▌    | 43248/76950 [00:41<00:32, 1044.05it/s] 56%|█████▋    | 43353/76950 [00:41<00:32, 1027.00it/s] 56%|█████▋    | 43464/76950 [00:41<00:32, 1045.29it/s] 57%|█████▋    | 43575/76950 [00:41<00:31, 1063.98it/s] 57%|█████▋    | 43687/76950 [00:41<00:30, 1080.34it/s] 57%|█████▋    | 43796/76950 [00:42<00:31, 1068.81it/s] 57%|█████▋    | 43903/76950 [00:42<00:31, 1059.38it/s] 57%|█████▋    | 44010/76950 [00:42<00:31, 1049.06it/s] 57%|█████▋    | 44115/76950 [00:42<00:32, 1019.41it/s] 57%|█████▋    | 44225/76950 [00:42<00:31, 1042.00it/s] 58%|█████▊    | 44336/76950 [00:42<00:30, 1061.65it/s] 58%|█████▊    | 44447/76950 [00:42<00:30, 1075.28it/s] 58%|█████▊    | 44557/76950 [00:42<00:29, 1081.24it/s] 58%|█████▊    | 44686/76950 [00:42<00:28, 1140.71it/s] 58%|█████▊    | 44801/76950 [00:42<00:28, 1125.60it/s] 58%|█████▊    | 44914/76950 [00:43<00:30, 1056.94it/s] 59%|█████▊    | 45021/76950 [00:43<00:31, 1027.11it/s] 59%|█████▊    | 45125/76950 [00:43<00:31, 1021.94it/s] 59%|█████▉    | 45235/76950 [00:43<00:30, 1044.06it/s] 59%|█████▉    | 45340/76950 [00:43<00:30, 1020.34it/s] 59%|█████▉    | 45456/76950 [00:43<00:29, 1051.55it/s] 59%|█████▉    | 45562/76950 [00:43<00:30, 1039.07it/s] 59%|█████▉    | 45670/76950 [00:43<00:29, 1050.29it/s] 59%|█████▉    | 45785/76950 [00:43<00:28, 1077.57it/s] 60%|█████▉    | 45895/76950 [00:44<00:28, 1079.35it/s] 60%|█████▉    | 46004/76950 [00:44<00:29, 1062.12it/s] 60%|█████▉    | 46118/76950 [00:44<00:28, 1084.32it/s] 60%|██████    | 46227/76950 [00:44<00:28, 1062.78it/s] 60%|██████    | 46334/76950 [00:44<00:29, 1055.28it/s] 60%|██████    | 46440/76950 [00:44<00:30, 1013.83it/s] 60%|██████    | 46549/76950 [00:44<00:29, 1024.50it/s] 61%|██████    | 46662/76950 [00:44<00:28, 1050.69it/s] 61%|██████    | 46774/76950 [00:44<00:28, 1070.41it/s] 61%|██████    | 46882/76950 [00:44<00:28, 1054.44it/s] 61%|██████    | 46988/76950 [00:45<00:29, 1018.71it/s] 61%|██████    | 47091/76950 [00:45<00:30, 987.45it/s]  61%|██████▏   | 47203/76950 [00:45<00:29, 1022.71it/s] 61%|██████▏   | 47308/76950 [00:45<00:28, 1028.94it/s] 62%|██████▏   | 47412/76950 [00:45<00:29, 999.29it/s]  62%|██████▏   | 47513/76950 [00:45<00:29, 998.15it/s] 62%|██████▏   | 47624/76950 [00:45<00:28, 1030.51it/s] 62%|██████▏   | 47735/76950 [00:45<00:27, 1045.21it/s] 62%|██████▏   | 47846/76950 [00:45<00:27, 1063.71it/s] 62%|██████▏   | 47963/76950 [00:45<00:26, 1094.72it/s] 62%|██████▏   | 48087/76950 [00:46<00:25, 1135.26it/s] 63%|██████▎   | 48201/76950 [00:46<00:27, 1055.43it/s] 63%|██████▎   | 48310/76950 [00:46<00:26, 1063.96it/s] 63%|██████▎   | 48418/76950 [00:46<00:27, 1052.77it/s] 63%|██████▎   | 48524/76950 [00:46<00:26, 1054.11it/s] 63%|██████▎   | 48630/76950 [00:46<00:26, 1055.01it/s] 63%|██████▎   | 48736/76950 [00:46<00:26, 1050.98it/s] 63%|██████▎   | 48851/76950 [00:46<00:26, 1075.85it/s] 64%|██████▎   | 48959/76950 [00:46<00:27, 1021.90it/s] 64%|██████▍   | 49075/76950 [00:47<00:26, 1058.47it/s] 64%|██████▍   | 49194/76950 [00:47<00:25, 1095.99it/s] 64%|██████▍   | 49305/76950 [00:47<00:25, 1093.13it/s] 64%|██████▍   | 49423/76950 [00:47<00:24, 1110.55it/s] 64%|██████▍   | 49535/76950 [00:47<00:24, 1101.30it/s] 65%|██████▍   | 49646/76950 [00:47<00:25, 1062.00it/s] 65%|██████▍   | 49753/76950 [00:47<00:26, 1019.89it/s] 65%|██████▍   | 49856/76950 [00:47<00:27, 971.43it/s]  65%|██████▍   | 49962/76950 [00:47<00:27, 993.15it/s] 65%|██████▌   | 50065/76950 [00:48<00:26, 1000.85it/s] 65%|██████▌   | 50166/76950 [00:48<00:27, 957.08it/s]  65%|██████▌   | 50263/76950 [00:48<00:28, 943.79it/s] 65%|██████▌   | 50369/76950 [00:48<00:27, 976.54it/s] 66%|██████▌   | 50479/76950 [00:48<00:26, 1007.64it/s] 66%|██████▌   | 50581/76950 [00:48<00:26, 992.15it/s]  66%|██████▌   | 50681/76950 [00:48<00:26, 977.19it/s] 66%|██████▌   | 50779/76950 [00:48<00:27, 945.22it/s] 66%|██████▌   | 50888/76950 [00:48<00:26, 985.79it/s] 66%|██████▋   | 50987/76950 [00:48<00:28, 920.76it/s] 66%|██████▋   | 51106/76950 [00:49<00:26, 992.62it/s] 67%|██████▋   | 51234/76950 [00:49<00:23, 1073.57it/s] 67%|██████▋   | 51349/76950 [00:49<00:23, 1094.35it/s] 67%|██████▋   | 51460/76950 [00:49<00:23, 1063.72it/s] 67%|██████▋   | 51575/76950 [00:49<00:23, 1088.12it/s] 67%|██████▋   | 51685/76950 [00:49<00:24, 1050.72it/s] 67%|██████▋   | 51798/76950 [00:49<00:23, 1072.63it/s] 67%|██████▋   | 51906/76950 [00:49<00:24, 1039.89it/s] 68%|██████▊   | 52012/76950 [00:49<00:23, 1044.99it/s] 68%|██████▊   | 52117/76950 [00:50<00:24, 1020.29it/s] 68%|██████▊   | 52221/76950 [00:50<00:24, 1023.72it/s] 68%|██████▊   | 52349/76950 [00:50<00:22, 1096.46it/s] 68%|██████▊   | 52464/76950 [00:50<00:22, 1108.29it/s] 68%|██████▊   | 52583/76950 [00:50<00:21, 1131.85it/s] 68%|██████▊   | 52697/76950 [00:50<00:22, 1088.32it/s] 69%|██████▊   | 52807/76950 [00:50<00:22, 1081.09it/s] 69%|██████▉   | 52916/76950 [00:50<00:23, 1034.69it/s] 69%|██████▉   | 53021/76950 [00:50<00:23, 1015.53it/s] 69%|██████▉   | 53124/76950 [00:50<00:23, 1018.94it/s] 69%|██████▉   | 53227/76950 [00:51<00:23, 1018.02it/s] 69%|██████▉   | 53338/76950 [00:51<00:22, 1037.92it/s] 69%|██████▉   | 53450/76950 [00:51<00:22, 1056.72it/s] 70%|██████▉   | 53556/76950 [00:51<00:22, 1040.92it/s] 70%|██████▉   | 53662/76950 [00:51<00:22, 1043.77it/s] 70%|██████▉   | 53777/76950 [00:51<00:21, 1068.87it/s] 70%|███████   | 53885/76950 [00:51<00:21, 1072.02it/s] 70%|███████   | 53995/76950 [00:51<00:21, 1078.40it/s] 70%|███████   | 54103/76950 [00:51<00:21, 1065.89it/s] 70%|███████   | 54210/76950 [00:52<00:22, 990.09it/s]  71%|███████   | 54321/76950 [00:52<00:22, 1020.11it/s] 71%|███████   | 54424/76950 [00:52<00:22, 1007.53it/s] 71%|███████   | 54526/76950 [00:52<00:24, 933.24it/s]  71%|███████   | 54632/76950 [00:52<00:23, 967.84it/s] 71%|███████   | 54734/76950 [00:52<00:22, 980.42it/s] 71%|███████▏  | 54838/76950 [00:52<00:22, 996.91it/s] 71%|███████▏  | 54953/76950 [00:52<00:21, 1036.78it/s] 72%|███████▏  | 55064/76950 [00:52<00:20, 1057.96it/s] 72%|███████▏  | 55178/76950 [00:52<00:20, 1079.11it/s] 72%|███████▏  | 55287/76950 [00:53<00:20, 1072.58it/s] 72%|███████▏  | 55395/76950 [00:53<00:20, 1072.19it/s] 72%|███████▏  | 55503/76950 [00:53<00:20, 1056.85it/s] 72%|███████▏  | 55609/76950 [00:53<00:20, 1042.76it/s] 72%|███████▏  | 55714/76950 [00:53<00:20, 1022.00it/s] 73%|███████▎  | 55831/76950 [00:53<00:19, 1063.45it/s] 73%|███████▎  | 55954/76950 [00:53<00:18, 1108.36it/s] 73%|███████▎  | 56067/76950 [00:53<00:18, 1113.60it/s] 73%|███████▎  | 56179/76950 [00:53<00:19, 1058.12it/s] 73%|███████▎  | 56296/76950 [00:53<00:18, 1087.22it/s] 73%|███████▎  | 56411/76950 [00:54<00:18, 1102.88it/s] 73%|███████▎  | 56522/76950 [00:54<00:18, 1079.93it/s] 74%|███████▎  | 56631/76950 [00:54<00:20, 1012.88it/s] 74%|███████▎  | 56738/76950 [00:54<00:19, 1026.99it/s] 74%|███████▍  | 56847/76950 [00:54<00:19, 1044.76it/s] 74%|███████▍  | 56953/76950 [00:54<00:19, 1013.50it/s] 74%|███████▍  | 57068/76950 [00:54<00:18, 1050.87it/s] 74%|███████▍  | 57174/76950 [00:54<00:19, 1033.34it/s] 74%|███████▍  | 57278/76950 [00:54<00:19, 1025.55it/s] 75%|███████▍  | 57381/76950 [00:55<00:19, 1011.02it/s] 75%|███████▍  | 57483/76950 [00:55<00:19, 1004.97it/s] 75%|███████▍  | 57608/76950 [00:55<00:17, 1076.20it/s] 75%|███████▌  | 57724/76950 [00:55<00:17, 1100.25it/s] 75%|███████▌  | 57835/76950 [00:55<00:17, 1098.62it/s] 75%|███████▌  | 57946/76950 [00:55<00:17, 1089.09it/s] 75%|███████▌  | 58063/76950 [00:55<00:16, 1112.13it/s] 76%|███████▌  | 58175/76950 [00:55<00:18, 1027.86it/s] 76%|███████▌  | 58286/76950 [00:55<00:17, 1050.47it/s] 76%|███████▌  | 58393/76950 [00:56<00:18, 1017.53it/s] 76%|███████▌  | 58496/76950 [00:56<00:18, 994.31it/s]  76%|███████▌  | 58607/76950 [00:56<00:17, 1025.45it/s] 76%|███████▋  | 58718/76950 [00:56<00:17, 1044.90it/s] 76%|███████▋  | 58823/76950 [00:56<00:17, 1045.00it/s] 77%|███████▋  | 58928/76950 [00:56<00:17, 1040.79it/s] 77%|███████▋  | 59053/76950 [00:56<00:16, 1101.91it/s] 77%|███████▋  | 59180/76950 [00:56<00:15, 1150.06it/s] 77%|███████▋  | 59296/76950 [00:56<00:15, 1143.91it/s] 77%|███████▋  | 59411/76950 [00:56<00:15, 1098.41it/s] 77%|███████▋  | 59525/76950 [00:57<00:15, 1109.58it/s] 78%|███████▊  | 59637/76950 [00:57<00:16, 1027.53it/s] 78%|███████▊  | 59742/76950 [00:57<00:16, 1019.74it/s] 78%|███████▊  | 59845/76950 [00:57<00:17, 988.51it/s]  78%|███████▊  | 59954/76950 [00:57<00:16, 1014.37it/s] 78%|███████▊  | 60057/76950 [00:57<00:16, 996.10it/s]  78%|███████▊  | 60168/76950 [00:57<00:16, 1028.29it/s] 78%|███████▊  | 60274/76950 [00:57<00:16, 1028.71it/s] 78%|███████▊  | 60386/76950 [00:57<00:15, 1054.83it/s] 79%|███████▊  | 60492/76950 [00:58<00:16, 1026.86it/s] 79%|███████▊  | 60596/76950 [00:58<00:16, 993.83it/s]  79%|███████▉  | 60710/76950 [00:58<00:15, 1030.48it/s] 79%|███████▉  | 60820/76950 [00:58<00:15, 1049.09it/s] 79%|███████▉  | 60928/76950 [00:58<00:15, 1053.05it/s] 79%|███████▉  | 61043/76950 [00:58<00:14, 1077.81it/s] 79%|███████▉  | 61168/76950 [00:58<00:14, 1126.88it/s] 80%|███████▉  | 61281/76950 [00:58<00:14, 1082.46it/s] 80%|███████▉  | 61391/76950 [00:58<00:14, 1079.37it/s] 80%|███████▉  | 61500/76950 [00:58<00:14, 1061.47it/s] 80%|████████  | 61612/76950 [00:59<00:14, 1066.20it/s] 80%|████████  | 61719/76950 [00:59<00:15, 1009.95it/s] 80%|████████  | 61821/76950 [00:59<00:15, 988.71it/s]  80%|████████  | 61929/76950 [00:59<00:14, 1012.13it/s] 81%|████████  | 62040/76950 [00:59<00:14, 1031.70it/s] 81%|████████  | 62144/76950 [00:59<00:14, 1033.96it/s] 81%|████████  | 62248/76950 [00:59<00:14, 1008.57it/s] 81%|████████  | 62350/76950 [00:59<00:15, 965.79it/s]  81%|████████  | 62449/76950 [00:59<00:14, 970.99it/s] 81%|████████▏ | 62552/76950 [01:00<00:14, 982.68it/s] 81%|████████▏ | 62662/76950 [01:00<00:14, 1011.17it/s] 82%|████████▏ | 62764/76950 [01:00<00:14, 987.99it/s]  82%|████████▏ | 62870/76950 [01:00<00:13, 1006.67it/s] 82%|████████▏ | 62971/76950 [01:00<00:14, 940.75it/s]  82%|████████▏ | 63110/76950 [01:00<00:13, 1064.09it/s] 82%|████████▏ | 63218/76950 [01:00<00:12, 1065.21it/s] 82%|████████▏ | 63326/76950 [01:00<00:12, 1050.90it/s] 82%|████████▏ | 63434/76950 [01:00<00:12, 1052.99it/s] 83%|████████▎ | 63545/76950 [01:00<00:12, 1068.44it/s] 83%|████████▎ | 63653/76950 [01:01<00:12, 1045.95it/s] 83%|████████▎ | 63771/76950 [01:01<00:12, 1081.76it/s] 83%|████████▎ | 63880/76950 [01:01<00:12, 1070.90it/s] 83%|████████▎ | 63988/76950 [01:01<00:12, 1058.15it/s] 83%|████████▎ | 64094/76950 [01:01<00:12, 1038.13it/s] 83%|████████▎ | 64198/76950 [01:01<00:12, 1000.67it/s] 84%|████████▎ | 64299/76950 [01:01<00:12, 1000.98it/s] 84%|████████▎ | 64417/76950 [01:01<00:11, 1051.55it/s] 84%|████████▍ | 64534/76950 [01:01<00:11, 1084.97it/s] 84%|████████▍ | 64643/76950 [01:02<00:11, 1070.47it/s] 84%|████████▍ | 64751/76950 [01:02<00:11, 1036.70it/s] 84%|████████▍ | 64856/76950 [01:02<00:11, 1019.90it/s] 84%|████████▍ | 64966/76950 [01:02<00:11, 1041.56it/s] 85%|████████▍ | 65071/76950 [01:02<00:11, 1013.56it/s] 85%|████████▍ | 65177/76950 [01:02<00:11, 1024.52it/s] 85%|████████▍ | 65280/76950 [01:02<00:11, 1021.65it/s] 85%|████████▌ | 65408/76950 [01:02<00:10, 1094.20it/s] 85%|████████▌ | 65519/76950 [01:02<00:10, 1096.70it/s] 85%|████████▌ | 65629/76950 [01:02<00:10, 1044.45it/s] 85%|████████▌ | 65751/76950 [01:03<00:10, 1094.69it/s] 86%|████████▌ | 65862/76950 [01:03<00:10, 1019.72it/s] 86%|████████▌ | 65968/76950 [01:03<00:10, 1028.42it/s] 86%|████████▌ | 66072/76950 [01:03<00:10, 1002.70it/s] 86%|████████▌ | 66190/76950 [01:03<00:10, 1049.87it/s] 86%|████████▌ | 66296/76950 [01:03<00:10, 1043.73it/s] 86%|████████▋ | 66409/76950 [01:03<00:09, 1067.83it/s] 86%|████████▋ | 66530/76950 [01:03<00:09, 1106.80it/s] 87%|████████▋ | 66642/76950 [01:03<00:09, 1082.51it/s] 87%|████████▋ | 66751/76950 [01:04<00:09, 1029.65it/s] 87%|████████▋ | 66865/76950 [01:04<00:09, 1056.27it/s] 87%|████████▋ | 66974/76950 [01:04<00:09, 1065.61it/s] 87%|████████▋ | 67082/76950 [01:04<00:09, 1064.65it/s] 87%|████████▋ | 67189/76950 [01:04<00:09, 1050.44it/s] 87%|████████▋ | 67298/76950 [01:04<00:09, 1060.60it/s] 88%|████████▊ | 67405/76950 [01:04<00:09, 1010.36it/s] 88%|████████▊ | 67507/76950 [01:04<00:09, 1008.86it/s] 88%|████████▊ | 67609/76950 [01:04<00:09, 966.27it/s]  88%|████████▊ | 67710/76950 [01:04<00:09, 973.47it/s] 88%|████████▊ | 67808/76950 [01:05<00:09, 938.79it/s] 88%|████████▊ | 67907/76950 [01:05<00:09, 951.48it/s] 88%|████████▊ | 68017/76950 [01:05<00:09, 991.69it/s] 89%|████████▊ | 68117/76950 [01:05<00:08, 992.90it/s] 89%|████████▊ | 68221/76950 [01:05<00:08, 1005.37it/s] 89%|████████▉ | 68328/76950 [01:05<00:08, 1023.56it/s] 89%|████████▉ | 68434/76950 [01:05<00:08, 1030.94it/s] 89%|████████▉ | 68538/76950 [01:05<00:08, 1025.09it/s] 89%|████████▉ | 68653/76950 [01:05<00:07, 1054.81it/s] 89%|████████▉ | 68759/76950 [01:06<00:07, 1027.71it/s] 90%|████████▉ | 68875/76950 [01:06<00:07, 1066.15it/s] 90%|████████▉ | 68982/76950 [01:06<00:07, 1056.03it/s] 90%|████████▉ | 69088/76950 [01:06<00:07, 1045.91it/s] 90%|████████▉ | 69193/76950 [01:06<00:07, 1035.00it/s] 90%|█████████ | 69298/76950 [01:06<00:07, 1038.43it/s] 90%|█████████ | 69402/76950 [01:06<00:07, 1030.40it/s] 90%|█████████ | 69506/76950 [01:06<00:07, 1008.62it/s] 90%|█████████ | 69622/76950 [01:06<00:06, 1052.67it/s] 91%|█████████ | 69728/76950 [01:06<00:07, 1010.38it/s] 91%|█████████ | 69834/76950 [01:07<00:06, 1024.38it/s] 91%|█████████ | 69937/76950 [01:07<00:06, 1023.08it/s] 91%|█████████ | 70047/76950 [01:07<00:06, 1044.62it/s] 91%|█████████ | 70153/76950 [01:07<00:06, 1046.96it/s] 91%|█████████▏| 70258/76950 [01:07<00:06, 1019.42it/s] 91%|█████████▏| 70368/76950 [01:07<00:06, 1040.25it/s] 92%|█████████▏| 70473/76950 [01:07<00:06, 1024.96it/s] 92%|█████████▏| 70576/76950 [01:07<00:06, 1015.40it/s] 92%|█████████▏| 70678/76950 [01:07<00:06, 960.38it/s]  92%|█████████▏| 70785/76950 [01:07<00:06, 988.29it/s] 92%|█████████▏| 70898/76950 [01:08<00:05, 1028.03it/s] 92%|█████████▏| 71002/76950 [01:08<00:05, 1019.43it/s] 92%|█████████▏| 71118/76950 [01:08<00:05, 1059.49it/s] 93%|█████████▎| 71225/76950 [01:08<00:05, 1056.67it/s] 93%|█████████▎| 71331/76950 [01:08<00:05, 1024.06it/s] 93%|█████████▎| 71434/76950 [01:08<00:05, 1017.99it/s] 93%|█████████▎| 71545/76950 [01:08<00:05, 1044.26it/s] 93%|█████████▎| 71650/76950 [01:08<00:05, 1024.93it/s] 93%|█████████▎| 71784/76950 [01:08<00:04, 1106.40it/s] 93%|█████████▎| 71895/76950 [01:09<00:04, 1072.48it/s] 94%|█████████▎| 72005/76950 [01:09<00:04, 1074.92it/s] 94%|█████████▎| 72113/76950 [01:09<00:04, 1067.42it/s] 94%|█████████▍| 72228/76950 [01:09<00:04, 1088.17it/s] 94%|█████████▍| 72337/76950 [01:09<00:04, 1080.61it/s] 94%|█████████▍| 72451/76950 [01:09<00:04, 1096.83it/s] 94%|█████████▍| 72561/76950 [01:09<00:04, 1086.69it/s] 94%|█████████▍| 72670/76950 [01:09<00:04, 1046.29it/s] 95%|█████████▍| 72777/76950 [01:09<00:03, 1052.27it/s] 95%|█████████▍| 72890/76950 [01:09<00:03, 1073.76it/s] 95%|█████████▍| 72998/76950 [01:10<00:03, 1036.07it/s] 95%|█████████▌| 73103/76950 [01:10<00:03, 1004.60it/s] 95%|█████████▌| 73217/76950 [01:10<00:03, 1042.90it/s] 95%|█████████▌| 73322/76950 [01:10<00:03, 1020.82it/s] 95%|█████████▌| 73425/76950 [01:10<00:03, 971.27it/s]  96%|█████████▌| 73533/76950 [01:10<00:03, 1001.43it/s] 96%|█████████▌| 73645/76950 [01:10<00:03, 1029.66it/s] 96%|█████████▌| 73776/76950 [01:10<00:02, 1107.02it/s] 96%|█████████▌| 73888/76950 [01:10<00:02, 1060.17it/s] 96%|█████████▌| 74004/76950 [01:11<00:02, 1085.45it/s] 96%|█████████▋| 74114/76950 [01:11<00:02, 1081.47it/s] 96%|█████████▋| 74223/76950 [01:11<00:02, 1079.53it/s] 97%|█████████▋| 74332/76950 [01:11<00:02, 1077.80it/s] 97%|█████████▋| 74440/76950 [01:11<00:02, 1020.12it/s] 97%|█████████▋| 74560/76950 [01:11<00:02, 1069.70it/s] 97%|█████████▋| 74670/76950 [01:11<00:02, 1078.14it/s] 97%|█████████▋| 74779/76950 [01:11<00:02, 1046.03it/s] 97%|█████████▋| 74900/76950 [01:11<00:01, 1092.55it/s] 97%|█████████▋| 75018/76950 [01:11<00:01, 1110.64it/s] 98%|█████████▊| 75130/76950 [01:12<00:01, 1091.03it/s] 98%|█████████▊| 75240/76950 [01:12<00:01, 1089.27it/s] 98%|█████████▊| 75350/76950 [01:12<00:01, 1088.33it/s] 98%|█████████▊| 75459/76950 [01:12<00:01, 1069.14it/s] 98%|█████████▊| 75567/76950 [01:12<00:01, 1065.64it/s] 98%|█████████▊| 75684/76950 [01:12<00:01, 1095.59it/s] 98%|█████████▊| 75794/76950 [01:12<00:01, 1080.26it/s] 99%|█████████▊| 75908/76950 [01:12<00:00, 1096.63it/s] 99%|█████████▉| 76018/76950 [01:12<00:00, 1069.68it/s] 99%|█████████▉| 76126/76950 [01:13<00:00, 1060.23it/s] 99%|█████████▉| 76233/76950 [01:13<00:00, 1047.87it/s] 99%|█████████▉| 76338/76950 [01:13<00:00, 1031.20it/s] 99%|█████████▉| 76448/76950 [01:13<00:00, 1050.05it/s] 99%|█████████▉| 76554/76950 [01:13<00:00, 1010.66it/s]100%|█████████▉| 76661/76950 [01:13<00:00, 1027.18it/s]100%|█████████▉| 76782/76950 [01:13<00:00, 1079.90it/s]100%|█████████▉| 76901/76950 [01:13<00:00, 1111.92it/s]100%|██████████| 76950/76950 [01:13<00:00, 1042.96it/s]
Shape of temporal_X: (76950, 2, 1000)
X nd y are loaded for temporal feature extraction
X shape is (8550, 10000)
y shape is (8550,)
  0%|          | 0/8550 [00:00<?, ?it/s]  1%|          | 97/8550 [00:00<00:08, 956.74it/s]  2%|▏         | 209/8550 [00:00<00:07, 1051.48it/s]  4%|▎         | 315/8550 [00:00<00:08, 1019.17it/s]  5%|▌         | 428/8550 [00:00<00:07, 1059.45it/s]  6%|▋         | 550/8550 [00:00<00:07, 1110.85it/s]  8%|▊         | 671/8550 [00:00<00:06, 1141.32it/s]  9%|▉         | 787/8550 [00:00<00:06, 1147.23it/s] 11%|█         | 902/8550 [00:00<00:07, 1084.67it/s] 12%|█▏        | 1018/8550 [00:00<00:06, 1104.97it/s] 13%|█▎        | 1135/8550 [00:01<00:06, 1122.37it/s] 15%|█▍        | 1248/8550 [00:01<00:06, 1113.54it/s] 16%|█▌        | 1360/8550 [00:01<00:06, 1106.27it/s] 17%|█▋        | 1477/8550 [00:01<00:06, 1120.17it/s] 19%|█▊        | 1590/8550 [00:01<00:06, 1082.73it/s] 20%|█▉        | 1699/8550 [00:01<00:06, 1063.52it/s] 21%|██        | 1810/8550 [00:01<00:06, 1076.32it/s] 22%|██▏       | 1918/8550 [00:01<00:06, 1002.36it/s] 24%|██▎       | 2020/8550 [00:01<00:06, 993.28it/s]  25%|██▍       | 2123/8550 [00:01<00:06, 1002.98it/s] 26%|██▌       | 2224/8550 [00:02<00:06, 950.34it/s]  27%|██▋       | 2322/8550 [00:02<00:06, 948.06it/s] 28%|██▊       | 2418/8550 [00:02<00:06, 945.01it/s] 30%|██▉       | 2527/8550 [00:02<00:06, 985.97it/s] 31%|███       | 2636/8550 [00:02<00:05, 1015.84it/s] 32%|███▏      | 2750/8550 [00:02<00:05, 1040.81it/s] 33%|███▎      | 2855/8550 [00:02<00:05, 1014.87it/s] 35%|███▍      | 2962/8550 [00:02<00:05, 1027.02it/s] 36%|███▌      | 3065/8550 [00:02<00:05, 1024.37it/s] 37%|███▋      | 3175/8550 [00:03<00:05, 1045.62it/s] 38%|███▊      | 3280/8550 [00:03<00:05, 1037.38it/s] 40%|███▉      | 3390/8550 [00:03<00:04, 1049.89it/s] 41%|████      | 3496/8550 [00:03<00:05, 987.75it/s]  42%|████▏     | 3601/8550 [00:03<00:04, 1000.66it/s] 43%|████▎     | 3703/8550 [00:03<00:04, 1004.82it/s] 44%|████▍     | 3804/8550 [00:03<00:04, 1005.46it/s] 46%|████▌     | 3913/8550 [00:03<00:04, 1027.86it/s] 47%|████▋     | 4029/8550 [00:03<00:04, 1063.08it/s] 48%|████▊     | 4143/8550 [00:03<00:04, 1081.27it/s] 50%|████▉     | 4252/8550 [00:04<00:04, 1074.41it/s] 51%|█████     | 4360/8550 [00:04<00:04, 1024.05it/s] 52%|█████▏    | 4469/8550 [00:04<00:03, 1042.69it/s] 53%|█████▎    | 4574/8550 [00:04<00:03, 1021.92it/s] 55%|█████▍    | 4694/8550 [00:04<00:03, 1071.66it/s] 56%|█████▌    | 4802/8550 [00:04<00:03, 1060.37it/s] 57%|█████▋    | 4909/8550 [00:04<00:03, 1053.16it/s] 59%|█████▊    | 5016/8550 [00:04<00:03, 1057.50it/s] 60%|██████    | 5136/8550 [00:04<00:03, 1098.45it/s] 61%|██████▏   | 5250/8550 [00:05<00:02, 1110.59it/s] 63%|██████▎   | 5385/8550 [00:05<00:02, 1181.67it/s] 64%|██████▍   | 5504/8550 [00:05<00:02, 1179.30it/s] 66%|██████▌   | 5623/8550 [00:05<00:02, 1124.50it/s] 67%|██████▋   | 5737/8550 [00:05<00:02, 1109.08it/s] 68%|██████▊   | 5849/8550 [00:05<00:02, 1062.83it/s] 70%|██████▉   | 5956/8550 [00:05<00:02, 1038.28it/s] 71%|███████   | 6063/8550 [00:05<00:02, 1037.82it/s] 72%|███████▏  | 6168/8550 [00:05<00:02, 1034.43it/s] 73%|███████▎  | 6279/8550 [00:05<00:02, 1042.44it/s] 75%|███████▍  | 6384/8550 [00:06<00:02, 984.79it/s]  76%|███████▌  | 6493/8550 [00:06<00:02, 1013.58it/s] 77%|███████▋  | 6599/8550 [00:06<00:01, 1024.01it/s] 78%|███████▊  | 6709/8550 [00:06<00:01, 1044.50it/s] 80%|███████▉  | 6814/8550 [00:06<00:01, 1015.45it/s] 81%|████████  | 6925/8550 [00:06<00:01, 1042.53it/s] 82%|████████▏ | 7030/8550 [00:06<00:01, 1032.29it/s] 83%|████████▎ | 7136/8550 [00:06<00:01, 1040.08it/s] 85%|████████▍ | 7255/8550 [00:06<00:01, 1083.79it/s] 86%|████████▌ | 7367/8550 [00:07<00:01, 1082.86it/s] 88%|████████▊ | 7486/8550 [00:07<00:00, 1108.36it/s] 89%|████████▉ | 7597/8550 [00:07<00:00, 1071.23it/s] 90%|█████████ | 7716/8550 [00:07<00:00, 1105.34it/s] 92%|█████████▏| 7827/8550 [00:07<00:00, 1050.94it/s] 93%|█████████▎| 7933/8550 [00:07<00:00, 1010.11it/s] 94%|█████████▍| 8035/8550 [00:07<00:00, 987.85it/s]  95%|█████████▌| 8138/8550 [00:07<00:00, 995.07it/s] 96%|█████████▋| 8238/8550 [00:07<00:00, 985.20it/s] 98%|█████████▊| 8337/8550 [00:07<00:00, 986.34it/s] 99%|█████████▊| 8436/8550 [00:08<00:00, 982.19it/s]100%|█████████▉| 8535/8550 [00:08<00:00, 977.47it/s]100%|██████████| 8550/8550 [00:08<00:00, 1043.79it/s]
Shape of temporal_X: (8550, 2, 1000)
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Train: X=torch.Size([76950, 1, 2, 1000]), y=torch.Size([76950])
Valid: X=torch.Size([8550, 1, 2, 1000]), y=torch.Size([8550])
num_classes: 15
No pre-trained model
Loss for training is CrossEntropyLoss
epoch: 0
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:01<10:48,  1.69s/it]going through batches for holmes training:   2%|▏         | 6/384 [00:01<01:25,  4.42it/s]going through batches for holmes training:   3%|▎         | 12/384 [00:01<00:37,  9.80it/s]going through batches for holmes training:   5%|▍         | 18/384 [00:02<00:22, 15.93it/s]going through batches for holmes training:   6%|▋         | 24/384 [00:02<00:16, 22.38it/s]going through batches for holmes training:   8%|▊         | 30/384 [00:02<00:12, 28.79it/s]going through batches for holmes training:   9%|▉         | 36/384 [00:02<00:10, 34.75it/s]going through batches for holmes training:  11%|█         | 42/384 [00:02<00:08, 39.99it/s]going through batches for holmes training:  12%|█▎        | 48/384 [00:02<00:07, 44.40it/s]going through batches for holmes training:  14%|█▍        | 54/384 [00:02<00:06, 47.95it/s]going through batches for holmes training:  16%|█▌        | 60/384 [00:02<00:06, 50.67it/s]going through batches for holmes training:  17%|█▋        | 66/384 [00:02<00:06, 52.63it/s]going through batches for holmes training:  19%|█▉        | 72/384 [00:02<00:05, 54.17it/s]going through batches for holmes training:  20%|██        | 78/384 [00:03<00:05, 55.25it/s]going through batches for holmes training:  22%|██▏       | 84/384 [00:03<00:05, 56.02it/s]going through batches for holmes training:  23%|██▎       | 90/384 [00:03<00:05, 56.67it/s]going through batches for holmes training:  25%|██▌       | 96/384 [00:03<00:05, 57.04it/s]going through batches for holmes training:  27%|██▋       | 102/384 [00:03<00:04, 57.28it/s]going through batches for holmes training:  28%|██▊       | 108/384 [00:03<00:04, 57.52it/s]going through batches for holmes training:  30%|██▉       | 114/384 [00:03<00:04, 57.64it/s]going through batches for holmes training:  31%|███▏      | 120/384 [00:03<00:04, 57.78it/s]going through batches for holmes training:  33%|███▎      | 126/384 [00:03<00:04, 57.78it/s]going through batches for holmes training:  34%|███▍      | 132/384 [00:03<00:04, 57.85it/s]going through batches for holmes training:  36%|███▌      | 138/384 [00:04<00:04, 57.84it/s]going through batches for holmes training:  38%|███▊      | 144/384 [00:04<00:04, 57.83it/s]going through batches for holmes training:  39%|███▉      | 150/384 [00:04<00:04, 57.99it/s]going through batches for holmes training:  41%|████      | 156/384 [00:04<00:03, 57.98it/s]going through batches for holmes training:  42%|████▏     | 162/384 [00:04<00:03, 57.95it/s]going through batches for holmes training:  44%|████▍     | 168/384 [00:04<00:03, 57.95it/s]going through batches for holmes training:  45%|████▌     | 174/384 [00:04<00:03, 57.91it/s]going through batches for holmes training:  47%|████▋     | 180/384 [00:04<00:03, 57.70it/s]going through batches for holmes training:  48%|████▊     | 186/384 [00:04<00:03, 57.72it/s]going through batches for holmes training:  50%|█████     | 192/384 [00:05<00:03, 55.15it/s]going through batches for holmes training:  52%|█████▏    | 198/384 [00:05<00:03, 55.37it/s]going through batches for holmes training:  53%|█████▎    | 204/384 [00:05<00:03, 56.01it/s]going through batches for holmes training:  55%|█████▍    | 210/384 [00:05<00:03, 56.62it/s]going through batches for holmes training:  56%|█████▋    | 216/384 [00:05<00:02, 56.96it/s]going through batches for holmes training:  58%|█████▊    | 222/384 [00:05<00:02, 57.24it/s]going through batches for holmes training:  59%|█████▉    | 228/384 [00:05<00:02, 57.37it/s]going through batches for holmes training:  61%|██████    | 234/384 [00:05<00:02, 57.59it/s]going through batches for holmes training:  62%|██████▎   | 240/384 [00:05<00:02, 57.68it/s]going through batches for holmes training:  64%|██████▍   | 246/384 [00:05<00:02, 57.55it/s]going through batches for holmes training:  66%|██████▌   | 252/384 [00:06<00:02, 57.61it/s]going through batches for holmes training:  67%|██████▋   | 258/384 [00:06<00:02, 57.67it/s]going through batches for holmes training:  69%|██████▉   | 264/384 [00:06<00:02, 57.77it/s]going through batches for holmes training:  70%|███████   | 270/384 [00:06<00:01, 57.81it/s]going through batches for holmes training:  72%|███████▏  | 276/384 [00:06<00:01, 57.70it/s]going through batches for holmes training:  73%|███████▎  | 282/384 [00:06<00:01, 57.74it/s]going through batches for holmes training:  75%|███████▌  | 288/384 [00:06<00:01, 57.82it/s]going through batches for holmes training:  77%|███████▋  | 294/384 [00:06<00:01, 57.79it/s]going through batches for holmes training:  78%|███████▊  | 300/384 [00:06<00:01, 57.72it/s]going through batches for holmes training:  80%|███████▉  | 306/384 [00:07<00:01, 57.75it/s]going through batches for holmes training:  81%|████████▏ | 312/384 [00:07<00:01, 57.79it/s]going through batches for holmes training:  83%|████████▎ | 318/384 [00:07<00:01, 57.75it/s]going through batches for holmes training:  84%|████████▍ | 324/384 [00:07<00:01, 57.69it/s]going through batches for holmes training:  86%|████████▌ | 330/384 [00:07<00:00, 57.78it/s]going through batches for holmes training:  88%|████████▊ | 336/384 [00:07<00:00, 57.76it/s]going through batches for holmes training:  89%|████████▉ | 342/384 [00:07<00:00, 57.83it/s]going through batches for holmes training:  91%|█████████ | 348/384 [00:07<00:00, 57.78it/s]going through batches for holmes training:  92%|█████████▏| 354/384 [00:07<00:00, 57.83it/s]going through batches for holmes training:  94%|█████████▍| 360/384 [00:07<00:00, 57.88it/s]going through batches for holmes training:  95%|█████████▌| 366/384 [00:08<00:00, 57.33it/s]going through batches for holmes training:  97%|█████████▋| 372/384 [00:08<00:00, 57.62it/s]going through batches for holmes training:  98%|█████████▊| 378/384 [00:08<00:00, 57.87it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:08<00:00, 58.03it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:08<00:00, 45.77it/s]
loss is 2.746415138244629
loss is 2.633211374282837
loss is 2.637925386428833
loss is 2.6319777965545654
loss is 2.585075616836548
loss is 2.600360631942749
loss is 2.608745813369751
loss is 2.561751127243042
loss is 2.5857226848602295
loss is 2.6168124675750732
loss is 2.588449478149414
loss is 2.5740861892700195
loss is 2.5605597496032715
loss is 2.5665853023529053
loss is 2.555166244506836
loss is 2.522888660430908
loss is 2.5893747806549072
loss is 2.5516507625579834
loss is 2.5156896114349365
loss is 2.5321836471557617
loss is 2.5521979331970215
loss is 2.5511057376861572
loss is 2.523031234741211
loss is 2.5410234928131104
loss is 2.5504324436187744
loss is 2.523710012435913
loss is 2.540884256362915
loss is 2.5442960262298584
loss is 2.5606985092163086
loss is 2.589444160461426
loss is 2.543675422668457
loss is 2.5858778953552246
loss is 2.484323024749756
loss is 2.5494024753570557
loss is 2.5808215141296387
loss is 2.529395818710327
loss is 2.486480951309204
loss is 2.5460469722747803
loss is 2.5577573776245117
loss is 2.4966468811035156
loss is 2.5353012084960938
loss is 2.4974217414855957
loss is 2.5077013969421387
loss is 2.479942560195923
loss is 2.462996244430542
loss is 2.443566083908081
loss is 2.509345293045044
loss is 2.5381722450256348
loss is 2.5018723011016846
loss is 2.5005764961242676
loss is 2.4496381282806396
loss is 2.5137290954589844
loss is 2.544224977493286
loss is 2.5057573318481445
loss is 2.465388059616089
loss is 2.458080291748047
loss is 2.5458180904388428
loss is 2.543347120285034
loss is 2.475731134414673
loss is 2.5096569061279297
loss is 2.475435972213745
loss is 2.462446689605713
loss is 2.4848759174346924
loss is 2.5295569896698
loss is 2.469222068786621
loss is 2.394151210784912
loss is 2.471982717514038
loss is 2.431428909301758
loss is 2.456200122833252
loss is 2.4801807403564453
loss is 2.4683785438537598
loss is 2.487732172012329
loss is 2.483769178390503
loss is 2.4790539741516113
loss is 2.460937738418579
loss is 2.3869102001190186
loss is 2.4511911869049072
loss is 2.425360918045044
loss is 2.5017621517181396
loss is 2.463156223297119
loss is 2.4950520992279053
loss is 2.5232956409454346
loss is 2.5371313095092773
loss is 2.431448221206665
loss is 2.477386474609375
loss is 2.4694368839263916
loss is 2.528927803039551
loss is 2.4607925415039062
loss is 2.4202890396118164
loss is 2.470686912536621
loss is 2.4294745922088623
loss is 2.476433753967285
loss is 2.495020627975464
loss is 2.41184401512146
loss is 2.4425954818725586
loss is 2.4458019733428955
loss is 2.420239210128784
loss is 2.4730100631713867
loss is 2.4702811241149902
loss is 2.3727967739105225
loss is 2.4059009552001953
loss is 2.4356377124786377
loss is 2.4886770248413086
loss is 2.3759140968322754
loss is 2.4237923622131348
loss is 2.455294609069824
loss is 2.4629440307617188
loss is 2.3513572216033936
loss is 2.417433261871338
loss is 2.396512985229492
loss is 2.4143316745758057
loss is 2.391401767730713
loss is 2.3692703247070312
loss is 2.4533679485321045
loss is 2.4373695850372314
loss is 2.429882049560547
loss is 2.4660866260528564
loss is 2.419313669204712
loss is 2.381950855255127
loss is 2.437058448791504
loss is 2.4483530521392822
loss is 2.3566102981567383
loss is 2.3870511054992676
loss is 2.418529748916626
loss is 2.4288766384124756
loss is 2.4061877727508545
loss is 2.392611503601074
loss is 2.3814971446990967
loss is 2.434908390045166
loss is 2.3991713523864746
loss is 2.3703479766845703
loss is 2.386737823486328
loss is 2.4365780353546143
loss is 2.400815963745117
loss is 2.422528028488159
loss is 2.4153456687927246
loss is 2.3976430892944336
loss is 2.373249053955078
loss is 2.4113729000091553
loss is 2.3441083431243896
loss is 2.417081832885742
loss is 2.373366594314575
loss is 2.5246071815490723
loss is 2.4015142917633057
loss is 2.401451826095581
loss is 2.395167589187622
loss is 2.3385918140411377
loss is 2.364682197570801
loss is 2.384768009185791
loss is 2.438082695007324
loss is 2.414750576019287
loss is 2.419691324234009
loss is 2.4080278873443604
loss is 2.455280065536499
loss is 2.389566421508789
loss is 2.4222567081451416
loss is 2.417140245437622
loss is 2.3617899417877197
loss is 2.388239860534668
loss is 2.3988208770751953
loss is 2.380575656890869
loss is 2.421760082244873
loss is 2.3269240856170654
loss is 2.278188705444336
loss is 2.3319592475891113
loss is 2.399022102355957
loss is 2.3658738136291504
loss is 2.3268492221832275
loss is 2.3761744499206543
loss is 2.35386323928833
loss is 2.369917869567871
loss is 2.4216039180755615
loss is 2.419266700744629
loss is 2.374704360961914
loss is 2.3529512882232666
loss is 2.3736746311187744
loss is 2.3572163581848145
loss is 2.402322769165039
loss is 2.43609356880188
loss is 2.4189982414245605
loss is 2.328994035720825
loss is 2.369652509689331
loss is 2.3677639961242676
loss is 2.374634027481079
loss is 2.3092234134674072
loss is 2.4402239322662354
loss is 2.4320478439331055
loss is 2.410303831100464
loss is 2.3769211769104004
loss is 2.3823463916778564
loss is 2.3773276805877686
loss is 2.3421356678009033
loss is 2.290607452392578
loss is 2.3787009716033936
loss is 2.381741523742676
loss is 2.406090497970581
loss is 2.3674182891845703
loss is 2.26998233795166
loss is 2.3014578819274902
loss is 2.418048620223999
loss is 2.364339590072632
loss is 2.301459550857544
loss is 2.3581626415252686
loss is 2.403989553451538
loss is 2.3274195194244385
loss is 2.297492504119873
loss is 2.2519381046295166
loss is 2.3629045486450195
loss is 2.4227771759033203
loss is 2.3521878719329834
loss is 2.332237958908081
loss is 2.2844197750091553
loss is 2.399813175201416
loss is 2.339312791824341
loss is 2.3476264476776123
loss is 2.270124912261963
loss is 2.3404760360717773
loss is 2.3260858058929443
loss is 2.3357763290405273
loss is 2.329928159713745
loss is 2.2752201557159424
loss is 2.2924845218658447
loss is 2.3465559482574463
loss is 2.316269874572754
loss is 2.3391306400299072
loss is 2.268805742263794
loss is 2.271695137023926
loss is 2.3910653591156006
loss is 2.3207831382751465
loss is 2.221404790878296
loss is 2.321375608444214
loss is 2.3265373706817627
loss is 2.311023473739624
loss is 2.3417346477508545
loss is 2.299543857574463
loss is 2.32856822013855
loss is 2.3356456756591797
loss is 2.2938003540039062
loss is 2.3391671180725098
loss is 2.3273987770080566
loss is 2.343658447265625
loss is 2.3654541969299316
loss is 2.3596901893615723
loss is 2.4405219554901123
loss is 2.33807373046875
loss is 2.283052682876587
loss is 2.2944087982177734
loss is 2.2972357273101807
loss is 2.2498271465301514
loss is 2.379474639892578
loss is 2.3144593238830566
loss is 2.296900749206543
loss is 2.3349835872650146
loss is 2.3007216453552246
loss is 2.4041402339935303
loss is 2.286363124847412
loss is 2.3557238578796387
loss is 2.361938238143921
loss is 2.2764525413513184
loss is 2.3324713706970215
loss is 2.2683353424072266
loss is 2.2953221797943115
loss is 2.3042469024658203
loss is 2.3309221267700195
loss is 2.379854679107666
loss is 2.345938205718994
loss is 2.2850143909454346
loss is 2.2677690982818604
loss is 2.341677665710449
loss is 2.2146177291870117
loss is 2.423766613006592
loss is 2.1995272636413574
loss is 2.2354037761688232
loss is 2.2811920642852783
loss is 2.3211188316345215
loss is 2.3706278800964355
loss is 2.3498921394348145
loss is 2.325974941253662
loss is 2.291987657546997
loss is 2.2954158782958984
loss is 2.2810420989990234
loss is 2.3472702503204346
loss is 2.3038454055786133
loss is 2.279881477355957
loss is 2.372326374053955
loss is 2.2799296379089355
loss is 2.3150436878204346
loss is 2.256263017654419
loss is 2.3373870849609375
loss is 2.324270248413086
loss is 2.3472001552581787
loss is 2.2342677116394043
loss is 2.2802908420562744
loss is 2.25689959526062
loss is 2.290644645690918
loss is 2.2928872108459473
loss is 2.309075117111206
loss is 2.318981409072876
loss is 2.268209218978882
loss is 2.350050926208496
loss is 2.268986701965332
loss is 2.3579506874084473
loss is 2.261734962463379
loss is 2.3523995876312256
loss is 2.324270725250244
loss is 2.3969478607177734
loss is 2.3508052825927734
loss is 2.2679150104522705
loss is 2.2283055782318115
loss is 2.300421953201294
loss is 2.344036817550659
loss is 2.3007190227508545
loss is 2.3242409229278564
loss is 2.247892141342163
loss is 2.3393025398254395
loss is 2.3011693954467773
loss is 2.268990993499756
loss is 2.303696393966675
loss is 2.304169178009033
loss is 2.2546231746673584
loss is 2.2415103912353516
loss is 2.209324359893799
loss is 2.310739278793335
loss is 2.2542288303375244
loss is 2.240670680999756
loss is 2.2899515628814697
loss is 2.2976863384246826
loss is 2.3283333778381348
loss is 2.3365478515625
loss is 2.260507583618164
loss is 2.313614845275879
loss is 2.301283359527588
loss is 2.2633860111236572
loss is 2.285801887512207
loss is 2.247032403945923
loss is 2.3626954555511475
loss is 2.2114293575286865
loss is 2.3191285133361816
loss is 2.292332649230957
loss is 2.1794121265411377
loss is 2.252708673477173
loss is 2.2218451499938965
loss is 2.3392434120178223
loss is 2.27750563621521
loss is 2.235089063644409
loss is 2.2247650623321533
loss is 2.299828290939331
loss is 2.3003532886505127
loss is 2.1943886280059814
loss is 2.2364342212677
loss is 2.2241263389587402
loss is 2.2962288856506348
loss is 2.270984411239624
loss is 2.198988676071167
loss is 2.3551225662231445
loss is 2.3206348419189453
loss is 2.307979106903076
loss is 2.3333568572998047
loss is 2.242077112197876
loss is 2.195669174194336
loss is 2.2658040523529053
loss is 2.203221559524536
loss is 2.269901990890503
loss is 2.2304294109344482
loss is 2.3168246746063232
loss is 2.2670929431915283
loss is 2.186009645462036
loss is 2.2505812644958496
loss is 2.2874042987823486
loss is 2.29732084274292
loss is 2.231745958328247
loss is 2.2820258140563965
loss is 2.2023825645446777
loss is 2.233205795288086
loss is 2.240715742111206
loss is 2.2332749366760254
loss is 2.152137041091919
loss is 2.303339719772339
loss is 2.3110342025756836
loss is 2.221623182296753
loss is 2.215498447418213
loss is 2.2529489994049072
loss is 2.160959243774414
loss is 2.2580463886260986
epoch 0: train_loss = 2.378
0: {'Accuracy': 0.3109, 'Precision': 0.3373, 'Recall': 0.3054, 'F1-score': 0.2923}
epoch: 1
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:18,  1.48it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:32, 11.50it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:17, 20.98it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:00<00:12, 29.47it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 36.67it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 42.26it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 46.70it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 49.99it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 52.43it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 54.23it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 55.47it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 56.40it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 57.10it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 57.57it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.97it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 58.16it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 58.32it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 58.40it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 58.49it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 58.57it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 58.55it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.58it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.62it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 58.67it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.67it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:03, 58.58it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.66it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.68it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.70it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.67it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.64it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.59it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:03<00:03, 58.60it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.48it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.56it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.58it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.67it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.63it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.68it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.66it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.55it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.54it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:04<00:02, 58.59it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.58it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.62it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.61it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.66it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.69it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.65it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.60it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.61it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 58.63it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.60it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.58it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 58.60it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.60it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.61it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 58.62it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.62it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.68it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.71it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:06<00:00, 58.34it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 58.56it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.69it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 53.00it/s]
loss is 2.178661584854126
loss is 2.310286283493042
loss is 2.23244047164917
loss is 2.3141562938690186
loss is 2.249444007873535
loss is 2.2927045822143555
loss is 2.2395849227905273
loss is 2.2281908988952637
loss is 2.216709852218628
loss is 2.2053728103637695
loss is 2.2752890586853027
loss is 2.192328691482544
loss is 2.217130184173584
loss is 2.240274429321289
loss is 2.18860125541687
loss is 2.312282085418701
loss is 2.2559707164764404
loss is 2.166351318359375
loss is 2.1997504234313965
loss is 2.270310878753662
loss is 2.1829864978790283
loss is 2.216562271118164
loss is 2.2665531635284424
loss is 2.1917922496795654
loss is 2.2992615699768066
loss is 2.2455551624298096
loss is 2.174095392227173
loss is 2.2234811782836914
loss is 2.231977939605713
loss is 2.296168565750122
loss is 2.1569998264312744
loss is 2.1347150802612305
loss is 2.287137031555176
loss is 2.243441581726074
loss is 2.160140037536621
loss is 2.192619800567627
loss is 2.1782474517822266
loss is 2.213412284851074
loss is 2.2549116611480713
loss is 2.0492467880249023
loss is 2.207106828689575
loss is 2.2546260356903076
loss is 2.2376816272735596
loss is 2.1393625736236572
loss is 2.2362685203552246
loss is 2.211062431335449
loss is 2.2266225814819336
loss is 2.1836719512939453
loss is 2.124509334564209
loss is 2.2374794483184814
loss is 2.123236894607544
loss is 2.232201099395752
loss is 2.1806800365448
loss is 2.2648932933807373
loss is 2.2508749961853027
loss is 2.2086596488952637
loss is 2.1900579929351807
loss is 2.264491319656372
loss is 2.236833095550537
loss is 2.1823537349700928
loss is 2.1585423946380615
loss is 2.1643710136413574
loss is 2.1887919902801514
loss is 2.2774405479431152
loss is 2.227174758911133
loss is 2.189099073410034
loss is 2.2775259017944336
loss is 2.176671266555786
loss is 2.2315030097961426
loss is 2.1682863235473633
loss is 2.232165813446045
loss is 2.2426652908325195
loss is 2.245649814605713
loss is 2.1538002490997314
loss is 2.1497883796691895
loss is 2.0766358375549316
loss is 2.1643667221069336
loss is 2.2405343055725098
loss is 2.142514228820801
loss is 2.2207558155059814
loss is 2.2238173484802246
loss is 2.2097983360290527
loss is 2.1670594215393066
loss is 2.160461187362671
loss is 2.1847314834594727
loss is 2.186213493347168
loss is 2.1503000259399414
loss is 2.1847684383392334
loss is 2.107729434967041
loss is 2.2624597549438477
loss is 2.0847930908203125
loss is 2.2737724781036377
loss is 2.174020528793335
loss is 2.2049331665039062
loss is 2.1296427249908447
loss is 2.175826072692871
loss is 2.2583959102630615
loss is 2.154383897781372
loss is 2.176378011703491
loss is 2.1737077236175537
loss is 2.1960372924804688
loss is 2.1191511154174805
loss is 2.1533539295196533
loss is 2.153041362762451
loss is 2.2038702964782715
loss is 2.187730312347412
loss is 2.1699235439300537
loss is 2.244422197341919
loss is 2.182772397994995
loss is 2.1546781063079834
loss is 2.1004550457000732
loss is 2.289973735809326
loss is 2.2125613689422607
loss is 2.197869300842285
loss is 2.1956279277801514
loss is 2.2081611156463623
loss is 2.1569607257843018
loss is 2.2072386741638184
loss is 2.1632883548736572
loss is 2.183953046798706
loss is 2.2250137329101562
loss is 2.1399893760681152
loss is 2.1827590465545654
loss is 2.149151563644409
loss is 2.202958345413208
loss is 2.154203414916992
loss is 2.1306235790252686
loss is 2.258192539215088
loss is 2.1285953521728516
loss is 2.203618049621582
loss is 2.0853631496429443
loss is 2.17048716545105
loss is 2.1934001445770264
loss is 2.1930947303771973
loss is 2.1555967330932617
loss is 2.148444414138794
loss is 2.231635570526123
loss is 2.2061500549316406
loss is 2.135087251663208
loss is 2.1632046699523926
loss is 2.108154058456421
loss is 2.1803174018859863
loss is 2.208949089050293
loss is 2.202073335647583
loss is 2.1941142082214355
loss is 2.111682176589966
loss is 2.208974838256836
loss is 2.120929718017578
loss is 2.1735024452209473
loss is 2.2311880588531494
loss is 2.087549924850464
loss is 2.1674599647521973
loss is 2.1311442852020264
loss is 2.1324222087860107
loss is 2.1501452922821045
loss is 2.129441261291504
loss is 2.1466996669769287
loss is 2.190185308456421
loss is 2.160320281982422
loss is 2.148071527481079
loss is 2.1237223148345947
loss is 2.110384702682495
loss is 2.043327569961548
loss is 2.1594607830047607
loss is 2.168142795562744
loss is 2.180485725402832
loss is 2.2745087146759033
loss is 2.245699882507324
loss is 2.076080799102783
loss is 2.17238712310791
loss is 2.2089226245880127
loss is 2.187661647796631
loss is 2.1021130084991455
loss is 2.175015449523926
loss is 2.0913047790527344
loss is 2.1023828983306885
loss is 2.1254959106445312
loss is 2.1785871982574463
loss is 2.1239333152770996
loss is 2.1939687728881836
loss is 2.0999677181243896
loss is 2.1286425590515137
loss is 2.2002153396606445
loss is 2.187803030014038
loss is 2.196107864379883
loss is 2.086484432220459
loss is 2.1288363933563232
loss is 2.123878240585327
loss is 2.0699801445007324
loss is 2.0115063190460205
loss is 2.058572769165039
loss is 2.0198421478271484
loss is 2.13206148147583
loss is 2.1101250648498535
loss is 2.092785596847534
loss is 2.1362884044647217
loss is 2.1382973194122314
loss is 2.0810370445251465
loss is 2.12764310836792
loss is 2.1293325424194336
loss is 2.044584035873413
loss is 1.97661554813385
loss is 2.0809943675994873
loss is 2.0608413219451904
loss is 2.0578806400299072
loss is 2.1603641510009766
loss is 2.0328104496002197
loss is 2.1683123111724854
loss is 2.023451805114746
loss is 2.121821165084839
loss is 2.1341135501861572
loss is 2.135723114013672
loss is 2.137037515640259
loss is 2.143638849258423
loss is 2.0772271156311035
loss is 2.145737409591675
loss is 2.142442226409912
loss is 2.1328988075256348
loss is 2.183753490447998
loss is 2.1274640560150146
loss is 2.160815477371216
loss is 2.1355926990509033
loss is 2.1150200366973877
loss is 2.0905802249908447
loss is 1.992761254310608
loss is 2.1287407875061035
loss is 2.0404653549194336
loss is 2.158879280090332
loss is 2.2185842990875244
loss is 2.1512513160705566
loss is 2.077064037322998
loss is 2.157416343688965
loss is 2.1473798751831055
loss is 2.118281841278076
loss is 2.1659932136535645
loss is 2.1422834396362305
loss is 2.1393330097198486
loss is 2.1115262508392334
loss is 2.162508726119995
loss is 2.158430814743042
loss is 1.9736632108688354
loss is 2.0503149032592773
loss is 2.071929454803467
loss is 2.0833005905151367
loss is 2.0361974239349365
loss is 2.123696804046631
loss is 2.0085666179656982
loss is 2.1640403270721436
loss is 2.0722951889038086
loss is 2.107673406600952
loss is 2.1086275577545166
loss is 2.044767379760742
loss is 2.112197160720825
loss is 2.1630327701568604
loss is 2.1755123138427734
loss is 2.063929796218872
loss is 2.1733531951904297
loss is 2.06581449508667
loss is 2.1455912590026855
loss is 2.1662895679473877
loss is 2.007697343826294
loss is 2.1786949634552
loss is 2.1559088230133057
loss is 2.0937912464141846
loss is 2.047473669052124
loss is 2.1032910346984863
loss is 2.161912679672241
loss is 2.1497373580932617
loss is 2.1505157947540283
loss is 2.0895628929138184
loss is 2.068091869354248
loss is 2.1261987686157227
loss is 1.9799792766571045
loss is 2.1307551860809326
loss is 2.158374547958374
loss is 2.062410831451416
loss is 2.0734808444976807
loss is 2.091017723083496
loss is 2.1487174034118652
loss is 2.186426877975464
loss is 2.0625672340393066
loss is 2.039205551147461
loss is 2.064152717590332
loss is 2.1160624027252197
loss is 2.1266136169433594
loss is 2.048511266708374
loss is 2.0682475566864014
loss is 2.0995748043060303
loss is 2.071498394012451
loss is 2.0691308975219727
loss is 2.1438581943511963
loss is 2.112993001937866
loss is 2.2148406505584717
loss is 2.0745725631713867
loss is 2.038691520690918
loss is 2.1564645767211914
loss is 2.1035656929016113
loss is 2.1119189262390137
loss is 2.122715711593628
loss is 2.0405220985412598
loss is 2.075648307800293
loss is 2.1749634742736816
loss is 2.0200533866882324
loss is 2.06526255607605
loss is 2.0958948135375977
loss is 2.1439244747161865
loss is 2.0305562019348145
loss is 2.0803542137145996
loss is 2.0900633335113525
loss is 2.1141154766082764
loss is 2.0833709239959717
loss is 2.0481317043304443
loss is 2.1563022136688232
loss is 2.093932628631592
loss is 2.084766149520874
loss is 2.0785083770751953
loss is 2.1932408809661865
loss is 2.107701301574707
loss is 2.04621958732605
loss is 2.072349786758423
loss is 2.048311233520508
loss is 2.017618417739868
loss is 2.0178847312927246
loss is 2.108278274536133
loss is 2.0465035438537598
loss is 2.0510072708129883
loss is 2.1293859481811523
loss is 2.1326868534088135
loss is 2.041018486022949
loss is 2.068920373916626
loss is 2.014763832092285
loss is 2.1640994548797607
loss is 2.109290599822998
loss is 2.0870509147644043
loss is 2.0466196537017822
loss is 2.0762546062469482
loss is 2.0564191341400146
loss is 2.1077654361724854
loss is 2.098752975463867
loss is 2.0611989498138428
loss is 2.082612991333008
loss is 2.000728130340576
loss is 2.0287256240844727
loss is 2.0645198822021484
loss is 2.0415875911712646
loss is 2.1414334774017334
loss is 2.1358237266540527
loss is 2.149292230606079
loss is 2.073005437850952
loss is 2.124213457107544
loss is 2.064215898513794
loss is 2.0440146923065186
loss is 2.0321760177612305
loss is 2.091850996017456
loss is 2.055659532546997
loss is 2.0763039588928223
loss is 2.0676944255828857
loss is 2.0663957595825195
loss is 1.9988783597946167
loss is 2.100825786590576
loss is 2.0383477210998535
loss is 2.1077563762664795
loss is 2.0336010456085205
loss is 1.952214002609253
loss is 2.058530807495117
loss is 2.107722520828247
loss is 2.042645215988159
loss is 2.0844621658325195
loss is 2.090822696685791
loss is 1.9919973611831665
loss is 2.091630458831787
loss is 2.088066816329956
loss is 2.0712156295776367
loss is 2.0522639751434326
loss is 2.004110336303711
loss is 1.9652742147445679
loss is 2.0234501361846924
loss is 1.9390445947647095
loss is 2.0633115768432617
loss is 2.0081090927124023
loss is 2.015225410461426
loss is 2.062540292739868
loss is 2.0002224445343018
loss is 1.9232113361358643
epoch 1: train_loss = 2.137
1: {'Accuracy': 0.3864, 'Precision': 0.4024, 'Recall': 0.3787, 'F1-score': 0.3779}
epoch: 2
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<03:42,  1.72it/s]going through batches for holmes training:   1%|          | 3/384 [00:00<01:14,  5.13it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:30, 12.40it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:15, 23.22it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:11, 32.27it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 39.49it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:07, 44.75it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 48.63it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 51.50it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 53.60it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:05, 55.16it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 56.22it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 56.86it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 57.41it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 57.75it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.99it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 58.26it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:05, 55.36it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 56.37it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.04it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.51it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.87it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.19it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.29it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 58.38it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.50it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:03, 58.58it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.57it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.64it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.70it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.70it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.64it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.69it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:03<00:03, 58.74it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.69it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.69it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.69it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.70it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.59it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.58it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.67it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.68it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.62it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 58.71it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.67it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.72it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.41it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.20it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 57.94it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 57.84it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 57.80it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 57.75it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 57.68it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 57.79it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 57.76it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 57.72it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 57.80it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 57.74it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 57.68it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 57.90it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.06it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.27it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:06<00:00, 57.94it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 58.27it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.46it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.51it/s]
loss is 2.05212140083313
loss is 1.9571605920791626
loss is 2.0397205352783203
loss is 2.0748467445373535
loss is 1.9640768766403198
loss is 2.0909171104431152
loss is 2.026782274246216
loss is 2.0532217025756836
loss is 2.0459535121917725
loss is 2.1419589519500732
loss is 1.970107913017273
loss is 2.086174726486206
loss is 2.0668015480041504
loss is 2.045942783355713
loss is 1.996427297592163
loss is 1.964491844177246
loss is 2.0698533058166504
loss is 1.9617255926132202
loss is 2.0337352752685547
loss is 1.9640474319458008
loss is 2.0348832607269287
loss is 2.0654873847961426
loss is 1.9964630603790283
loss is 2.06904673576355
loss is 2.081393241882324
loss is 2.0215423107147217
loss is 2.0513787269592285
loss is 1.98537015914917
loss is 1.9856778383255005
loss is 1.9536312818527222
loss is 2.0049233436584473
loss is 2.0005834102630615
loss is 2.102463722229004
loss is 1.9763803482055664
loss is 2.0905866622924805
loss is 2.123232364654541
loss is 2.1130189895629883
loss is 2.0421204566955566
loss is 2.047407388687134
loss is 2.0180914402008057
loss is 2.016836166381836
loss is 2.0511889457702637
loss is 2.065248966217041
loss is 2.01210880279541
loss is 1.9912549257278442
loss is 1.971509575843811
loss is 2.0156288146972656
loss is 2.0822694301605225
loss is 2.0306851863861084
loss is 2.107851982116699
loss is 1.9962546825408936
loss is 1.9788503646850586
loss is 1.9998213052749634
loss is 1.9820157289505005
loss is 1.9969000816345215
loss is 1.938408613204956
loss is 2.0339622497558594
loss is 1.9850172996520996
loss is 2.076634168624878
loss is 2.065919876098633
loss is 1.9915642738342285
loss is 2.000542640686035
loss is 1.9284802675247192
loss is 1.9795317649841309
loss is 2.0654516220092773
loss is 2.052412748336792
loss is 2.053528070449829
loss is 2.0585687160491943
loss is 2.014704704284668
loss is 2.063690185546875
loss is 1.9831743240356445
loss is 1.8964060544967651
loss is 2.0358970165252686
loss is 2.047410011291504
loss is 2.0385735034942627
loss is 2.065666437149048
loss is 2.0708467960357666
loss is 2.1064634323120117
loss is 1.936282753944397
loss is 2.0667366981506348
loss is 2.0123088359832764
loss is 1.9867972135543823
loss is 1.9482345581054688
loss is 1.9876389503479004
loss is 2.0419321060180664
loss is 2.1096630096435547
loss is 2.010570764541626
loss is 2.0489280223846436
loss is 1.9995349645614624
loss is 1.9449338912963867
loss is 2.024805784225464
loss is 2.024156093597412
loss is 1.9690088033676147
loss is 2.097780466079712
loss is 2.0322518348693848
loss is 1.9613540172576904
loss is 1.9960205554962158
loss is 2.0602645874023438
loss is 1.9486448764801025
loss is 1.9364609718322754
loss is 2.080934524536133
loss is 2.0375144481658936
loss is 2.0063772201538086
loss is 2.053623914718628
loss is 1.9790267944335938
loss is 2.069535255432129
loss is 2.010209083557129
loss is 2.0313925743103027
loss is 1.8906391859054565
loss is 2.0170176029205322
loss is 1.9677047729492188
loss is 1.9104541540145874
loss is 2.059558629989624
loss is 1.9588253498077393
loss is 1.996768593788147
loss is 2.1675219535827637
loss is 2.053417921066284
loss is 1.8700238466262817
loss is 1.9212678670883179
loss is 1.9334187507629395
loss is 2.0097150802612305
loss is 2.026873826980591
loss is 2.0527756214141846
loss is 1.9984030723571777
loss is 2.0503478050231934
loss is 2.0105860233306885
loss is 2.004422664642334
loss is 2.03033185005188
loss is 1.939782977104187
loss is 2.0459861755371094
loss is 1.9598350524902344
loss is 1.9255871772766113
loss is 2.0280725955963135
loss is 1.9595826864242554
loss is 1.9370944499969482
loss is 1.9915257692337036
loss is 2.0839626789093018
loss is 1.9358209371566772
loss is 2.060959577560425
loss is 1.9813156127929688
loss is 1.9787887334823608
loss is 1.936371922492981
loss is 2.0941829681396484
loss is 2.0469045639038086
loss is 2.0434916019439697
loss is 2.0764973163604736
loss is 2.01861834526062
loss is 2.0441722869873047
loss is 1.9359807968139648
loss is 2.0668463706970215
loss is 2.040402412414551
loss is 2.0066356658935547
loss is 1.924971580505371
loss is 1.9761358499526978
loss is 1.9763991832733154
loss is 2.0150046348571777
loss is 1.9918289184570312
loss is 1.9714646339416504
loss is 1.903730869293213
loss is 2.0435476303100586
loss is 2.0141425132751465
loss is 2.016761064529419
loss is 2.0461268424987793
loss is 1.8899012804031372
loss is 2.0137693881988525
loss is 2.01914119720459
loss is 1.9814969301223755
loss is 1.901920199394226
loss is 1.9576562643051147
loss is 1.8873668909072876
loss is 1.9595489501953125
loss is 1.9749702215194702
loss is 1.9201109409332275
loss is 1.9124242067337036
loss is 1.917590618133545
loss is 1.9833980798721313
loss is 2.015277862548828
loss is 1.9935113191604614
loss is 1.8973349332809448
loss is 1.9451919794082642
loss is 1.990118384361267
loss is 2.037825584411621
loss is 1.9346507787704468
loss is 1.963181495666504
loss is 1.9683078527450562
loss is 1.9480338096618652
loss is 1.9385908842086792
loss is 1.9925318956375122
loss is 1.981046438217163
loss is 2.017665147781372
loss is 1.9531010389328003
loss is 1.9970710277557373
loss is 1.9626764059066772
loss is 1.9560166597366333
loss is 1.990842342376709
loss is 2.0847465991973877
loss is 1.9278658628463745
loss is 1.9961451292037964
loss is 1.8581756353378296
loss is 2.04376220703125
loss is 1.988139033317566
loss is 2.06984281539917
loss is 1.9481587409973145
loss is 1.8272030353546143
loss is 1.9702115058898926
loss is 2.0238726139068604
loss is 2.046898365020752
loss is 1.9172751903533936
loss is 1.9841742515563965
loss is 1.9191612005233765
loss is 1.9762027263641357
loss is 2.0026655197143555
loss is 2.0258634090423584
loss is 1.9268434047698975
loss is 1.9691433906555176
loss is 2.03639554977417
loss is 1.9461745023727417
loss is 1.9104951620101929
loss is 2.114354133605957
loss is 1.9541124105453491
loss is 1.8968501091003418
loss is 2.023787260055542
loss is 2.0090866088867188
loss is 2.042344331741333
loss is 2.0238568782806396
loss is 1.950208306312561
loss is 1.8295940160751343
loss is 2.007596492767334
loss is 2.030256509780884
loss is 2.029242515563965
loss is 1.8800721168518066
loss is 1.9144517183303833
loss is 2.008270263671875
loss is 1.955656886100769
loss is 1.9190820455551147
loss is 1.9768048524856567
loss is 1.9963951110839844
loss is 1.9306855201721191
loss is 1.9838236570358276
loss is 1.9394176006317139
loss is 2.0124101638793945
loss is 1.9133561849594116
loss is 2.0371298789978027
loss is 1.9883579015731812
loss is 2.0651285648345947
loss is 1.9656819105148315
loss is 1.8957768678665161
loss is 1.891478419303894
loss is 1.997737169265747
loss is 1.909342646598816
loss is 1.9207605123519897
loss is 1.7801226377487183
loss is 1.9479478597640991
loss is 1.9959716796875
loss is 1.9042819738388062
loss is 1.9496371746063232
loss is 1.9245790243148804
loss is 1.9435725212097168
loss is 1.9940954446792603
loss is 1.8048678636550903
loss is 1.8511656522750854
loss is 1.9057012796401978
loss is 1.9630507230758667
loss is 2.073814630508423
loss is 1.8687329292297363
loss is 1.9018245935440063
loss is 1.9089001417160034
loss is 1.9260430335998535
loss is 1.981088399887085
loss is 1.9158068895339966
loss is 1.9596524238586426
loss is 1.9994301795959473
loss is 1.876304268836975
loss is 2.0803093910217285
loss is 1.9676073789596558
loss is 2.0220632553100586
loss is 1.914230465888977
loss is 1.8982739448547363
loss is 2.0513181686401367
loss is 1.9064898490905762
loss is 1.9195544719696045
loss is 1.961588740348816
loss is 1.829179048538208
loss is 2.002209424972534
loss is 1.8593146800994873
loss is 1.9216032028198242
loss is 1.9578263759613037
loss is 1.887089490890503
loss is 1.9333003759384155
loss is 2.0495166778564453
loss is 1.9277513027191162
loss is 1.9304617643356323
loss is 1.9107425212860107
loss is 1.9607893228530884
loss is 1.9551218748092651
loss is 1.838721752166748
loss is 1.9392898082733154
loss is 1.8191932439804077
loss is 1.9208858013153076
loss is 1.8312160968780518
loss is 1.913144588470459
loss is 1.927341103553772
loss is 1.868046760559082
loss is 1.9953720569610596
loss is 1.8206275701522827
loss is 1.9306464195251465
loss is 1.9471752643585205
loss is 1.928723931312561
loss is 1.946466088294983
loss is 1.899001121520996
loss is 1.9741843938827515
loss is 1.935368537902832
loss is 1.8446465730667114
loss is 1.9769684076309204
loss is 1.9360219240188599
loss is 1.9784388542175293
loss is 1.9165339469909668
loss is 1.9851347208023071
loss is 1.972750186920166
loss is 1.9893854856491089
loss is 1.9578487873077393
loss is 2.032029628753662
loss is 2.035358190536499
loss is 2.0377848148345947
loss is 1.987676978111267
loss is 1.9543633460998535
loss is 1.932093858718872
loss is 1.8808070421218872
loss is 2.095954656600952
loss is 1.8258390426635742
loss is 1.9460502862930298
loss is 1.9083861112594604
loss is 1.9365254640579224
loss is 1.9284440279006958
loss is 1.957373857498169
loss is 1.895472526550293
loss is 1.924380898475647
loss is 2.013432502746582
loss is 2.0158607959747314
loss is 1.8386895656585693
loss is 1.9654098749160767
loss is 1.8905844688415527
loss is 1.9028805494308472
loss is 1.8743892908096313
loss is 1.947701096534729
loss is 1.8665210008621216
loss is 1.9122722148895264
loss is 1.8815124034881592
loss is 1.9654704332351685
loss is 1.9901490211486816
loss is 1.8911114931106567
loss is 1.8588403463363647
loss is 1.8863192796707153
loss is 1.9383960962295532
loss is 2.0122780799865723
loss is 2.0398190021514893
loss is 2.0102274417877197
loss is 1.956984043121338
loss is 1.954054594039917
loss is 1.8988293409347534
loss is 2.0403449535369873
loss is 1.9467312097549438
loss is 1.9206362962722778
loss is 1.8880088329315186
loss is 1.9018155336380005
loss is 1.8631188869476318
loss is 1.9431676864624023
loss is 2.004297971725464
loss is 1.9139888286590576
loss is 1.8724067211151123
loss is 1.8808658123016357
loss is 1.8345844745635986
loss is 1.955496072769165
loss is 1.9514877796173096
loss is 1.8355106115341187
loss is 1.949110984802246
loss is 1.940037488937378
loss is 1.8921786546707153
loss is 1.9322004318237305
loss is 1.8702516555786133
loss is 2.025141954421997
loss is 1.9103765487670898
loss is 1.917639970779419
loss is 1.8536845445632935
epoch 2: train_loss = 1.976
2: {'Accuracy': 0.4131, 'Precision': 0.434, 'Recall': 0.4099, 'F1-score': 0.4033}
epoch: 3
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<05:20,  1.20it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:39,  9.64it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:01<00:20, 18.13it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:13, 26.22it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:10, 33.35it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 39.27it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 44.10it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:07, 47.83it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 50.65it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 52.68it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 54.25it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 55.32it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:02<00:05, 56.14it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.68it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.08it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.42it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 57.52it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.61it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.72it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.77it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.83it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:03<00:04, 57.84it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:03<00:04, 57.91it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 57.99it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 57.99it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 57.96it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 57.90it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 57.91it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 57.98it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 57.98it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.03it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:04<00:03, 58.06it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 58.01it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.03it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 57.95it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:03, 57.48it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 57.62it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 57.60it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 57.73it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 57.81it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 57.64it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:05<00:02, 57.70it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 57.79it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 57.83it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 57.95it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 57.91it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 57.94it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 57.97it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 57.95it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 57.87it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:06<00:01, 57.92it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:06<00:01, 57.91it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 57.97it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 57.94it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 57.98it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.05it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 57.94it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 57.97it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 57.93it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 57.93it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:07<00:00, 57.95it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 57.23it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 57.47it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 57.67it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 51.14it/s]
loss is 1.8796148300170898
loss is 1.9538651704788208
loss is 1.89168381690979
loss is 1.9244258403778076
loss is 1.7464970350265503
loss is 1.8838924169540405
loss is 1.8757115602493286
loss is 1.9388203620910645
loss is 1.8405675888061523
loss is 1.8987183570861816
loss is 1.8430367708206177
loss is 1.8749725818634033
loss is 1.8256890773773193
loss is 1.8674323558807373
loss is 1.9261674880981445
loss is 1.839945673942566
loss is 1.9804257154464722
loss is 1.9691864252090454
loss is 1.9205669164657593
loss is 1.919177532196045
loss is 1.9382634162902832
loss is 1.7954614162445068
loss is 1.8895832300186157
loss is 1.9330999851226807
loss is 1.9420257806777954
loss is 1.9179404973983765
loss is 1.9258655309677124
loss is 1.8623101711273193
loss is 1.9407892227172852
loss is 1.9019062519073486
loss is 1.8842520713806152
loss is 1.9596513509750366
loss is 1.8858859539031982
loss is 1.9847809076309204
loss is 1.9177465438842773
loss is 1.8731106519699097
loss is 1.8726905584335327
loss is 1.8725237846374512
loss is 1.882340669631958
loss is 1.8882200717926025
loss is 1.8899863958358765
loss is 1.893580675125122
loss is 1.9462162256240845
loss is 1.8445336818695068
loss is 1.8650192022323608
loss is 2.017273187637329
loss is 1.8854279518127441
loss is 1.8567092418670654
loss is 1.8384714126586914
loss is 1.9148615598678589
loss is 1.8932057619094849
loss is 1.8491631746292114
loss is 1.9352020025253296
loss is 1.872738003730774
loss is 1.965944766998291
loss is 1.90397310256958
loss is 1.9099781513214111
loss is 1.9867020845413208
loss is 1.8520922660827637
loss is 1.8495882749557495
loss is 1.9112119674682617
loss is 1.8744016885757446
loss is 1.8892207145690918
loss is 1.8824243545532227
loss is 1.8986990451812744
loss is 1.9418902397155762
loss is 1.8682007789611816
loss is 1.9324145317077637
loss is 1.9552639722824097
loss is 1.9320576190948486
loss is 1.8474135398864746
loss is 1.8788708448410034
loss is 1.8301341533660889
loss is 1.8418707847595215
loss is 1.9105284214019775
loss is 1.8745421171188354
loss is 1.9302184581756592
loss is 1.8435869216918945
loss is 1.805189847946167
loss is 1.9020090103149414
loss is 1.8684451580047607
loss is 1.8116004467010498
loss is 1.926910400390625
loss is 1.936203122138977
loss is 1.9370334148406982
loss is 1.8876742124557495
loss is 1.888358473777771
loss is 1.8146930932998657
loss is 1.8833595514297485
loss is 1.8935978412628174
loss is 2.006450653076172
loss is 1.839913010597229
loss is 1.852241039276123
loss is 1.8737406730651855
loss is 1.7631794214248657
loss is 1.7760242223739624
loss is 1.8616160154342651
loss is 1.882960319519043
loss is 1.9054481983184814
loss is 1.8299641609191895
loss is 1.9131649732589722
loss is 1.8303338289260864
loss is 1.8861021995544434
loss is 1.8151109218597412
loss is 1.8030385971069336
loss is 1.8210690021514893
loss is 1.9428176879882812
loss is 1.8465064764022827
loss is 1.8314770460128784
loss is 1.9228509664535522
loss is 1.858540654182434
loss is 1.8028383255004883
loss is 1.816146969795227
loss is 1.7790926694869995
loss is 1.8494051694869995
loss is 1.9398279190063477
loss is 1.7764198780059814
loss is 1.810316801071167
loss is 1.7778822183609009
loss is 1.9035450220108032
loss is 1.8934919834136963
loss is 1.8536320924758911
loss is 1.9451199769973755
loss is 1.7951078414916992
loss is 1.853327751159668
loss is 1.9424346685409546
loss is 1.931045413017273
loss is 1.9335919618606567
loss is 1.797340989112854
loss is 1.8383662700653076
loss is 1.9298655986785889
loss is 1.8843553066253662
loss is 1.7964063882827759
loss is 1.8051725625991821
loss is 1.935388445854187
loss is 1.9262617826461792
loss is 1.8128142356872559
loss is 1.9110596179962158
loss is 1.8260550498962402
loss is 1.914588451385498
loss is 1.8383209705352783
loss is 1.7839961051940918
loss is 1.8903766870498657
loss is 1.8328396081924438
loss is 1.774390459060669
loss is 1.9039257764816284
loss is 1.778030276298523
loss is 1.8573843240737915
loss is 1.7805935144424438
loss is 1.8117413520812988
loss is 1.8913570642471313
loss is 1.8203530311584473
loss is 1.8503828048706055
loss is 1.8409687280654907
loss is 1.9451959133148193
loss is 1.7611973285675049
loss is 1.8171690702438354
loss is 1.8522049188613892
loss is 1.8215446472167969
loss is 1.874524712562561
loss is 1.7489465475082397
loss is 1.841201663017273
loss is 1.9768394231796265
loss is 1.8597122430801392
loss is 1.9394164085388184
loss is 1.9127310514450073
loss is 1.8573089838027954
loss is 1.8305079936981201
loss is 1.8689557313919067
loss is 1.9616692066192627
loss is 1.8623753786087036
loss is 1.9095947742462158
loss is 1.9246066808700562
loss is 1.8360466957092285
loss is 1.8467342853546143
loss is 1.9373037815093994
loss is 1.8612449169158936
loss is 1.8404030799865723
loss is 1.8196282386779785
loss is 1.8137933015823364
loss is 1.9288581609725952
loss is 1.855256199836731
loss is 1.8115299940109253
loss is 1.7713356018066406
loss is 1.8278656005859375
loss is 1.8050554990768433
loss is 1.6805812120437622
loss is 1.849489688873291
loss is 1.870882272720337
loss is 1.9085502624511719
loss is 1.8168636560440063
loss is 1.784909725189209
loss is 1.798936367034912
loss is 1.8914581537246704
loss is 1.8129150867462158
loss is 1.930192232131958
loss is 1.80031418800354
loss is 1.8929649591445923
loss is 1.8854129314422607
loss is 1.7427411079406738
loss is 1.8321934938430786
loss is 1.894721269607544
loss is 1.858015775680542
loss is 1.8572871685028076
loss is 1.9016308784484863
loss is 1.8346545696258545
loss is 1.892662763595581
loss is 1.8654632568359375
loss is 1.8886219263076782
loss is 1.8562228679656982
loss is 1.7866836786270142
loss is 1.7957528829574585
loss is 1.8311494588851929
loss is 1.916115403175354
loss is 1.8825008869171143
loss is 1.8369158506393433
loss is 1.823742389678955
loss is 1.8474929332733154
loss is 1.7822470664978027
loss is 1.7416284084320068
loss is 1.8020291328430176
loss is 1.8167823553085327
loss is 1.882550835609436
loss is 1.7934396266937256
loss is 1.861241102218628
loss is 1.83199143409729
loss is 1.7586363554000854
loss is 1.8362131118774414
loss is 1.8335258960723877
loss is 1.732529878616333
loss is 1.7875866889953613
loss is 1.8458061218261719
loss is 1.7996972799301147
loss is 1.931673526763916
loss is 1.836879014968872
loss is 1.922408938407898
loss is 1.7739458084106445
loss is 1.7272658348083496
loss is 1.8280885219573975
loss is 1.8014167547225952
loss is 1.7761050462722778
loss is 1.7995747327804565
loss is 1.847902536392212
loss is 1.8432377576828003
loss is 1.7307125329971313
loss is 1.7884896993637085
loss is 1.782055377960205
loss is 1.7738351821899414
loss is 1.8466938734054565
loss is 1.756445288658142
loss is 1.7675409317016602
loss is 1.8263306617736816
loss is 1.9703772068023682
loss is 1.8908178806304932
loss is 1.8040930032730103
loss is 1.7609639167785645
loss is 1.7901579141616821
loss is 1.844089150428772
loss is 1.8221226930618286
loss is 1.7771741151809692
loss is 1.8084564208984375
loss is 1.949591040611267
loss is 1.8740448951721191
loss is 1.9227094650268555
loss is 1.8313889503479004
loss is 1.8626044988632202
loss is 1.8538225889205933
loss is 1.8821396827697754
loss is 1.8758444786071777
loss is 1.8295164108276367
loss is 1.7837803363800049
loss is 1.799791693687439
loss is 1.926865816116333
loss is 1.7930314540863037
loss is 1.8728278875350952
loss is 1.8008366823196411
loss is 1.8175976276397705
loss is 1.937563180923462
loss is 1.9024229049682617
loss is 1.792956829071045
loss is 1.8124642372131348
loss is 1.7646211385726929
loss is 1.828036904335022
loss is 1.773288369178772
loss is 1.8404594659805298
loss is 1.7725261449813843
loss is 1.8432683944702148
loss is 1.8128290176391602
loss is 1.7736400365829468
loss is 1.8415205478668213
loss is 1.9193377494812012
loss is 1.7547962665557861
loss is 1.7570239305496216
loss is 1.8147659301757812
loss is 1.9409619569778442
loss is 1.770960807800293
loss is 1.7647758722305298
loss is 1.8013328313827515
loss is 1.7999088764190674
loss is 1.8423930406570435
loss is 1.8642092943191528
loss is 1.8204896450042725
loss is 1.8281524181365967
loss is 1.863797903060913
loss is 1.728782296180725
loss is 1.8449989557266235
loss is 1.7628158330917358
loss is 1.7965798377990723
loss is 1.788723111152649
loss is 1.8075921535491943
loss is 1.7761741876602173
loss is 1.7702637910842896
loss is 1.7568275928497314
loss is 1.7387323379516602
loss is 1.8363744020462036
loss is 1.8728578090667725
loss is 1.8705530166625977
loss is 1.8752589225769043
loss is 1.7415533065795898
loss is 1.7680021524429321
loss is 1.8074487447738647
loss is 1.8261195421218872
loss is 1.866434931755066
loss is 1.8566595315933228
loss is 1.8500335216522217
loss is 1.9011229276657104
loss is 1.8913322687149048
loss is 1.852423906326294
loss is 1.8243097066879272
loss is 1.8862282037734985
loss is 1.8189316987991333
loss is 1.8076969385147095
loss is 1.8088364601135254
loss is 1.7534469366073608
loss is 1.8773273229599
loss is 1.9200786352157593
loss is 1.822884202003479
loss is 1.8451685905456543
loss is 1.8627312183380127
loss is 1.8574023246765137
loss is 1.8699089288711548
loss is 1.7881284952163696
loss is 1.7854995727539062
loss is 1.847834825515747
loss is 1.7567754983901978
loss is 1.8065829277038574
loss is 1.7783379554748535
loss is 1.8182862997055054
loss is 1.783301830291748
loss is 1.8274297714233398
loss is 1.801742672920227
loss is 1.8499716520309448
loss is 1.7203788757324219
loss is 1.8073375225067139
loss is 1.7535327672958374
loss is 1.9154250621795654
loss is 1.8175835609436035
loss is 1.7657647132873535
loss is 1.9296324253082275
loss is 1.8828866481781006
loss is 1.7741786241531372
loss is 1.7265524864196777
loss is 1.8636527061462402
loss is 1.6956044435501099
loss is 1.8101924657821655
loss is 1.7220343351364136
loss is 1.726755976676941
loss is 1.7365130186080933
loss is 1.7988797426223755
loss is 1.7763094902038574
loss is 1.8140150308609009
loss is 1.7805616855621338
loss is 1.9031201601028442
loss is 1.8084477186203003
loss is 1.9263792037963867
loss is 1.7802882194519043
loss is 1.811976671218872
loss is 1.8327038288116455
loss is 1.795789361000061
loss is 1.8173484802246094
loss is 1.8546580076217651
loss is 1.8156565427780151
loss is 1.779005527496338
loss is 1.832192063331604
epoch 3: train_loss = 1.848
3: {'Accuracy': 0.4363, 'Precision': 0.4399, 'Recall': 0.4295, 'F1-score': 0.4216}
epoch: 4
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:33,  1.40it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:33, 11.13it/s]going through batches for holmes training:   4%|▎         | 14/384 [00:00<00:17, 21.75it/s]going through batches for holmes training:   5%|▌         | 20/384 [00:01<00:12, 29.78it/s]going through batches for holmes training:   7%|▋         | 26/384 [00:01<00:09, 36.55it/s]going through batches for holmes training:   8%|▊         | 32/384 [00:01<00:08, 42.11it/s]going through batches for holmes training:  10%|▉         | 38/384 [00:01<00:07, 46.49it/s]going through batches for holmes training:  11%|█▏        | 44/384 [00:01<00:06, 49.82it/s]going through batches for holmes training:  13%|█▎        | 50/384 [00:01<00:06, 52.31it/s]going through batches for holmes training:  15%|█▍        | 56/384 [00:01<00:06, 54.14it/s]going through batches for holmes training:  16%|█▌        | 62/384 [00:01<00:05, 55.37it/s]going through batches for holmes training:  18%|█▊        | 68/384 [00:01<00:05, 56.33it/s]going through batches for holmes training:  19%|█▉        | 74/384 [00:01<00:05, 56.99it/s]going through batches for holmes training:  21%|██        | 80/384 [00:02<00:05, 57.47it/s]going through batches for holmes training:  22%|██▏       | 86/384 [00:02<00:05, 57.84it/s]going through batches for holmes training:  24%|██▍       | 92/384 [00:02<00:05, 58.01it/s]going through batches for holmes training:  26%|██▌       | 98/384 [00:02<00:04, 58.25it/s]going through batches for holmes training:  27%|██▋       | 104/384 [00:02<00:04, 58.46it/s]going through batches for holmes training:  29%|██▊       | 110/384 [00:02<00:04, 58.48it/s]going through batches for holmes training:  30%|███       | 116/384 [00:02<00:04, 58.58it/s]going through batches for holmes training:  32%|███▏      | 122/384 [00:02<00:04, 58.67it/s]going through batches for holmes training:  33%|███▎      | 128/384 [00:02<00:04, 58.73it/s]going through batches for holmes training:  35%|███▍      | 134/384 [00:02<00:04, 58.71it/s]going through batches for holmes training:  36%|███▋      | 140/384 [00:03<00:04, 58.72it/s]going through batches for holmes training:  38%|███▊      | 146/384 [00:03<00:04, 58.52it/s]going through batches for holmes training:  40%|███▉      | 152/384 [00:03<00:03, 58.51it/s]going through batches for holmes training:  41%|████      | 158/384 [00:03<00:03, 58.52it/s]going through batches for holmes training:  43%|████▎     | 164/384 [00:03<00:03, 58.55it/s]going through batches for holmes training:  44%|████▍     | 170/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  46%|████▌     | 176/384 [00:03<00:03, 58.66it/s]going through batches for holmes training:  47%|████▋     | 182/384 [00:03<00:03, 58.64it/s]going through batches for holmes training:  49%|████▉     | 188/384 [00:03<00:03, 58.64it/s]going through batches for holmes training:  51%|█████     | 194/384 [00:03<00:03, 58.67it/s]going through batches for holmes training:  52%|█████▏    | 200/384 [00:04<00:03, 58.65it/s]going through batches for holmes training:  54%|█████▎    | 206/384 [00:04<00:03, 58.70it/s]going through batches for holmes training:  55%|█████▌    | 212/384 [00:04<00:02, 58.67it/s]going through batches for holmes training:  57%|█████▋    | 218/384 [00:04<00:02, 58.69it/s]going through batches for holmes training:  58%|█████▊    | 224/384 [00:04<00:02, 58.66it/s]going through batches for holmes training:  60%|█████▉    | 230/384 [00:04<00:02, 58.74it/s]going through batches for holmes training:  61%|██████▏   | 236/384 [00:04<00:02, 58.73it/s]going through batches for holmes training:  63%|██████▎   | 242/384 [00:04<00:02, 58.72it/s]going through batches for holmes training:  65%|██████▍   | 248/384 [00:04<00:02, 58.73it/s]going through batches for holmes training:  66%|██████▌   | 254/384 [00:05<00:02, 58.78it/s]going through batches for holmes training:  68%|██████▊   | 260/384 [00:05<00:02, 58.75it/s]going through batches for holmes training:  69%|██████▉   | 266/384 [00:05<00:02, 58.77it/s]going through batches for holmes training:  71%|███████   | 272/384 [00:05<00:01, 58.78it/s]going through batches for holmes training:  72%|███████▏  | 278/384 [00:05<00:01, 58.43it/s]going through batches for holmes training:  74%|███████▍  | 284/384 [00:05<00:01, 58.49it/s]going through batches for holmes training:  76%|███████▌  | 290/384 [00:05<00:01, 58.56it/s]going through batches for holmes training:  77%|███████▋  | 296/384 [00:05<00:01, 58.53it/s]going through batches for holmes training:  79%|███████▊  | 302/384 [00:05<00:01, 58.58it/s]going through batches for holmes training:  80%|████████  | 308/384 [00:05<00:01, 58.56it/s]going through batches for holmes training:  82%|████████▏ | 314/384 [00:06<00:01, 58.59it/s]going through batches for holmes training:  83%|████████▎ | 320/384 [00:06<00:01, 58.49it/s]going through batches for holmes training:  85%|████████▍ | 326/384 [00:06<00:00, 58.58it/s]going through batches for holmes training:  86%|████████▋ | 332/384 [00:06<00:00, 58.63it/s]going through batches for holmes training:  88%|████████▊ | 338/384 [00:06<00:00, 58.62it/s]going through batches for holmes training:  90%|████████▉ | 344/384 [00:06<00:00, 58.57it/s]going through batches for holmes training:  91%|█████████ | 350/384 [00:06<00:00, 58.59it/s]going through batches for holmes training:  93%|█████████▎| 356/384 [00:06<00:00, 58.62it/s]going through batches for holmes training:  94%|█████████▍| 362/384 [00:06<00:00, 58.54it/s]going through batches for holmes training:  96%|█████████▌| 368/384 [00:06<00:00, 58.10it/s]going through batches for holmes training:  97%|█████████▋| 374/384 [00:07<00:00, 58.37it/s]going through batches for holmes training:  99%|█████████▉| 380/384 [00:07<00:00, 58.59it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.77it/s]
loss is 1.74837064743042
loss is 1.8265130519866943
loss is 1.8177850246429443
loss is 1.784245491027832
loss is 1.872727632522583
loss is 1.806325078010559
loss is 1.7351993322372437
loss is 1.8399194478988647
loss is 1.7018400430679321
loss is 1.7751784324645996
loss is 1.7150015830993652
loss is 1.7961828708648682
loss is 1.8542784452438354
loss is 1.8778822422027588
loss is 1.7398470640182495
loss is 1.7628121376037598
loss is 1.7725203037261963
loss is 1.820364236831665
loss is 1.8190442323684692
loss is 1.8078123331069946
loss is 1.8110177516937256
loss is 1.8103784322738647
loss is 1.8588006496429443
loss is 1.8298389911651611
loss is 1.709670066833496
loss is 1.8269745111465454
loss is 1.7087997198104858
loss is 1.7876880168914795
loss is 1.8011265993118286
loss is 1.734952688217163
loss is 1.8248207569122314
loss is 1.748746633529663
loss is 1.7355178594589233
loss is 1.7962323427200317
loss is 1.7890719175338745
loss is 1.695674180984497
loss is 1.803787112236023
loss is 1.784781813621521
loss is 1.7484016418457031
loss is 1.7371364831924438
loss is 1.676790475845337
loss is 1.7167375087738037
loss is 1.9092910289764404
loss is 1.8572670221328735
loss is 1.7403606176376343
loss is 1.6949355602264404
loss is 1.8635468482971191
loss is 1.7089594602584839
loss is 1.7765225172042847
loss is 1.7815954685211182
loss is 1.819299340248108
loss is 1.7801185846328735
loss is 1.668013572692871
loss is 1.8238977193832397
loss is 1.6957728862762451
loss is 1.7866756916046143
loss is 1.7354443073272705
loss is 1.7325855493545532
loss is 1.7451982498168945
loss is 1.7546993494033813
loss is 1.7474063634872437
loss is 1.8335998058319092
loss is 1.820034384727478
loss is 1.7822662591934204
loss is 1.8455089330673218
loss is 1.7230725288391113
loss is 1.7871395349502563
loss is 1.8264567852020264
loss is 1.7704179286956787
loss is 1.7991353273391724
loss is 1.7432736158370972
loss is 1.740361213684082
loss is 1.8373651504516602
loss is 1.6943144798278809
loss is 1.7471230030059814
loss is 1.7989169359207153
loss is 1.804042100906372
loss is 1.7224327325820923
loss is 1.7025843858718872
loss is 1.7358298301696777
loss is 1.6933834552764893
loss is 1.7738569974899292
loss is 1.7710213661193848
loss is 1.7915122509002686
loss is 1.738425612449646
loss is 1.8244297504425049
loss is 1.766666293144226
loss is 1.7440791130065918
loss is 1.7797932624816895
loss is 1.7434495687484741
loss is 1.7477471828460693
loss is 1.7544560432434082
loss is 1.796359896659851
loss is 1.822069525718689
loss is 1.6947308778762817
loss is 1.7398356199264526
loss is 1.749189853668213
loss is 1.7659562826156616
loss is 1.7923403978347778
loss is 1.7193084955215454
loss is 1.7445260286331177
loss is 1.7674689292907715
loss is 1.6850794553756714
loss is 1.7633243799209595
loss is 1.7830619812011719
loss is 1.804071307182312
loss is 1.720341444015503
loss is 1.7547504901885986
loss is 1.7837495803833008
loss is 1.7045801877975464
loss is 1.7166600227355957
loss is 1.8092273473739624
loss is 1.801646113395691
loss is 1.7344152927398682
loss is 1.8019061088562012
loss is 1.8709478378295898
loss is 1.7525017261505127
loss is 1.7280784845352173
loss is 1.7193739414215088
loss is 1.7699542045593262
loss is 1.8551214933395386
loss is 1.8206939697265625
loss is 1.8064521551132202
loss is 1.7698016166687012
loss is 1.7781524658203125
loss is 1.7811410427093506
loss is 1.8670328855514526
loss is 1.8942327499389648
loss is 1.7405619621276855
loss is 1.7412207126617432
loss is 1.703554391860962
loss is 1.707069754600525
loss is 1.7264814376831055
loss is 1.7321950197219849
loss is 1.8052444458007812
loss is 1.723201036453247
loss is 1.7455514669418335
loss is 1.694124460220337
loss is 1.6946830749511719
loss is 1.869295597076416
loss is 1.801360011100769
loss is 1.7259361743927002
loss is 1.6409562826156616
loss is 1.6986775398254395
loss is 1.7440581321716309
loss is 1.8039723634719849
loss is 1.749189853668213
loss is 1.7279572486877441
loss is 1.7375227212905884
loss is 1.7598015069961548
loss is 1.739047646522522
loss is 1.7299821376800537
loss is 1.7238763570785522
loss is 1.7605172395706177
loss is 1.8788139820098877
loss is 1.6942205429077148
loss is 1.7254118919372559
loss is 1.8803412914276123
loss is 1.7293117046356201
loss is 1.8062901496887207
loss is 1.7328674793243408
loss is 1.6651601791381836
loss is 1.7118208408355713
loss is 1.6469182968139648
loss is 1.6528064012527466
loss is 1.771594524383545
loss is 1.8492823839187622
loss is 1.6275968551635742
loss is 1.6622803211212158
loss is 1.7175031900405884
loss is 1.8050159215927124
loss is 1.744588017463684
loss is 1.7917121648788452
loss is 1.8471568822860718
loss is 1.6652308702468872
loss is 1.780024766921997
loss is 1.7007616758346558
loss is 1.8850091695785522
loss is 1.8313014507293701
loss is 1.7296125888824463
loss is 1.842238187789917
loss is 1.7364552021026611
loss is 1.753409743309021
loss is 1.7162853479385376
loss is 1.7668346166610718
loss is 1.774986743927002
loss is 1.659538984298706
loss is 1.7368980646133423
loss is 1.7340288162231445
loss is 1.682462453842163
loss is 1.8059449195861816
loss is 1.7022262811660767
loss is 1.6843383312225342
loss is 1.7249184846878052
loss is 1.7082960605621338
loss is 1.783197045326233
loss is 1.6932618618011475
loss is 1.7900253534317017
loss is 1.7165477275848389
loss is 1.6995818614959717
loss is 1.7770531177520752
loss is 1.6748497486114502
loss is 1.8118908405303955
loss is 1.7216203212738037
loss is 1.7446808815002441
loss is 1.6725691556930542
loss is 1.6220206022262573
loss is 1.7944821119308472
loss is 1.742795467376709
loss is 1.6747767925262451
loss is 1.7549781799316406
loss is 1.7308952808380127
loss is 1.9076417684555054
loss is 1.7144713401794434
loss is 1.7053782939910889
loss is 1.647058129310608
loss is 1.7503966093063354
loss is 1.7390646934509277
loss is 1.7591761350631714
loss is 1.7294780015945435
loss is 1.7521384954452515
loss is 1.7474011182785034
loss is 1.8228520154953003
loss is 1.656419277191162
loss is 1.754225730895996
loss is 1.6971426010131836
loss is 1.7547177076339722
loss is 1.8204317092895508
loss is 1.8126115798950195
loss is 1.7725051641464233
loss is 1.6321324110031128
loss is 1.6342782974243164
loss is 1.691462755203247
loss is 1.8020057678222656
loss is 1.753690242767334
loss is 1.6213164329528809
loss is 1.8626611232757568
loss is 1.7466762065887451
loss is 1.6386232376098633
loss is 1.7306898832321167
loss is 1.7031707763671875
loss is 1.8289153575897217
loss is 1.8265795707702637
loss is 1.6787573099136353
loss is 1.6852421760559082
loss is 1.7715997695922852
loss is 1.6609282493591309
loss is 1.702952265739441
loss is 1.740614891052246
loss is 1.6434638500213623
loss is 1.740485668182373
loss is 1.6765462160110474
loss is 1.7085210084915161
loss is 1.72800612449646
loss is 1.797113060951233
loss is 1.760367751121521
loss is 1.739839792251587
loss is 1.7760719060897827
loss is 1.738682508468628
loss is 1.7110350131988525
loss is 1.7069240808486938
loss is 1.714730978012085
loss is 1.6821017265319824
loss is 1.653355598449707
loss is 1.7331719398498535
loss is 1.6471452713012695
loss is 1.8425959348678589
loss is 1.7462210655212402
loss is 1.8339838981628418
loss is 1.6866337060928345
loss is 1.7448533773422241
loss is 1.7864060401916504
loss is 1.747494101524353
loss is 1.789329171180725
loss is 1.8075201511383057
loss is 1.759337306022644
loss is 1.7461705207824707
loss is 1.7264299392700195
loss is 1.7559024095535278
loss is 1.6355186700820923
loss is 1.7210235595703125
loss is 1.7740283012390137
loss is 1.7420231103897095
loss is 1.8395276069641113
loss is 1.7652255296707153
loss is 1.7342257499694824
loss is 1.6903539896011353
loss is 1.6920628547668457
loss is 1.6421259641647339
loss is 1.7115778923034668
loss is 1.6925336122512817
loss is 1.7036471366882324
loss is 1.5975890159606934
loss is 1.7170937061309814
loss is 1.7955552339553833
loss is 1.8151886463165283
loss is 1.6607285737991333
loss is 1.8099641799926758
loss is 1.7451661825180054
loss is 1.7103815078735352
loss is 1.7678627967834473
loss is 1.7263396978378296
loss is 1.6591644287109375
loss is 1.6983779668807983
loss is 1.7434130907058716
loss is 1.647287130355835
loss is 1.603233814239502
loss is 1.663543701171875
loss is 1.7425804138183594
loss is 1.6747817993164062
loss is 1.7643300294876099
loss is 1.5996471643447876
loss is 1.6667288541793823
loss is 1.613327145576477
loss is 1.8923373222351074
loss is 1.7370632886886597
loss is 1.6545735597610474
loss is 1.7084243297576904
loss is 1.7591564655303955
loss is 1.7340073585510254
loss is 1.6895132064819336
loss is 1.7186392545700073
loss is 1.670312523841858
loss is 1.6476237773895264
loss is 1.7497973442077637
loss is 1.7515047788619995
loss is 1.6751970052719116
loss is 1.7025624513626099
loss is 1.7928270101547241
loss is 1.7421367168426514
loss is 1.713879108428955
loss is 1.6021835803985596
loss is 1.7412387132644653
loss is 1.7455062866210938
loss is 1.7739660739898682
loss is 1.7090307474136353
loss is 1.8590861558914185
loss is 1.7546496391296387
loss is 1.7421882152557373
loss is 1.579378604888916
loss is 1.7698184251785278
loss is 1.6109390258789062
loss is 1.7660517692565918
loss is 1.6634067296981812
loss is 1.7068616151809692
loss is 1.8074777126312256
loss is 1.7178452014923096
loss is 1.6429988145828247
loss is 1.7513624429702759
loss is 1.6728602647781372
loss is 1.6916581392288208
loss is 1.798294186592102
loss is 1.6878889799118042
loss is 1.701804757118225
loss is 1.686172604560852
loss is 1.6239274740219116
loss is 1.708058476448059
loss is 1.765255331993103
loss is 1.6528337001800537
loss is 1.6728639602661133
loss is 1.7554339170455933
loss is 1.6397652626037598
loss is 1.7068859338760376
loss is 1.624518632888794
loss is 1.7009539604187012
loss is 1.8463703393936157
loss is 1.7069878578186035
loss is 1.7491252422332764
loss is 1.7132744789123535
loss is 1.7460194826126099
loss is 1.6274504661560059
loss is 1.7443429231643677
loss is 1.7312895059585571
loss is 1.6340816020965576
loss is 1.7361582517623901
loss is 1.7932590246200562
loss is 1.6599940061569214
loss is 1.7605420351028442
loss is 1.6618379354476929
loss is 1.7384570837020874
loss is 1.686728835105896
loss is 1.7476752996444702
loss is 1.688950777053833
loss is 1.7331334352493286
epoch 4: train_loss = 1.744
4: {'Accuracy': 0.4654, 'Precision': 0.4614, 'Recall': 0.4605, 'F1-score': 0.4512}
epoch: 5
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:23,  1.45it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:33, 11.37it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:17, 20.78it/s]going through batches for holmes training:   5%|▌         | 20/384 [00:01<00:11, 30.39it/s]going through batches for holmes training:   7%|▋         | 26/384 [00:01<00:09, 37.04it/s]going through batches for holmes training:   8%|▊         | 32/384 [00:01<00:08, 42.43it/s]going through batches for holmes training:  10%|▉         | 38/384 [00:01<00:07, 46.73it/s]going through batches for holmes training:  11%|█▏        | 44/384 [00:01<00:06, 49.98it/s]going through batches for holmes training:  13%|█▎        | 50/384 [00:01<00:06, 52.45it/s]going through batches for holmes training:  15%|█▍        | 56/384 [00:01<00:06, 54.27it/s]going through batches for holmes training:  16%|█▌        | 62/384 [00:01<00:05, 55.56it/s]going through batches for holmes training:  18%|█▊        | 68/384 [00:01<00:05, 56.54it/s]going through batches for holmes training:  19%|█▉        | 74/384 [00:01<00:05, 57.22it/s]going through batches for holmes training:  21%|██        | 80/384 [00:02<00:05, 57.68it/s]going through batches for holmes training:  22%|██▏       | 86/384 [00:02<00:05, 58.00it/s]going through batches for holmes training:  24%|██▍       | 92/384 [00:02<00:05, 58.19it/s]going through batches for holmes training:  26%|██▌       | 98/384 [00:02<00:04, 58.36it/s]going through batches for holmes training:  27%|██▋       | 104/384 [00:02<00:04, 58.47it/s]going through batches for holmes training:  29%|██▊       | 110/384 [00:02<00:04, 58.55it/s]going through batches for holmes training:  30%|███       | 116/384 [00:02<00:04, 58.56it/s]going through batches for holmes training:  32%|███▏      | 122/384 [00:02<00:04, 58.65it/s]going through batches for holmes training:  33%|███▎      | 128/384 [00:02<00:04, 58.65it/s]going through batches for holmes training:  35%|███▍      | 134/384 [00:02<00:04, 58.70it/s]going through batches for holmes training:  36%|███▋      | 140/384 [00:03<00:04, 58.66it/s]going through batches for holmes training:  38%|███▊      | 146/384 [00:03<00:04, 58.68it/s]going through batches for holmes training:  40%|███▉      | 152/384 [00:03<00:03, 58.74it/s]going through batches for holmes training:  41%|████      | 158/384 [00:03<00:03, 58.61it/s]going through batches for holmes training:  43%|████▎     | 164/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  44%|████▍     | 170/384 [00:03<00:03, 58.61it/s]going through batches for holmes training:  46%|████▌     | 176/384 [00:03<00:03, 58.67it/s]going through batches for holmes training:  47%|████▋     | 182/384 [00:03<00:03, 58.70it/s]going through batches for holmes training:  49%|████▉     | 188/384 [00:03<00:03, 58.66it/s]going through batches for holmes training:  51%|█████     | 194/384 [00:03<00:03, 58.65it/s]going through batches for holmes training:  52%|█████▏    | 200/384 [00:04<00:03, 58.71it/s]going through batches for holmes training:  54%|█████▎    | 206/384 [00:04<00:03, 58.68it/s]going through batches for holmes training:  55%|█████▌    | 212/384 [00:04<00:02, 58.72it/s]going through batches for holmes training:  57%|█████▋    | 218/384 [00:04<00:02, 58.76it/s]going through batches for holmes training:  58%|█████▊    | 224/384 [00:04<00:02, 58.70it/s]going through batches for holmes training:  60%|█████▉    | 230/384 [00:04<00:02, 58.71it/s]going through batches for holmes training:  61%|██████▏   | 236/384 [00:04<00:02, 58.72it/s]going through batches for holmes training:  63%|██████▎   | 242/384 [00:04<00:02, 58.67it/s]going through batches for holmes training:  65%|██████▍   | 248/384 [00:04<00:02, 58.62it/s]going through batches for holmes training:  66%|██████▌   | 254/384 [00:05<00:02, 58.70it/s]going through batches for holmes training:  68%|██████▊   | 260/384 [00:05<00:02, 58.74it/s]going through batches for holmes training:  69%|██████▉   | 266/384 [00:05<00:02, 58.68it/s]going through batches for holmes training:  71%|███████   | 272/384 [00:05<00:01, 58.62it/s]going through batches for holmes training:  72%|███████▏  | 278/384 [00:05<00:01, 58.72it/s]going through batches for holmes training:  74%|███████▍  | 284/384 [00:05<00:01, 58.68it/s]going through batches for holmes training:  76%|███████▌  | 290/384 [00:05<00:01, 58.71it/s]going through batches for holmes training:  77%|███████▋  | 296/384 [00:05<00:01, 58.67it/s]going through batches for holmes training:  79%|███████▊  | 302/384 [00:05<00:01, 58.71it/s]going through batches for holmes training:  80%|████████  | 308/384 [00:05<00:01, 58.74it/s]going through batches for holmes training:  82%|████████▏ | 314/384 [00:06<00:01, 58.68it/s]going through batches for holmes training:  83%|████████▎ | 320/384 [00:06<00:01, 58.73it/s]going through batches for holmes training:  85%|████████▍ | 326/384 [00:06<00:00, 58.68it/s]going through batches for holmes training:  86%|████████▋ | 332/384 [00:06<00:00, 58.68it/s]going through batches for holmes training:  88%|████████▊ | 338/384 [00:06<00:00, 58.66it/s]going through batches for holmes training:  90%|████████▉ | 344/384 [00:06<00:00, 58.68it/s]going through batches for holmes training:  91%|█████████ | 350/384 [00:06<00:00, 58.65it/s]going through batches for holmes training:  93%|█████████▎| 356/384 [00:06<00:00, 58.71it/s]going through batches for holmes training:  94%|█████████▍| 362/384 [00:06<00:00, 58.70it/s]going through batches for holmes training:  96%|█████████▌| 368/384 [00:06<00:00, 58.34it/s]going through batches for holmes training:  97%|█████████▋| 374/384 [00:07<00:00, 58.56it/s]going through batches for holmes training:  99%|█████████▉| 380/384 [00:07<00:00, 58.69it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.87it/s]
loss is 1.6711879968643188
loss is 1.7038654088974
loss is 1.7074635028839111
loss is 1.6117135286331177
loss is 1.7668125629425049
loss is 1.6206234693527222
loss is 1.7270599603652954
loss is 1.652773141860962
loss is 1.6663159132003784
loss is 1.6300420761108398
loss is 1.6308881044387817
loss is 1.6654413938522339
loss is 1.6228179931640625
loss is 1.5869914293289185
loss is 1.7687076330184937
loss is 1.669889211654663
loss is 1.597642183303833
loss is 1.6135530471801758
loss is 1.6866774559020996
loss is 1.6699470281600952
loss is 1.6691840887069702
loss is 1.6863843202590942
loss is 1.7399312257766724
loss is 1.6388506889343262
loss is 1.7138270139694214
loss is 1.6840193271636963
loss is 1.6852316856384277
loss is 1.646580457687378
loss is 1.6995594501495361
loss is 1.6638898849487305
loss is 1.6810885667800903
loss is 1.690274953842163
loss is 1.7032932043075562
loss is 1.6215081214904785
loss is 1.6603753566741943
loss is 1.7043604850769043
loss is 1.7536941766738892
loss is 1.5502448081970215
loss is 1.813530445098877
loss is 1.638485074043274
loss is 1.667556643486023
loss is 1.7225346565246582
loss is 1.721561312675476
loss is 1.6828434467315674
loss is 1.6529629230499268
loss is 1.739559531211853
loss is 1.6108763217926025
loss is 1.682273507118225
loss is 1.7568790912628174
loss is 1.5761971473693848
loss is 1.6733465194702148
loss is 1.6807905435562134
loss is 1.592067837715149
loss is 1.6442139148712158
loss is 1.7591270208358765
loss is 1.6128960847854614
loss is 1.6552550792694092
loss is 1.6516963243484497
loss is 1.6522821187973022
loss is 1.6149569749832153
loss is 1.5856475830078125
loss is 1.7125210762023926
loss is 1.715193748474121
loss is 1.6412482261657715
loss is 1.7205498218536377
loss is 1.685732126235962
loss is 1.7447538375854492
loss is 1.7049241065979004
loss is 1.6966943740844727
loss is 1.6873948574066162
loss is 1.656986117362976
loss is 1.6754437685012817
loss is 1.598813533782959
loss is 1.7069144248962402
loss is 1.7506725788116455
loss is 1.6396805047988892
loss is 1.596481204032898
loss is 1.7504116296768188
loss is 1.5966421365737915
loss is 1.6628799438476562
loss is 1.7002453804016113
loss is 1.645472526550293
loss is 1.693759799003601
loss is 1.7471064329147339
loss is 1.682010531425476
loss is 1.57151460647583
loss is 1.5923359394073486
loss is 1.580237627029419
loss is 1.614646553993225
loss is 1.6686042547225952
loss is 1.74811589717865
loss is 1.5641592741012573
loss is 1.747036099433899
loss is 1.6814401149749756
loss is 1.7226312160491943
loss is 1.671864628791809
loss is 1.5839684009552002
loss is 1.5380383729934692
loss is 1.6337532997131348
loss is 1.6615288257598877
loss is 1.6765666007995605
loss is 1.622004747390747
loss is 1.6785027980804443
loss is 1.5950164794921875
loss is 1.7758511304855347
loss is 1.7051368951797485
loss is 1.6800341606140137
loss is 1.7132971286773682
loss is 1.7243218421936035
loss is 1.599147081375122
loss is 1.7434401512145996
loss is 1.6755037307739258
loss is 1.670229196548462
loss is 1.7457244396209717
loss is 1.7606456279754639
loss is 1.6474796533584595
loss is 1.645860195159912
loss is 1.6765583753585815
loss is 1.7670717239379883
loss is 1.6277052164077759
loss is 1.6292921304702759
loss is 1.7378685474395752
loss is 1.66803777217865
loss is 1.6547348499298096
loss is 1.672776460647583
loss is 1.6773532629013062
loss is 1.6384085416793823
loss is 1.7080540657043457
loss is 1.661189079284668
loss is 1.7794042825698853
loss is 1.7049860954284668
loss is 1.6088792085647583
loss is 1.8073129653930664
loss is 1.6254125833511353
loss is 1.600182056427002
loss is 1.6534637212753296
loss is 1.653304100036621
loss is 1.667640209197998
loss is 1.6828372478485107
loss is 1.6215269565582275
loss is 1.6823467016220093
loss is 1.722131371498108
loss is 1.7039636373519897
loss is 1.5688700675964355
loss is 1.7455083131790161
loss is 1.6934387683868408
loss is 1.638750433921814
loss is 1.6864409446716309
loss is 1.7338476181030273
loss is 1.6678071022033691
loss is 1.693878173828125
loss is 1.6572109460830688
loss is 1.689588189125061
loss is 1.6832565069198608
loss is 1.6432888507843018
loss is 1.7874274253845215
loss is 1.66217839717865
loss is 1.584571361541748
loss is 1.6288769245147705
loss is 1.670994520187378
loss is 1.729448676109314
loss is 1.6744024753570557
loss is 1.5734068155288696
loss is 1.596804141998291
loss is 1.6526706218719482
loss is 1.6275690793991089
loss is 1.698764681816101
loss is 1.6500825881958008
loss is 1.6595288515090942
loss is 1.6493662595748901
loss is 1.6409345865249634
loss is 1.635439395904541
loss is 1.6099528074264526
loss is 1.7776163816452026
loss is 1.600786566734314
loss is 1.693213939666748
loss is 1.802067756652832
loss is 1.6809051036834717
loss is 1.6900181770324707
loss is 1.6176954507827759
loss is 1.754197359085083
loss is 1.6916851997375488
loss is 1.7175756692886353
loss is 1.5669949054718018
loss is 1.6287412643432617
loss is 1.647403597831726
loss is 1.7276268005371094
loss is 1.6918303966522217
loss is 1.6067029237747192
loss is 1.575616717338562
loss is 1.742539644241333
loss is 1.6101815700531006
loss is 1.6468719244003296
loss is 1.73529052734375
loss is 1.689056396484375
loss is 1.5852311849594116
loss is 1.663039207458496
loss is 1.5619834661483765
loss is 1.620363712310791
loss is 1.6702476739883423
loss is 1.764454960823059
loss is 1.7039504051208496
loss is 1.5396114587783813
loss is 1.6607290506362915
loss is 1.663698673248291
loss is 1.617039680480957
loss is 1.5998468399047852
loss is 1.6259945631027222
loss is 1.6307164430618286
loss is 1.630967140197754
loss is 1.6875500679016113
loss is 1.6768954992294312
loss is 1.7245101928710938
loss is 1.6821320056915283
loss is 1.6311614513397217
loss is 1.6610532999038696
loss is 1.6117637157440186
loss is 1.5870301723480225
loss is 1.655930995941162
loss is 1.549464225769043
loss is 1.7196427583694458
loss is 1.6637201309204102
loss is 1.6070176362991333
loss is 1.6531301736831665
loss is 1.6905262470245361
loss is 1.733705997467041
loss is 1.6937631368637085
loss is 1.6082605123519897
loss is 1.6663978099822998
loss is 1.6624647378921509
loss is 1.675389289855957
loss is 1.6464136838912964
loss is 1.610458254814148
loss is 1.7205485105514526
loss is 1.688523530960083
loss is 1.709611415863037
loss is 1.5823779106140137
loss is 1.6135362386703491
loss is 1.6207504272460938
loss is 1.605061650276184
loss is 1.6592603921890259
loss is 1.6885305643081665
loss is 1.6403687000274658
loss is 1.5017050504684448
loss is 1.7533096075057983
loss is 1.651241660118103
loss is 1.5516891479492188
loss is 1.6310955286026
loss is 1.6975445747375488
loss is 1.602622628211975
loss is 1.6612437963485718
loss is 1.7309666872024536
loss is 1.5502506494522095
loss is 1.5821384191513062
loss is 1.58555269241333
loss is 1.6262218952178955
loss is 1.5939054489135742
loss is 1.6354095935821533
loss is 1.6273852586746216
loss is 1.847830057144165
loss is 1.5800871849060059
loss is 1.5858006477355957
loss is 1.6623976230621338
loss is 1.701777696609497
loss is 1.6712121963500977
loss is 1.6959370374679565
loss is 1.55629301071167
loss is 1.5416417121887207
loss is 1.669029712677002
loss is 1.6972167491912842
loss is 1.8646011352539062
loss is 1.6030751466751099
loss is 1.6064143180847168
loss is 1.6399530172348022
loss is 1.5778412818908691
loss is 1.6568621397018433
loss is 1.6838934421539307
loss is 1.643059492111206
loss is 1.6385791301727295
loss is 1.6462013721466064
loss is 1.5856866836547852
loss is 1.6145596504211426
loss is 1.6172937154769897
loss is 1.5374422073364258
loss is 1.6491626501083374
loss is 1.593785047531128
loss is 1.6613658666610718
loss is 1.5788726806640625
loss is 1.5947940349578857
loss is 1.6220672130584717
loss is 1.6097782850265503
loss is 1.6534297466278076
loss is 1.6413743495941162
loss is 1.6455061435699463
loss is 1.5988554954528809
loss is 1.6736443042755127
loss is 1.5941722393035889
loss is 1.6371034383773804
loss is 1.5937278270721436
loss is 1.6495128870010376
loss is 1.633020281791687
loss is 1.6669387817382812
loss is 1.6317678689956665
loss is 1.6831201314926147
loss is 1.5975747108459473
loss is 1.6812465190887451
loss is 1.6388499736785889
loss is 1.6338562965393066
loss is 1.6410359144210815
loss is 1.5447745323181152
loss is 1.6319106817245483
loss is 1.6416016817092896
loss is 1.5160834789276123
loss is 1.5872114896774292
loss is 1.6702858209609985
loss is 1.5784292221069336
loss is 1.6926519870758057
loss is 1.6875262260437012
loss is 1.5274803638458252
loss is 1.6122236251831055
loss is 1.6156761646270752
loss is 1.5964411497116089
loss is 1.6365830898284912
loss is 1.6178596019744873
loss is 1.6431416273117065
loss is 1.6484391689300537
loss is 1.658258318901062
loss is 1.6417683362960815
loss is 1.6455035209655762
loss is 1.6549925804138184
loss is 1.6737545728683472
loss is 1.776914119720459
loss is 1.70734441280365
loss is 1.6328233480453491
loss is 1.6981537342071533
loss is 1.6179580688476562
loss is 1.585296630859375
loss is 1.6416618824005127
loss is 1.571710467338562
loss is 1.6215081214904785
loss is 1.499019980430603
loss is 1.6618945598602295
loss is 1.7618234157562256
loss is 1.5632920265197754
loss is 1.6674258708953857
loss is 1.6818183660507202
loss is 1.5540337562561035
loss is 1.6239235401153564
loss is 1.6882473230361938
loss is 1.6245558261871338
loss is 1.6484547853469849
loss is 1.6647334098815918
loss is 1.6530356407165527
loss is 1.6015496253967285
loss is 1.684424638748169
loss is 1.570152997970581
loss is 1.5291869640350342
loss is 1.624294400215149
loss is 1.5932965278625488
loss is 1.550653100013733
loss is 1.5013293027877808
loss is 1.6267640590667725
loss is 1.6396392583847046
loss is 1.5661579370498657
loss is 1.6015313863754272
loss is 1.6548945903778076
loss is 1.647457480430603
loss is 1.6079919338226318
loss is 1.572402000427246
loss is 1.5206462144851685
loss is 1.6787693500518799
loss is 1.6509257555007935
loss is 1.6149413585662842
loss is 1.6630501747131348
loss is 1.6322410106658936
loss is 1.6143563985824585
loss is 1.6979148387908936
loss is 1.5857599973678589
loss is 1.5964255332946777
loss is 1.582821011543274
loss is 1.6884897947311401
loss is 1.5407164096832275
loss is 1.5203883647918701
loss is 1.5513149499893188
epoch 5: train_loss = 1.652
5: {'Accuracy': 0.4786, 'Precision': 0.4783, 'Recall': 0.4739, 'F1-score': 0.4701}
epoch: 6
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<03:53,  1.64it/s]going through batches for holmes training:   2%|▏         | 6/384 [00:00<00:35, 10.60it/s]going through batches for holmes training:   3%|▎         | 12/384 [00:00<00:17, 20.74it/s]going through batches for holmes training:   5%|▍         | 18/384 [00:00<00:12, 29.66it/s]going through batches for holmes training:   6%|▋         | 24/384 [00:01<00:09, 36.99it/s]going through batches for holmes training:   8%|▊         | 30/384 [00:01<00:08, 42.55it/s]going through batches for holmes training:   9%|▉         | 36/384 [00:01<00:07, 46.73it/s]going through batches for holmes training:  11%|█         | 42/384 [00:01<00:06, 49.88it/s]going through batches for holmes training:  12%|█▎        | 48/384 [00:01<00:06, 52.16it/s]going through batches for holmes training:  14%|█▍        | 54/384 [00:01<00:06, 53.84it/s]going through batches for holmes training:  16%|█▌        | 60/384 [00:01<00:05, 55.03it/s]going through batches for holmes training:  17%|█▋        | 66/384 [00:01<00:05, 55.90it/s]going through batches for holmes training:  19%|█▉        | 72/384 [00:01<00:05, 56.51it/s]going through batches for holmes training:  20%|██        | 78/384 [00:01<00:05, 56.88it/s]going through batches for holmes training:  22%|██▏       | 84/384 [00:02<00:05, 57.24it/s]going through batches for holmes training:  23%|██▎       | 90/384 [00:02<00:05, 57.44it/s]going through batches for holmes training:  25%|██▌       | 96/384 [00:02<00:05, 57.52it/s]going through batches for holmes training:  27%|██▋       | 102/384 [00:02<00:04, 57.60it/s]going through batches for holmes training:  28%|██▊       | 108/384 [00:02<00:04, 57.71it/s]going through batches for holmes training:  30%|██▉       | 114/384 [00:02<00:04, 57.70it/s]going through batches for holmes training:  31%|███▏      | 120/384 [00:02<00:04, 57.75it/s]going through batches for holmes training:  33%|███▎      | 126/384 [00:02<00:04, 57.81it/s]going through batches for holmes training:  34%|███▍      | 132/384 [00:02<00:04, 57.88it/s]going through batches for holmes training:  36%|███▌      | 138/384 [00:02<00:04, 57.81it/s]going through batches for holmes training:  38%|███▊      | 144/384 [00:03<00:04, 57.87it/s]going through batches for holmes training:  39%|███▉      | 150/384 [00:03<00:04, 57.89it/s]going through batches for holmes training:  41%|████      | 156/384 [00:03<00:03, 57.87it/s]going through batches for holmes training:  42%|████▏     | 162/384 [00:03<00:03, 57.53it/s]going through batches for holmes training:  44%|████▍     | 168/384 [00:03<00:03, 57.52it/s]going through batches for holmes training:  45%|████▌     | 174/384 [00:03<00:03, 57.73it/s]going through batches for holmes training:  47%|████▋     | 180/384 [00:03<00:03, 57.80it/s]going through batches for holmes training:  48%|████▊     | 186/384 [00:03<00:03, 57.74it/s]going through batches for holmes training:  50%|█████     | 192/384 [00:03<00:03, 57.83it/s]going through batches for holmes training:  52%|█████▏    | 198/384 [00:04<00:03, 57.78it/s]going through batches for holmes training:  53%|█████▎    | 204/384 [00:04<00:03, 57.79it/s]going through batches for holmes training:  55%|█████▍    | 210/384 [00:04<00:03, 57.81it/s]going through batches for holmes training:  56%|█████▋    | 216/384 [00:04<00:02, 57.85it/s]going through batches for holmes training:  58%|█████▊    | 222/384 [00:04<00:02, 57.94it/s]going through batches for holmes training:  59%|█████▉    | 228/384 [00:04<00:02, 57.82it/s]going through batches for holmes training:  61%|██████    | 234/384 [00:04<00:02, 57.89it/s]going through batches for holmes training:  62%|██████▎   | 240/384 [00:04<00:02, 57.91it/s]going through batches for holmes training:  64%|██████▍   | 246/384 [00:04<00:02, 57.93it/s]going through batches for holmes training:  66%|██████▌   | 252/384 [00:04<00:02, 57.96it/s]going through batches for holmes training:  67%|██████▋   | 258/384 [00:05<00:02, 57.92it/s]going through batches for holmes training:  69%|██████▉   | 264/384 [00:05<00:02, 57.93it/s]going through batches for holmes training:  70%|███████   | 270/384 [00:05<00:01, 57.93it/s]going through batches for holmes training:  72%|███████▏  | 276/384 [00:05<00:01, 57.92it/s]going through batches for holmes training:  73%|███████▎  | 282/384 [00:05<00:01, 57.96it/s]going through batches for holmes training:  75%|███████▌  | 288/384 [00:05<00:01, 57.89it/s]going through batches for holmes training:  77%|███████▋  | 294/384 [00:05<00:01, 57.84it/s]going through batches for holmes training:  78%|███████▊  | 300/384 [00:05<00:01, 57.86it/s]going through batches for holmes training:  80%|███████▉  | 306/384 [00:05<00:01, 57.86it/s]going through batches for holmes training:  81%|████████▏ | 312/384 [00:06<00:01, 57.85it/s]going through batches for holmes training:  83%|████████▎ | 318/384 [00:06<00:01, 57.80it/s]going through batches for holmes training:  84%|████████▍ | 324/384 [00:06<00:01, 57.84it/s]going through batches for holmes training:  86%|████████▌ | 330/384 [00:06<00:00, 57.84it/s]going through batches for holmes training:  88%|████████▊ | 336/384 [00:06<00:00, 57.81it/s]going through batches for holmes training:  89%|████████▉ | 342/384 [00:06<00:00, 57.80it/s]going through batches for holmes training:  91%|█████████ | 348/384 [00:06<00:00, 57.82it/s]going through batches for holmes training:  92%|█████████▏| 354/384 [00:06<00:00, 57.77it/s]going through batches for holmes training:  94%|█████████▍| 360/384 [00:06<00:00, 57.80it/s]going through batches for holmes training:  95%|█████████▌| 366/384 [00:06<00:00, 57.33it/s]going through batches for holmes training:  97%|█████████▋| 372/384 [00:07<00:00, 57.63it/s]going through batches for holmes training:  98%|█████████▊| 378/384 [00:07<00:00, 57.72it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 57.88it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.60it/s]
loss is 1.5995733737945557
loss is 1.7863414287567139
loss is 1.560514211654663
loss is 1.5506304502487183
loss is 1.5919790267944336
loss is 1.5804845094680786
loss is 1.5889534950256348
loss is 1.611238718032837
loss is 1.5916157960891724
loss is 1.5740793943405151
loss is 1.583412766456604
loss is 1.719174861907959
loss is 1.5640151500701904
loss is 1.5917460918426514
loss is 1.6821879148483276
loss is 1.5893315076828003
loss is 1.6077510118484497
loss is 1.5856971740722656
loss is 1.6660536527633667
loss is 1.555859088897705
loss is 1.6646778583526611
loss is 1.5987536907196045
loss is 1.5692138671875
loss is 1.5412521362304688
loss is 1.6585650444030762
loss is 1.6497284173965454
loss is 1.5454456806182861
loss is 1.7368152141571045
loss is 1.7250381708145142
loss is 1.6595841646194458
loss is 1.5128462314605713
loss is 1.6271543502807617
loss is 1.6291905641555786
loss is 1.5727134943008423
loss is 1.7384356260299683
loss is 1.679659366607666
loss is 1.6344300508499146
loss is 1.729796290397644
loss is 1.5100935697555542
loss is 1.5710911750793457
loss is 1.5086979866027832
loss is 1.514825701713562
loss is 1.6716233491897583
loss is 1.6098805665969849
loss is 1.6154893636703491
loss is 1.5899391174316406
loss is 1.619448184967041
loss is 1.6353873014450073
loss is 1.6618714332580566
loss is 1.5527451038360596
loss is 1.4284248352050781
loss is 1.6264721155166626
loss is 1.5459057092666626
loss is 1.5348172187805176
loss is 1.6582417488098145
loss is 1.5626797676086426
loss is 1.6412322521209717
loss is 1.6587954759597778
loss is 1.5368397235870361
loss is 1.5545110702514648
loss is 1.6039890050888062
loss is 1.5474509000778198
loss is 1.622521162033081
loss is 1.5394662618637085
loss is 1.5476844310760498
loss is 1.6361701488494873
loss is 1.549028754234314
loss is 1.635654091835022
loss is 1.5706249475479126
loss is 1.5471105575561523
loss is 1.5720150470733643
loss is 1.6826317310333252
loss is 1.7593498229980469
loss is 1.5207186937332153
loss is 1.437193751335144
loss is 1.5676138401031494
loss is 1.5669639110565186
loss is 1.522548794746399
loss is 1.6061309576034546
loss is 1.4924999475479126
loss is 1.6364015340805054
loss is 1.6603437662124634
loss is 1.4621882438659668
loss is 1.530408501625061
loss is 1.5369999408721924
loss is 1.556585431098938
loss is 1.5829108953475952
loss is 1.5877147912979126
loss is 1.5053181648254395
loss is 1.625037431716919
loss is 1.6930404901504517
loss is 1.7018046379089355
loss is 1.608627438545227
loss is 1.6851938962936401
loss is 1.5886201858520508
loss is 1.6319242715835571
loss is 1.623526930809021
loss is 1.5623512268066406
loss is 1.5218205451965332
loss is 1.4911820888519287
loss is 1.5675361156463623
loss is 1.5154916048049927
loss is 1.579978346824646
loss is 1.546546459197998
loss is 1.609584093093872
loss is 1.6455490589141846
loss is 1.5298316478729248
loss is 1.5936110019683838
loss is 1.4739705324172974
loss is 1.5179804563522339
loss is 1.7057173252105713
loss is 1.638564109802246
loss is 1.6271045207977295
loss is 1.5665431022644043
loss is 1.5104835033416748
loss is 1.6241954565048218
loss is 1.592044472694397
loss is 1.5635889768600464
loss is 1.6315257549285889
loss is 1.666133999824524
loss is 1.5083918571472168
loss is 1.5938360691070557
loss is 1.4450634717941284
loss is 1.6458380222320557
loss is 1.5428781509399414
loss is 1.662845492362976
loss is 1.58585524559021
loss is 1.5021405220031738
loss is 1.6452996730804443
loss is 1.5261071920394897
loss is 1.584192156791687
loss is 1.5609688758850098
loss is 1.58634614944458
loss is 1.6021661758422852
loss is 1.5460089445114136
loss is 1.545544147491455
loss is 1.6059776544570923
loss is 1.544339656829834
loss is 1.6382352113723755
loss is 1.540134310722351
loss is 1.514624834060669
loss is 1.693336844444275
loss is 1.5678824186325073
loss is 1.592329502105713
loss is 1.5042272806167603
loss is 1.6825813055038452
loss is 1.6647485494613647
loss is 1.5821930170059204
loss is 1.5217835903167725
loss is 1.6213058233261108
loss is 1.626434326171875
loss is 1.4810105562210083
loss is 1.540908694267273
loss is 1.6276217699050903
loss is 1.5901316404342651
loss is 1.5845381021499634
loss is 1.535092830657959
loss is 1.5760722160339355
loss is 1.5618480443954468
loss is 1.5345994234085083
loss is 1.585445523262024
loss is 1.6057990789413452
loss is 1.5672188997268677
loss is 1.5712730884552002
loss is 1.4801907539367676
loss is 1.5409356355667114
loss is 1.5651772022247314
loss is 1.6717934608459473
loss is 1.5984230041503906
loss is 1.585403561592102
loss is 1.6565830707550049
loss is 1.5965558290481567
loss is 1.691890001296997
loss is 1.6553878784179688
loss is 1.5624903440475464
loss is 1.647729516029358
loss is 1.6150249242782593
loss is 1.5725706815719604
loss is 1.5973111391067505
loss is 1.4814050197601318
loss is 1.5097684860229492
loss is 1.576249122619629
loss is 1.5903065204620361
loss is 1.6700271368026733
loss is 1.6407243013381958
loss is 1.6915793418884277
loss is 1.5949288606643677
loss is 1.5571959018707275
loss is 1.4927140474319458
loss is 1.618157148361206
loss is 1.5759100914001465
loss is 1.4971916675567627
loss is 1.5745408535003662
loss is 1.6385306119918823
loss is 1.6283612251281738
loss is 1.6062562465667725
loss is 1.4911061525344849
loss is 1.5143420696258545
loss is 1.501930832862854
loss is 1.5701746940612793
loss is 1.5807387828826904
loss is 1.5612831115722656
loss is 1.5324221849441528
loss is 1.509966254234314
loss is 1.640953540802002
loss is 1.4858527183532715
loss is 1.629601001739502
loss is 1.552473783493042
loss is 1.4656680822372437
loss is 1.4935394525527954
loss is 1.5746632814407349
loss is 1.53668212890625
loss is 1.6039924621582031
loss is 1.556699514389038
loss is 1.5402367115020752
loss is 1.4983657598495483
loss is 1.5392587184906006
loss is 1.6704603433609009
loss is 1.486987590789795
loss is 1.5025306940078735
loss is 1.66194748878479
loss is 1.5598224401474
loss is 1.616945505142212
loss is 1.4769394397735596
loss is 1.6598248481750488
loss is 1.5808465480804443
loss is 1.5464690923690796
loss is 1.6088885068893433
loss is 1.5374436378479004
loss is 1.6690165996551514
loss is 1.6017032861709595
loss is 1.6070621013641357
loss is 1.4906083345413208
loss is 1.57413911819458
loss is 1.5571366548538208
loss is 1.667653203010559
loss is 1.5747253894805908
loss is 1.458247184753418
loss is 1.5052406787872314
loss is 1.5391751527786255
loss is 1.5827746391296387
loss is 1.7048242092132568
loss is 1.5393691062927246
loss is 1.6051462888717651
loss is 1.551263689994812
loss is 1.6112644672393799
loss is 1.5824460983276367
loss is 1.5472432374954224
loss is 1.5074843168258667
loss is 1.5311108827590942
loss is 1.64780592918396
loss is 1.512980341911316
loss is 1.542791485786438
loss is 1.5028162002563477
loss is 1.57197904586792
loss is 1.6121739149093628
loss is 1.4775381088256836
loss is 1.6067603826522827
loss is 1.5094867944717407
loss is 1.5482182502746582
loss is 1.698838472366333
loss is 1.5405787229537964
loss is 1.422447919845581
loss is 1.3872178792953491
loss is 1.4834702014923096
loss is 1.53450608253479
loss is 1.470321536064148
loss is 1.5372450351715088
loss is 1.6242344379425049
loss is 1.626985788345337
loss is 1.6282637119293213
loss is 1.5399656295776367
loss is 1.5636074542999268
loss is 1.596812129020691
loss is 1.5687838792800903
loss is 1.488446831703186
loss is 1.4299752712249756
loss is 1.6626417636871338
loss is 1.587782382965088
loss is 1.630897045135498
loss is 1.5624072551727295
loss is 1.4817239046096802
loss is 1.675706386566162
loss is 1.4573009014129639
loss is 1.631710410118103
loss is 1.5989363193511963
loss is 1.5271780490875244
loss is 1.5376530885696411
loss is 1.6072039604187012
loss is 1.5294064283370972
loss is 1.5823115110397339
loss is 1.488675594329834
loss is 1.5111976861953735
loss is 1.5441102981567383
loss is 1.5138041973114014
loss is 1.5358061790466309
loss is 1.4748032093048096
loss is 1.5067132711410522
loss is 1.499515414237976
loss is 1.4181561470031738
loss is 1.6561890840530396
loss is 1.5590330362319946
loss is 1.4272165298461914
loss is 1.621774435043335
loss is 1.5259567499160767
loss is 1.5129352807998657
loss is 1.4705555438995361
loss is 1.519018530845642
loss is 1.5604701042175293
loss is 1.634186863899231
loss is 1.5085535049438477
loss is 1.5756727457046509
loss is 1.5388737916946411
loss is 1.5475637912750244
loss is 1.626023292541504
loss is 1.4375717639923096
loss is 1.5701297521591187
loss is 1.5838154554367065
loss is 1.5327776670455933
loss is 1.508575439453125
loss is 1.7142133712768555
loss is 1.4959923028945923
loss is 1.5257624387741089
loss is 1.587870478630066
loss is 1.4706971645355225
loss is 1.5331016778945923
loss is 1.3703341484069824
loss is 1.3813759088516235
loss is 1.607038140296936
loss is 1.6355996131896973
loss is 1.5240260362625122
loss is 1.506784200668335
loss is 1.6310629844665527
loss is 1.492429256439209
loss is 1.4718393087387085
loss is 1.5783706903457642
loss is 1.5298696756362915
loss is 1.5972181558609009
loss is 1.5434757471084595
loss is 1.4982203245162964
loss is 1.593904733657837
loss is 1.5711838006973267
loss is 1.4646658897399902
loss is 1.6248077154159546
loss is 1.508542537689209
loss is 1.5014222860336304
loss is 1.5950814485549927
loss is 1.571070671081543
loss is 1.5567408800125122
loss is 1.482286810874939
loss is 1.5519070625305176
loss is 1.5672388076782227
loss is 1.6480811834335327
loss is 1.4798192977905273
loss is 1.4739042520523071
loss is 1.5519843101501465
loss is 1.5565446615219116
loss is 1.5299841165542603
loss is 1.4304592609405518
loss is 1.565240502357483
loss is 1.494858741760254
loss is 1.5938758850097656
loss is 1.6116491556167603
loss is 1.5220611095428467
loss is 1.561049222946167
loss is 1.6208088397979736
loss is 1.4448193311691284
loss is 1.6436340808868408
loss is 1.542181372642517
loss is 1.48581862449646
loss is 1.5278760194778442
loss is 1.6721452474594116
loss is 1.6146900653839111
loss is 1.581221580505371
loss is 1.4190853834152222
loss is 1.528497338294983
loss is 1.6115775108337402
loss is 1.5620918273925781
loss is 1.5988975763320923
loss is 1.5164223909378052
loss is 1.469416618347168
loss is 1.6124753952026367
loss is 1.4822192192077637
loss is 1.459534764289856
epoch 6: train_loss = 1.57
6: {'Accuracy': 0.4929, 'Precision': 0.5014, 'Recall': 0.4912, 'F1-score': 0.4877}
epoch: 7
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:33,  1.40it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:34, 11.00it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 20.30it/s]going through batches for holmes training:   5%|▌         | 20/384 [00:01<00:12, 29.86it/s]going through batches for holmes training:   7%|▋         | 26/384 [00:01<00:09, 36.54it/s]going through batches for holmes training:   8%|▊         | 32/384 [00:01<00:08, 42.01it/s]going through batches for holmes training:  10%|▉         | 38/384 [00:01<00:07, 46.35it/s]going through batches for holmes training:  11%|█▏        | 44/384 [00:01<00:06, 49.69it/s]going through batches for holmes training:  13%|█▎        | 50/384 [00:01<00:06, 52.19it/s]going through batches for holmes training:  15%|█▍        | 56/384 [00:01<00:06, 54.00it/s]going through batches for holmes training:  16%|█▌        | 62/384 [00:01<00:05, 55.35it/s]going through batches for holmes training:  18%|█▊        | 68/384 [00:01<00:05, 56.36it/s]going through batches for holmes training:  19%|█▉        | 74/384 [00:01<00:05, 56.98it/s]going through batches for holmes training:  21%|██        | 80/384 [00:02<00:05, 57.45it/s]going through batches for holmes training:  22%|██▏       | 86/384 [00:02<00:05, 57.81it/s]going through batches for holmes training:  24%|██▍       | 92/384 [00:02<00:05, 58.01it/s]going through batches for holmes training:  26%|██▌       | 98/384 [00:02<00:04, 58.18it/s]going through batches for holmes training:  27%|██▋       | 104/384 [00:02<00:04, 58.34it/s]going through batches for holmes training:  29%|██▊       | 110/384 [00:02<00:04, 58.48it/s]going through batches for holmes training:  30%|███       | 116/384 [00:02<00:04, 58.52it/s]going through batches for holmes training:  32%|███▏      | 122/384 [00:02<00:04, 58.52it/s]going through batches for holmes training:  33%|███▎      | 128/384 [00:02<00:04, 58.60it/s]going through batches for holmes training:  35%|███▍      | 134/384 [00:02<00:04, 58.59it/s]going through batches for holmes training:  36%|███▋      | 140/384 [00:03<00:04, 58.58it/s]going through batches for holmes training:  38%|███▊      | 146/384 [00:03<00:04, 58.56it/s]going through batches for holmes training:  40%|███▉      | 152/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  41%|████      | 158/384 [00:03<00:03, 58.63it/s]going through batches for holmes training:  43%|████▎     | 164/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  44%|████▍     | 170/384 [00:03<00:03, 58.66it/s]going through batches for holmes training:  46%|████▌     | 176/384 [00:03<00:03, 58.65it/s]going through batches for holmes training:  47%|████▋     | 182/384 [00:03<00:03, 58.65it/s]going through batches for holmes training:  49%|████▉     | 188/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  51%|█████     | 194/384 [00:04<00:03, 58.61it/s]going through batches for holmes training:  52%|█████▏    | 200/384 [00:04<00:03, 58.67it/s]going through batches for holmes training:  54%|█████▎    | 206/384 [00:04<00:03, 58.63it/s]going through batches for holmes training:  55%|█████▌    | 212/384 [00:04<00:02, 58.62it/s]going through batches for holmes training:  57%|█████▋    | 218/384 [00:04<00:02, 58.62it/s]going through batches for holmes training:  58%|█████▊    | 224/384 [00:04<00:02, 58.64it/s]going through batches for holmes training:  60%|█████▉    | 230/384 [00:04<00:02, 58.71it/s]going through batches for holmes training:  61%|██████▏   | 236/384 [00:04<00:02, 58.70it/s]going through batches for holmes training:  63%|██████▎   | 242/384 [00:04<00:02, 58.72it/s]going through batches for holmes training:  65%|██████▍   | 248/384 [00:04<00:02, 58.66it/s]going through batches for holmes training:  66%|██████▌   | 254/384 [00:05<00:02, 58.68it/s]going through batches for holmes training:  68%|██████▊   | 260/384 [00:05<00:02, 58.69it/s]going through batches for holmes training:  69%|██████▉   | 266/384 [00:05<00:02, 58.72it/s]going through batches for holmes training:  71%|███████   | 272/384 [00:05<00:01, 58.68it/s]going through batches for holmes training:  72%|███████▏  | 278/384 [00:05<00:01, 58.61it/s]going through batches for holmes training:  74%|███████▍  | 284/384 [00:05<00:01, 58.65it/s]going through batches for holmes training:  76%|███████▌  | 290/384 [00:05<00:01, 58.67it/s]going through batches for holmes training:  77%|███████▋  | 296/384 [00:05<00:01, 58.66it/s]going through batches for holmes training:  79%|███████▊  | 302/384 [00:05<00:01, 58.69it/s]going through batches for holmes training:  80%|████████  | 308/384 [00:05<00:01, 56.53it/s]going through batches for holmes training:  82%|████████▏ | 314/384 [00:06<00:01, 57.11it/s]going through batches for holmes training:  83%|████████▎ | 320/384 [00:06<00:01, 57.55it/s]going through batches for holmes training:  85%|████████▍ | 326/384 [00:06<00:01, 57.93it/s]going through batches for holmes training:  86%|████████▋ | 332/384 [00:06<00:00, 58.19it/s]going through batches for holmes training:  88%|████████▊ | 338/384 [00:06<00:00, 58.37it/s]going through batches for holmes training:  90%|████████▉ | 344/384 [00:06<00:00, 58.45it/s]going through batches for holmes training:  91%|█████████ | 350/384 [00:06<00:00, 58.54it/s]going through batches for holmes training:  93%|█████████▎| 356/384 [00:06<00:00, 58.57it/s]going through batches for holmes training:  94%|█████████▍| 362/384 [00:06<00:00, 58.65it/s]going through batches for holmes training:  96%|█████████▌| 368/384 [00:06<00:00, 58.13it/s]going through batches for holmes training:  97%|█████████▋| 374/384 [00:07<00:00, 58.40it/s]going through batches for holmes training:  99%|█████████▉| 380/384 [00:07<00:00, 58.64it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.57it/s]
loss is 1.4857500791549683
loss is 1.5453193187713623
loss is 1.5014715194702148
loss is 1.5264298915863037
loss is 1.441402792930603
loss is 1.5624127388000488
loss is 1.5010805130004883
loss is 1.6093276739120483
loss is 1.5132640600204468
loss is 1.6015276908874512
loss is 1.5376265048980713
loss is 1.5130407810211182
loss is 1.5065759420394897
loss is 1.5280708074569702
loss is 1.5310953855514526
loss is 1.520288348197937
loss is 1.5826783180236816
loss is 1.426475167274475
loss is 1.5474514961242676
loss is 1.577950119972229
loss is 1.545222520828247
loss is 1.4954009056091309
loss is 1.509833812713623
loss is 1.6122616529464722
loss is 1.4842103719711304
loss is 1.4766169786453247
loss is 1.4705227613449097
loss is 1.4322277307510376
loss is 1.5436543226242065
loss is 1.4537019729614258
loss is 1.571595311164856
loss is 1.4645123481750488
loss is 1.5148147344589233
loss is 1.6171021461486816
loss is 1.488771677017212
loss is 1.56534743309021
loss is 1.5798307657241821
loss is 1.573335886001587
loss is 1.4992610216140747
loss is 1.5194249153137207
loss is 1.4167759418487549
loss is 1.4370636940002441
loss is 1.5067334175109863
loss is 1.573424220085144
loss is 1.4896388053894043
loss is 1.4786884784698486
loss is 1.5764219760894775
loss is 1.4857968091964722
loss is 1.5989967584609985
loss is 1.499647617340088
loss is 1.615697979927063
loss is 1.4907448291778564
loss is 1.5885435342788696
loss is 1.455137014389038
loss is 1.4976829290390015
loss is 1.490939974784851
loss is 1.529131293296814
loss is 1.527281641960144
loss is 1.496098518371582
loss is 1.4913482666015625
loss is 1.5405405759811401
loss is 1.4771368503570557
loss is 1.5825892686843872
loss is 1.638804316520691
loss is 1.4428579807281494
loss is 1.5149023532867432
loss is 1.4536693096160889
loss is 1.4519656896591187
loss is 1.6058872938156128
loss is 1.423952341079712
loss is 1.4226288795471191
loss is 1.5692124366760254
loss is 1.436294674873352
loss is 1.5282098054885864
loss is 1.527288794517517
loss is 1.4900821447372437
loss is 1.487043023109436
loss is 1.5654923915863037
loss is 1.5363153219223022
loss is 1.4384214878082275
loss is 1.546644926071167
loss is 1.4662878513336182
loss is 1.522823691368103
loss is 1.5266923904418945
loss is 1.5773686170578003
loss is 1.509493350982666
loss is 1.5212026834487915
loss is 1.5481102466583252
loss is 1.5511904954910278
loss is 1.5472792387008667
loss is 1.569787859916687
loss is 1.5387709140777588
loss is 1.624153733253479
loss is 1.5891203880310059
loss is 1.3785630464553833
loss is 1.4218753576278687
loss is 1.6891186237335205
loss is 1.4991390705108643
loss is 1.593900203704834
loss is 1.5029406547546387
loss is 1.6294374465942383
loss is 1.4318363666534424
loss is 1.6171497106552124
loss is 1.5765827894210815
loss is 1.5000338554382324
loss is 1.585424542427063
loss is 1.4379167556762695
loss is 1.506282091140747
loss is 1.514222264289856
loss is 1.491044521331787
loss is 1.5234177112579346
loss is 1.5120426416397095
loss is 1.529868721961975
loss is 1.6109563112258911
loss is 1.6489580869674683
loss is 1.489275336265564
loss is 1.5103378295898438
loss is 1.3984637260437012
loss is 1.4827625751495361
loss is 1.5151655673980713
loss is 1.501133680343628
loss is 1.5704799890518188
loss is 1.5147584676742554
loss is 1.5848010778427124
loss is 1.512930154800415
loss is 1.409751296043396
loss is 1.4481467008590698
loss is 1.499695897102356
loss is 1.503769040107727
loss is 1.5461901426315308
loss is 1.6122616529464722
loss is 1.4613429307937622
loss is 1.6210473775863647
loss is 1.5639539957046509
loss is 1.4693547487258911
loss is 1.5183825492858887
loss is 1.534347414970398
loss is 1.4975699186325073
loss is 1.611449122428894
loss is 1.5672000646591187
loss is 1.4924017190933228
loss is 1.5238875150680542
loss is 1.457541823387146
loss is 1.580887794494629
loss is 1.4812861680984497
loss is 1.4567052125930786
loss is 1.4464025497436523
loss is 1.4864600896835327
loss is 1.4630496501922607
loss is 1.5123913288116455
loss is 1.5981556177139282
loss is 1.3609976768493652
loss is 1.5219072103500366
loss is 1.382657766342163
loss is 1.4751564264297485
loss is 1.5706883668899536
loss is 1.5364723205566406
loss is 1.4199514389038086
loss is 1.505988597869873
loss is 1.4189702272415161
loss is 1.5151073932647705
loss is 1.472163200378418
loss is 1.4716753959655762
loss is 1.5915549993515015
loss is 1.4711270332336426
loss is 1.5387144088745117
loss is 1.5128834247589111
loss is 1.5352836847305298
loss is 1.5725406408309937
loss is 1.530484914779663
loss is 1.5749616622924805
loss is 1.473892092704773
loss is 1.500732421875
loss is 1.557724952697754
loss is 1.4856925010681152
loss is 1.430496335029602
loss is 1.4774603843688965
loss is 1.4837892055511475
loss is 1.5921162366867065
loss is 1.5478206872940063
loss is 1.4861332178115845
loss is 1.4812769889831543
loss is 1.4593876600265503
loss is 1.4916410446166992
loss is 1.598246455192566
loss is 1.5715820789337158
loss is 1.420445442199707
loss is 1.4679620265960693
loss is 1.5819109678268433
loss is 1.550083875656128
loss is 1.5272976160049438
loss is 1.5157665014266968
loss is 1.5380750894546509
loss is 1.5721646547317505
loss is 1.5675861835479736
loss is 1.6183363199234009
loss is 1.4638901948928833
loss is 1.505747675895691
loss is 1.5689221620559692
loss is 1.4710509777069092
loss is 1.5638632774353027
loss is 1.555148720741272
loss is 1.4530051946640015
loss is 1.509966254234314
loss is 1.5201689004898071
loss is 1.610121250152588
loss is 1.4695466756820679
loss is 1.3892260789871216
loss is 1.5109434127807617
loss is 1.5844182968139648
loss is 1.5137906074523926
loss is 1.5071791410446167
loss is 1.5632320642471313
loss is 1.5111972093582153
loss is 1.5557599067687988
loss is 1.5237431526184082
loss is 1.5036990642547607
loss is 1.5269755125045776
loss is 1.5250935554504395
loss is 1.4436132907867432
loss is 1.5261483192443848
loss is 1.522151231765747
loss is 1.6090034246444702
loss is 1.4735499620437622
loss is 1.578750491142273
loss is 1.5300921201705933
loss is 1.455102801322937
loss is 1.5831695795059204
loss is 1.4366666078567505
loss is 1.5593559741973877
loss is 1.5063623189926147
loss is 1.5617625713348389
loss is 1.4960726499557495
loss is 1.4828811883926392
loss is 1.4226337671279907
loss is 1.4824894666671753
loss is 1.4995851516723633
loss is 1.4082896709442139
loss is 1.5767916440963745
loss is 1.4791048765182495
loss is 1.4641348123550415
loss is 1.465032935142517
loss is 1.5182956457138062
loss is 1.4496142864227295
loss is 1.590905785560608
loss is 1.4158881902694702
loss is 1.508378267288208
loss is 1.4748725891113281
loss is 1.5666344165802002
loss is 1.548535943031311
loss is 1.5565104484558105
loss is 1.5173732042312622
loss is 1.4131133556365967
loss is 1.6130489110946655
loss is 1.4496068954467773
loss is 1.5664535760879517
loss is 1.536422610282898
loss is 1.5415370464324951
loss is 1.51960027217865
loss is 1.4770562648773193
loss is 1.5348032712936401
loss is 1.4695605039596558
loss is 1.4589065313339233
loss is 1.3974744081497192
loss is 1.5286533832550049
loss is 1.4940557479858398
loss is 1.5028342008590698
loss is 1.418538212776184
loss is 1.53444504737854
loss is 1.589113473892212
loss is 1.4624398946762085
loss is 1.5447611808776855
loss is 1.563055157661438
loss is 1.487175464630127
loss is 1.5193567276000977
loss is 1.4654356241226196
loss is 1.504525899887085
loss is 1.4401462078094482
loss is 1.442803144454956
loss is 1.5893112421035767
loss is 1.5082533359527588
loss is 1.3881815671920776
loss is 1.5454399585723877
loss is 1.5322514772415161
loss is 1.4596869945526123
loss is 1.4766725301742554
loss is 1.4628093242645264
loss is 1.5706696510314941
loss is 1.538978934288025
loss is 1.4912784099578857
loss is 1.5088621377944946
loss is 1.6014056205749512
loss is 1.5428838729858398
loss is 1.5062003135681152
loss is 1.5722101926803589
loss is 1.5906933546066284
loss is 1.5349208116531372
loss is 1.4436830282211304
loss is 1.428023338317871
loss is 1.422768235206604
loss is 1.4952034950256348
loss is 1.5201600790023804
loss is 1.364790916442871
loss is 1.5716125965118408
loss is 1.4830451011657715
loss is 1.6103050708770752
loss is 1.562625765800476
loss is 1.4664329290390015
loss is 1.5605548620224
loss is 1.3737825155258179
loss is 1.502540946006775
loss is 1.505467176437378
loss is 1.507771611213684
loss is 1.501405954360962
loss is 1.581458330154419
loss is 1.4133449792861938
loss is 1.5149213075637817
loss is 1.5120036602020264
loss is 1.487584114074707
loss is 1.4456510543823242
loss is 1.4555469751358032
loss is 1.408945918083191
loss is 1.6280382871627808
loss is 1.5899882316589355
loss is 1.6173063516616821
loss is 1.4685249328613281
loss is 1.5365591049194336
loss is 1.4899803400039673
loss is 1.4731618165969849
loss is 1.6176751852035522
loss is 1.4350061416625977
loss is 1.4515175819396973
loss is 1.5630494356155396
loss is 1.4973608255386353
loss is 1.4782029390335083
loss is 1.6292015314102173
loss is 1.437890648841858
loss is 1.539745807647705
loss is 1.387865424156189
loss is 1.5867531299591064
loss is 1.34201979637146
loss is 1.519974946975708
loss is 1.4967541694641113
loss is 1.4594520330429077
loss is 1.5672944784164429
loss is 1.518326759338379
loss is 1.5010875463485718
loss is 1.5036391019821167
loss is 1.5144885778427124
loss is 1.441685676574707
loss is 1.5184561014175415
loss is 1.4416382312774658
loss is 1.515163540840149
loss is 1.4284636974334717
loss is 1.4821244478225708
loss is 1.5115361213684082
loss is 1.4635449647903442
loss is 1.4971216917037964
loss is 1.4861540794372559
loss is 1.4031635522842407
loss is 1.4747898578643799
loss is 1.4458298683166504
loss is 1.439521312713623
loss is 1.3946183919906616
loss is 1.3679099082946777
loss is 1.4631364345550537
loss is 1.5780572891235352
loss is 1.4500328302383423
loss is 1.4037752151489258
loss is 1.5742218494415283
loss is 1.4573395252227783
loss is 1.5438275337219238
loss is 1.4252305030822754
loss is 1.5650345087051392
loss is 1.5082931518554688
loss is 1.3697701692581177
loss is 1.4553978443145752
loss is 1.5164778232574463
loss is 1.5114855766296387
loss is 1.468589425086975
loss is 1.4156118631362915
loss is 1.5497535467147827
loss is 1.4522373676300049
loss is 1.4695172309875488
epoch 7: train_loss = 1.509
7: {'Accuracy': 0.5022, 'Precision': 0.506, 'Recall': 0.4967, 'F1-score': 0.4912}
epoch: 8
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:12,  1.51it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:32, 11.76it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:17, 21.17it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:00<00:12, 29.68it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 36.79it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 42.42it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 46.79it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 50.04it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 52.48it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 54.22it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 55.50it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 56.39it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 57.03it/s]going through batches for holmes training:  21%|██        | 79/384 [00:01<00:05, 57.56it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.91it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 58.08it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 58.27it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 58.38it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 58.46it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 58.52it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 58.64it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.72it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.57it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 58.59it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.66it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.57it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.59it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.61it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.72it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.64it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:03<00:03, 58.63it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.63it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.58it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.49it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.49it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.50it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.65it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.66it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.65it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.61it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:04<00:02, 58.64it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.60it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.63it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.63it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.56it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.67it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.62it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.61it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.53it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 58.59it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:05<00:01, 58.58it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.58it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 58.56it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.57it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.58it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 58.47it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.46it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.49it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.57it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:06<00:00, 58.21it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 58.37it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.64it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 53.00it/s]
loss is 1.4692528247833252
loss is 1.5038341283798218
loss is 1.3840880393981934
loss is 1.4729554653167725
loss is 1.3717570304870605
loss is 1.453568935394287
loss is 1.4387125968933105
loss is 1.469603419303894
loss is 1.4228684902191162
loss is 1.5470068454742432
loss is 1.4653329849243164
loss is 1.3517508506774902
loss is 1.5318049192428589
loss is 1.5757954120635986
loss is 1.571530818939209
loss is 1.4921625852584839
loss is 1.5636769533157349
loss is 1.4640278816223145
loss is 1.3747649192810059
loss is 1.442637324333191
loss is 1.5359114408493042
loss is 1.4767106771469116
loss is 1.3604652881622314
loss is 1.4877614974975586
loss is 1.42670476436615
loss is 1.4441910982131958
loss is 1.4458285570144653
loss is 1.426793098449707
loss is 1.4074645042419434
loss is 1.395424485206604
loss is 1.4741086959838867
loss is 1.4331228733062744
loss is 1.4213011264801025
loss is 1.4004032611846924
loss is 1.3998918533325195
loss is 1.4383410215377808
loss is 1.465541124343872
loss is 1.3453140258789062
loss is 1.559261441230774
loss is 1.4891046285629272
loss is 1.3832192420959473
loss is 1.36031174659729
loss is 1.3931260108947754
loss is 1.5438177585601807
loss is 1.5169094800949097
loss is 1.502468228340149
loss is 1.4040210247039795
loss is 1.4410160779953003
loss is 1.4553046226501465
loss is 1.4939686059951782
loss is 1.4916915893554688
loss is 1.4730421304702759
loss is 1.4959328174591064
loss is 1.501624345779419
loss is 1.4778388738632202
loss is 1.4670524597167969
loss is 1.409084677696228
loss is 1.5259290933609009
loss is 1.4873976707458496
loss is 1.4112647771835327
loss is 1.399013638496399
loss is 1.4973466396331787
loss is 1.435206651687622
loss is 1.5338218212127686
loss is 1.528070092201233
loss is 1.4357773065567017
loss is 1.5096673965454102
loss is 1.5007154941558838
loss is 1.5762934684753418
loss is 1.5478345155715942
loss is 1.295217752456665
loss is 1.3855595588684082
loss is 1.4300782680511475
loss is 1.3706992864608765
loss is 1.3126351833343506
loss is 1.5535576343536377
loss is 1.401208758354187
loss is 1.5030906200408936
loss is 1.4060395956039429
loss is 1.448592185974121
loss is 1.3973174095153809
loss is 1.3861829042434692
loss is 1.439239501953125
loss is 1.4949350357055664
loss is 1.4449594020843506
loss is 1.4615411758422852
loss is 1.468302607536316
loss is 1.4089572429656982
loss is 1.529280185699463
loss is 1.5901936292648315
loss is 1.4273451566696167
loss is 1.4167914390563965
loss is 1.5759663581848145
loss is 1.4277713298797607
loss is 1.454007625579834
loss is 1.4322853088378906
loss is 1.5701130628585815
loss is 1.3898024559020996
loss is 1.4357341527938843
loss is 1.4672390222549438
loss is 1.4467859268188477
loss is 1.356958270072937
loss is 1.5052461624145508
loss is 1.5602983236312866
loss is 1.4496556520462036
loss is 1.555799961090088
loss is 1.438755989074707
loss is 1.4997934103012085
loss is 1.5083132982254028
loss is 1.433630347251892
loss is 1.5239269733428955
loss is 1.416312575340271
loss is 1.5277810096740723
loss is 1.4788624048233032
loss is 1.4175097942352295
loss is 1.4057655334472656
loss is 1.482479453086853
loss is 1.4211703538894653
loss is 1.4204639196395874
loss is 1.4363768100738525
loss is 1.6146866083145142
loss is 1.4429951906204224
loss is 1.3770257234573364
loss is 1.4521205425262451
loss is 1.464047908782959
loss is 1.5327163934707642
loss is 1.473465085029602
loss is 1.454535961151123
loss is 1.368654727935791
loss is 1.4761269092559814
loss is 1.5267149209976196
loss is 1.4435445070266724
loss is 1.488906979560852
loss is 1.3479260206222534
loss is 1.4336546659469604
loss is 1.4691616296768188
loss is 1.3879870176315308
loss is 1.4384987354278564
loss is 1.4992775917053223
loss is 1.4265538454055786
loss is 1.3785101175308228
loss is 1.3235411643981934
loss is 1.4012160301208496
loss is 1.3731151819229126
loss is 1.4800324440002441
loss is 1.3984174728393555
loss is 1.4292418956756592
loss is 1.510402798652649
loss is 1.5446929931640625
loss is 1.5082446336746216
loss is 1.4278541803359985
loss is 1.4648756980895996
loss is 1.3752425909042358
loss is 1.418088436126709
loss is 1.4505667686462402
loss is 1.567947268486023
loss is 1.46242094039917
loss is 1.5539172887802124
loss is 1.4038435220718384
loss is 1.4941974878311157
loss is 1.4409286975860596
loss is 1.3704397678375244
loss is 1.330622911453247
loss is 1.5582106113433838
loss is 1.5345163345336914
loss is 1.3658702373504639
loss is 1.367470383644104
loss is 1.367152452468872
loss is 1.4337085485458374
loss is 1.5506457090377808
loss is 1.5711497068405151
loss is 1.462767481803894
loss is 1.4260555505752563
loss is 1.3825786113739014
loss is 1.3869096040725708
loss is 1.4870113134384155
loss is 1.4686775207519531
loss is 1.4054651260375977
loss is 1.4067343473434448
loss is 1.606967806816101
loss is 1.5044777393341064
loss is 1.4086670875549316
loss is 1.4666529893875122
loss is 1.406724452972412
loss is 1.4764043092727661
loss is 1.422154188156128
loss is 1.449751853942871
loss is 1.6213263273239136
loss is 1.4553052186965942
loss is 1.4392503499984741
loss is 1.5683053731918335
loss is 1.389022707939148
loss is 1.4611538648605347
loss is 1.5684919357299805
loss is 1.387069582939148
loss is 1.3453365564346313
loss is 1.4772307872772217
loss is 1.288425326347351
loss is 1.3788343667984009
loss is 1.5390704870224
loss is 1.4537485837936401
loss is 1.4577453136444092
loss is 1.3566585779190063
loss is 1.4501920938491821
loss is 1.3806039094924927
loss is 1.5421321392059326
loss is 1.3800207376480103
loss is 1.469571590423584
loss is 1.4067714214324951
loss is 1.390722393989563
loss is 1.3232733011245728
loss is 1.3891719579696655
loss is 1.5760259628295898
loss is 1.52592134475708
loss is 1.486831784248352
loss is 1.4763866662979126
loss is 1.3979829549789429
loss is 1.5320119857788086
loss is 1.4643362760543823
loss is 1.4090778827667236
loss is 1.4420151710510254
loss is 1.5820921659469604
loss is 1.536110520362854
loss is 1.4138973951339722
loss is 1.5089426040649414
loss is 1.4561724662780762
loss is 1.4366604089736938
loss is 1.4457951784133911
loss is 1.4542760848999023
loss is 1.3702094554901123
loss is 1.3870322704315186
loss is 1.479811191558838
loss is 1.419093370437622
loss is 1.4474912881851196
loss is 1.4466770887374878
loss is 1.4387421607971191
loss is 1.429856777191162
loss is 1.4503920078277588
loss is 1.4953031539916992
loss is 1.3936190605163574
loss is 1.507642388343811
loss is 1.4795973300933838
loss is 1.41558837890625
loss is 1.4708147048950195
loss is 1.4906792640686035
loss is 1.458277702331543
loss is 1.5364832878112793
loss is 1.535343050956726
loss is 1.4106160402297974
loss is 1.2776294946670532
loss is 1.516114354133606
loss is 1.526126503944397
loss is 1.346047043800354
loss is 1.4355988502502441
loss is 1.4286046028137207
loss is 1.5113834142684937
loss is 1.467484474182129
loss is 1.4263304471969604
loss is 1.4872217178344727
loss is 1.4130663871765137
loss is 1.4332976341247559
loss is 1.458289623260498
loss is 1.3365015983581543
loss is 1.5270854234695435
loss is 1.5184727907180786
loss is 1.4431982040405273
loss is 1.3950341939926147
loss is 1.5069987773895264
loss is 1.5056228637695312
loss is 1.4337764978408813
loss is 1.479634404182434
loss is 1.3744378089904785
loss is 1.3795950412750244
loss is 1.5079113245010376
loss is 1.4719799757003784
loss is 1.432361125946045
loss is 1.4736627340316772
loss is 1.4773832559585571
loss is 1.5440768003463745
loss is 1.4111361503601074
loss is 1.390242338180542
loss is 1.5364587306976318
loss is 1.5461453199386597
loss is 1.4563078880310059
loss is 1.428269624710083
loss is 1.4207881689071655
loss is 1.472266435623169
loss is 1.419977068901062
loss is 1.412748098373413
loss is 1.5614429712295532
loss is 1.492207646369934
loss is 1.3851426839828491
loss is 1.4347631931304932
loss is 1.501389503479004
loss is 1.4347901344299316
loss is 1.5267395973205566
loss is 1.4118075370788574
loss is 1.5174387693405151
loss is 1.3611303567886353
loss is 1.4787746667861938
loss is 1.4046542644500732
loss is 1.4538975954055786
loss is 1.4616317749023438
loss is 1.3685182332992554
loss is 1.4617362022399902
loss is 1.4621169567108154
loss is 1.487879753112793
loss is 1.4209474325180054
loss is 1.46482253074646
loss is 1.4606226682662964
loss is 1.4175297021865845
loss is 1.4848508834838867
loss is 1.4748620986938477
loss is 1.5037082433700562
loss is 1.5135959386825562
loss is 1.3864463567733765
loss is 1.3578250408172607
loss is 1.4100230932235718
loss is 1.3636558055877686
loss is 1.4136210680007935
loss is 1.4397916793823242
loss is 1.5063952207565308
loss is 1.400974988937378
loss is 1.3996984958648682
loss is 1.4528770446777344
loss is 1.337393045425415
loss is 1.4270548820495605
loss is 1.3289730548858643
loss is 1.4310342073440552
loss is 1.4583064317703247
loss is 1.41801118850708
loss is 1.5205615758895874
loss is 1.5056941509246826
loss is 1.481699824333191
loss is 1.405755639076233
loss is 1.3439067602157593
loss is 1.3896199464797974
loss is 1.4462249279022217
loss is 1.4134372472763062
loss is 1.4199975728988647
loss is 1.4547085762023926
loss is 1.3980828523635864
loss is 1.5269427299499512
loss is 1.4575902223587036
loss is 1.434701681137085
loss is 1.425421953201294
loss is 1.4235244989395142
loss is 1.480015754699707
loss is 1.4333781003952026
loss is 1.457044243812561
loss is 1.4097869396209717
loss is 1.434909462928772
loss is 1.4704698324203491
loss is 1.3933050632476807
loss is 1.472439169883728
loss is 1.399462103843689
loss is 1.31141996383667
loss is 1.5044609308242798
loss is 1.3491290807724
loss is 1.4651236534118652
loss is 1.3973640203475952
loss is 1.3683689832687378
loss is 1.4921396970748901
loss is 1.450901746749878
loss is 1.4290225505828857
loss is 1.3711680173873901
loss is 1.3561677932739258
loss is 1.419878363609314
loss is 1.4658000469207764
loss is 1.455165147781372
loss is 1.3956291675567627
loss is 1.5227454900741577
loss is 1.4350045919418335
loss is 1.529300332069397
loss is 1.3299131393432617
loss is 1.3739417791366577
loss is 1.42118239402771
loss is 1.470646858215332
loss is 1.4329372644424438
loss is 1.3949350118637085
loss is 1.4879378080368042
loss is 1.460506558418274
loss is 1.3959605693817139
loss is 1.2589225769042969
epoch 8: train_loss = 1.449
8: {'Accuracy': 0.5117, 'Precision': 0.5232, 'Recall': 0.5064, 'F1-score': 0.504}
epoch: 9
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:23,  1.45it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:33, 11.27it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:17, 20.64it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:12, 29.04it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 36.17it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 41.73it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 46.08it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 49.50it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 52.07it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.89it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 55.22it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 56.12it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.90it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 57.42it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.79it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 58.00it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 58.25it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 58.43it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 58.44it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 58.54it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 58.55it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.56it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.63it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 58.67it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.60it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.67it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.65it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.67it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.56it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:03<00:03, 58.64it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.70it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.62it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.61it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:03, 55.48it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 56.22it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 56.90it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 57.37it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 57.63it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 57.95it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 57.91it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 57.91it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 57.95it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 57.86it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 57.83it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.10it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.24it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.33it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.48it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 58.52it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.58it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.60it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 58.63it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.69it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.63it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 58.64it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.64it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.63it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.60it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:06<00:00, 58.20it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 58.44it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.61it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.58it/s]
loss is 1.3296046257019043
loss is 1.345731258392334
loss is 1.3529475927352905
loss is 1.51478910446167
loss is 1.4052051305770874
loss is 1.4053748846054077
loss is 1.5155028104782104
loss is 1.4176701307296753
loss is 1.4013177156448364
loss is 1.3673347234725952
loss is 1.4894267320632935
loss is 1.354630470275879
loss is 1.4383894205093384
loss is 1.4009953737258911
loss is 1.369492769241333
loss is 1.2945491075515747
loss is 1.3089832067489624
loss is 1.469286322593689
loss is 1.4644365310668945
loss is 1.400935173034668
loss is 1.3997018337249756
loss is 1.3693325519561768
loss is 1.2815905809402466
loss is 1.4542670249938965
loss is 1.3509389162063599
loss is 1.3520535230636597
loss is 1.3958224058151245
loss is 1.3708038330078125
loss is 1.4068831205368042
loss is 1.3724592924118042
loss is 1.3920512199401855
loss is 1.5691819190979004
loss is 1.3669253587722778
loss is 1.3407180309295654
loss is 1.3154008388519287
loss is 1.444369912147522
loss is 1.4090098142623901
loss is 1.2996230125427246
loss is 1.519128441810608
loss is 1.3019458055496216
loss is 1.3232630491256714
loss is 1.4484387636184692
loss is 1.3739444017410278
loss is 1.4326560497283936
loss is 1.2805339097976685
loss is 1.4700387716293335
loss is 1.361128330230713
loss is 1.220197319984436
loss is 1.422255277633667
loss is 1.3691132068634033
loss is 1.458545207977295
loss is 1.429331660270691
loss is 1.384494423866272
loss is 1.410722255706787
loss is 1.3667024374008179
loss is 1.382796049118042
loss is 1.3872228860855103
loss is 1.371148705482483
loss is 1.430495023727417
loss is 1.3975237607955933
loss is 1.358920693397522
loss is 1.2962373495101929
loss is 1.3395593166351318
loss is 1.4950058460235596
loss is 1.4965373277664185
loss is 1.3811514377593994
loss is 1.4684422016143799
loss is 1.3086977005004883
loss is 1.387087345123291
loss is 1.5503287315368652
loss is 1.434688687324524
loss is 1.261677622795105
loss is 1.450394630432129
loss is 1.4913612604141235
loss is 1.3757071495056152
loss is 1.4048092365264893
loss is 1.3102569580078125
loss is 1.5203956365585327
loss is 1.4405148029327393
loss is 1.3830540180206299
loss is 1.4929299354553223
loss is 1.3846110105514526
loss is 1.400766134262085
loss is 1.4503092765808105
loss is 1.4127331972122192
loss is 1.461751103401184
loss is 1.4465826749801636
loss is 1.5532431602478027
loss is 1.395697832107544
loss is 1.3821803331375122
loss is 1.4334547519683838
loss is 1.3538775444030762
loss is 1.3944216966629028
loss is 1.3279370069503784
loss is 1.4099841117858887
loss is 1.4560459852218628
loss is 1.424318790435791
loss is 1.3992804288864136
loss is 1.4427924156188965
loss is 1.3784524202346802
loss is 1.4452366828918457
loss is 1.3656784296035767
loss is 1.3506157398223877
loss is 1.497098684310913
loss is 1.4225245714187622
loss is 1.389506220817566
loss is 1.3745911121368408
loss is 1.4366880655288696
loss is 1.4348758459091187
loss is 1.4202184677124023
loss is 1.5171107053756714
loss is 1.4170281887054443
loss is 1.4703716039657593
loss is 1.430389404296875
loss is 1.4950953722000122
loss is 1.3412301540374756
loss is 1.4221086502075195
loss is 1.3596693277359009
loss is 1.3619345426559448
loss is 1.429703950881958
loss is 1.362001657485962
loss is 1.3491368293762207
loss is 1.4478027820587158
loss is 1.4232670068740845
loss is 1.2904763221740723
loss is 1.3087074756622314
loss is 1.4890830516815186
loss is 1.4014954566955566
loss is 1.4641772508621216
loss is 1.454941749572754
loss is 1.3610416650772095
loss is 1.350691795349121
loss is 1.430841088294983
loss is 1.4944111108779907
loss is 1.4242950677871704
loss is 1.4354671239852905
loss is 1.4567815065383911
loss is 1.4284672737121582
loss is 1.4212838411331177
loss is 1.4381263256072998
loss is 1.4654675722122192
loss is 1.3866996765136719
loss is 1.4269698858261108
loss is 1.361527681350708
loss is 1.407602071762085
loss is 1.3112255334854126
loss is 1.3706812858581543
loss is 1.3271455764770508
loss is 1.4330406188964844
loss is 1.376777172088623
loss is 1.4273806810379028
loss is 1.4562623500823975
loss is 1.4342148303985596
loss is 1.4109899997711182
loss is 1.3850953578948975
loss is 1.4592519998550415
loss is 1.4445668458938599
loss is 1.3412377834320068
loss is 1.3086671829223633
loss is 1.4300428628921509
loss is 1.4523321390151978
loss is 1.3883388042449951
loss is 1.4051601886749268
loss is 1.3109883069992065
loss is 1.4740312099456787
loss is 1.4596554040908813
loss is 1.3254836797714233
loss is 1.3823974132537842
loss is 1.41740083694458
loss is 1.411868929862976
loss is 1.4265894889831543
loss is 1.3417783975601196
loss is 1.3726528882980347
loss is 1.4326783418655396
loss is 1.4909666776657104
loss is 1.3210442066192627
loss is 1.412431001663208
loss is 1.3888682126998901
loss is 1.4139618873596191
loss is 1.391495943069458
loss is 1.3900738954544067
loss is 1.3277093172073364
loss is 1.5015366077423096
loss is 1.4989192485809326
loss is 1.345656394958496
loss is 1.4323093891143799
loss is 1.3579397201538086
loss is 1.439131736755371
loss is 1.5034047365188599
loss is 1.3749972581863403
loss is 1.4167152643203735
loss is 1.4244836568832397
loss is 1.4699397087097168
loss is 1.351039171218872
loss is 1.426173448562622
loss is 1.3095165491104126
loss is 1.391740322113037
loss is 1.2992043495178223
loss is 1.3326023817062378
loss is 1.3892245292663574
loss is 1.4102023839950562
loss is 1.4601608514785767
loss is 1.401107907295227
loss is 1.4585403203964233
loss is 1.2854264974594116
loss is 1.4583733081817627
loss is 1.418817400932312
loss is 1.3402106761932373
loss is 1.4302293062210083
loss is 1.4034974575042725
loss is 1.4515678882598877
loss is 1.3719816207885742
loss is 1.4251124858856201
loss is 1.3142238855361938
loss is 1.2860134840011597
loss is 1.3835384845733643
loss is 1.5228755474090576
loss is 1.3441052436828613
loss is 1.4212164878845215
loss is 1.4376463890075684
loss is 1.3632049560546875
loss is 1.3735123872756958
loss is 1.3635629415512085
loss is 1.3747929334640503
loss is 1.339239239692688
loss is 1.3988277912139893
loss is 1.405470609664917
loss is 1.4088828563690186
loss is 1.4350295066833496
loss is 1.3737314939498901
loss is 1.4005343914031982
loss is 1.3530675172805786
loss is 1.477734088897705
loss is 1.4887615442276
loss is 1.397822618484497
loss is 1.457352876663208
loss is 1.3188254833221436
loss is 1.3929967880249023
loss is 1.3346116542816162
loss is 1.277482509613037
loss is 1.3795156478881836
loss is 1.4822157621383667
loss is 1.4447646141052246
loss is 1.4711601734161377
loss is 1.4205039739608765
loss is 1.5174815654754639
loss is 1.4181816577911377
loss is 1.4339135885238647
loss is 1.3504021167755127
loss is 1.4739584922790527
loss is 1.377336025238037
loss is 1.436416506767273
loss is 1.5847084522247314
loss is 1.3955270051956177
loss is 1.4162631034851074
loss is 1.4609490633010864
loss is 1.4125639200210571
loss is 1.4720817804336548
loss is 1.3769500255584717
loss is 1.4603583812713623
loss is 1.447354793548584
loss is 1.359655499458313
loss is 1.3593597412109375
loss is 1.465938687324524
loss is 1.3094691038131714
loss is 1.4455369710922241
loss is 1.3569250106811523
loss is 1.4283031225204468
loss is 1.3574484586715698
loss is 1.4238461256027222
loss is 1.3864343166351318
loss is 1.4969983100891113
loss is 1.4835439920425415
loss is 1.3855823278427124
loss is 1.327651023864746
loss is 1.3403390645980835
loss is 1.3040698766708374
loss is 1.3964629173278809
loss is 1.4449763298034668
loss is 1.4580093622207642
loss is 1.4304238557815552
loss is 1.4081182479858398
loss is 1.2907887697219849
loss is 1.441885232925415
loss is 1.4034106731414795
loss is 1.233615517616272
loss is 1.3893241882324219
loss is 1.423708438873291
loss is 1.4369970560073853
loss is 1.4679614305496216
loss is 1.3542650938034058
loss is 1.4459878206253052
loss is 1.376820683479309
loss is 1.3451452255249023
loss is 1.4117958545684814
loss is 1.45560622215271
loss is 1.3794578313827515
loss is 1.4096678495407104
loss is 1.439815878868103
loss is 1.444154977798462
loss is 1.4405317306518555
loss is 1.353855013847351
loss is 1.4408013820648193
loss is 1.306766390800476
loss is 1.4040721654891968
loss is 1.4217222929000854
loss is 1.355629563331604
loss is 1.3841992616653442
loss is 1.4696694612503052
loss is 1.2832814455032349
loss is 1.3349580764770508
loss is 1.4846975803375244
loss is 1.3753907680511475
loss is 1.3444781303405762
loss is 1.3710054159164429
loss is 1.3872877359390259
loss is 1.4254064559936523
loss is 1.321332573890686
loss is 1.4051333665847778
loss is 1.3527815341949463
loss is 1.3687481880187988
loss is 1.2762174606323242
loss is 1.3784019947052002
loss is 1.3336994647979736
loss is 1.3623744249343872
loss is 1.297093391418457
loss is 1.3614070415496826
loss is 1.4509446620941162
loss is 1.358120322227478
loss is 1.4224668741226196
loss is 1.3071917295455933
loss is 1.3251348733901978
loss is 1.4342471361160278
loss is 1.4079697132110596
loss is 1.2920609712600708
loss is 1.3901429176330566
loss is 1.3431596755981445
loss is 1.4011056423187256
loss is 1.4285277128219604
loss is 1.4038093090057373
loss is 1.462684154510498
loss is 1.5312085151672363
loss is 1.4902622699737549
loss is 1.3744736909866333
loss is 1.3717387914657593
loss is 1.3721450567245483
loss is 1.2385871410369873
loss is 1.4140594005584717
loss is 1.3898800611495972
loss is 1.4215846061706543
loss is 1.4467954635620117
loss is 1.4872225522994995
loss is 1.4460679292678833
loss is 1.3495503664016724
loss is 1.4714564085006714
loss is 1.2709976434707642
loss is 1.3712255954742432
loss is 1.3802012205123901
loss is 1.3891738653182983
loss is 1.4686046838760376
loss is 1.4078950881958008
loss is 1.4473958015441895
loss is 1.294790506362915
loss is 1.2775490283966064
loss is 1.3731127977371216
loss is 1.4683258533477783
loss is 1.3960107564926147
loss is 1.394696831703186
loss is 1.320022702217102
loss is 1.4156891107559204
loss is 1.372900366783142
loss is 1.4290639162063599
loss is 1.3272650241851807
loss is 1.4793106317520142
loss is 1.4153072834014893
loss is 1.387182593345642
loss is 1.3431280851364136
loss is 1.4185789823532104
loss is 1.3912736177444458
loss is 1.4511843919754028
loss is 1.377988576889038
loss is 1.4281182289123535
loss is 1.3377805948257446
loss is 1.4016294479370117
epoch 9: train_loss = 1.4
9: {'Accuracy': 0.5229, 'Precision': 0.5275, 'Recall': 0.5198, 'F1-score': 0.5188}
epoch: 10
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:04,  1.57it/s]going through batches for holmes training:   1%|▏         | 5/384 [00:00<00:45,  8.38it/s]going through batches for holmes training:   3%|▎         | 11/384 [00:00<00:19, 18.74it/s]going through batches for holmes training:   5%|▍         | 18/384 [00:00<00:12, 29.12it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 37.29it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 42.59it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 46.79it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 50.03it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 52.43it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 54.24it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 55.56it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 56.44it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 57.04it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 57.51it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.88it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 58.11it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 58.26it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 58.34it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 58.51it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 58.59it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 58.60it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.59it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.62it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 58.58it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.64it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:03, 58.68it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.67it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.69it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.70it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.63it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.66it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.72it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:03<00:03, 58.73it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.65it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.64it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.63it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.10it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.33it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.42it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.44it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.46it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.53it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:04<00:02, 58.57it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.52it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.46it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.46it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.48it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.53it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.57it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.64it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.61it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 58.58it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.56it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.62it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 58.59it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.59it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.69it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 58.68it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.65it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.63it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.71it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:06<00:00, 58.28it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 58.44it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.72it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.93it/s]
loss is 1.2980561256408691
loss is 1.3902348279953003
loss is 1.3409857749938965
loss is 1.358384132385254
loss is 1.347010612487793
loss is 1.3820899724960327
loss is 1.3110878467559814
loss is 1.457068920135498
loss is 1.3599520921707153
loss is 1.280177116394043
loss is 1.3557100296020508
loss is 1.368404746055603
loss is 1.3421790599822998
loss is 1.3280128240585327
loss is 1.3624827861785889
loss is 1.2668461799621582
loss is 1.3519141674041748
loss is 1.2731901407241821
loss is 1.263437271118164
loss is 1.3143421411514282
loss is 1.3979811668395996
loss is 1.338647723197937
loss is 1.3291990756988525
loss is 1.3580724000930786
loss is 1.2724343538284302
loss is 1.3916826248168945
loss is 1.3442285060882568
loss is 1.336057424545288
loss is 1.3368709087371826
loss is 1.3506684303283691
loss is 1.3165028095245361
loss is 1.2404694557189941
loss is 1.3117860555648804
loss is 1.461167335510254
loss is 1.3887608051300049
loss is 1.3493784666061401
loss is 1.3212313652038574
loss is 1.3645446300506592
loss is 1.2434043884277344
loss is 1.1789382696151733
loss is 1.3419626951217651
loss is 1.2518062591552734
loss is 1.3734276294708252
loss is 1.256535530090332
loss is 1.433964729309082
loss is 1.276731014251709
loss is 1.358657717704773
loss is 1.3306281566619873
loss is 1.298007845878601
loss is 1.3838365077972412
loss is 1.3980152606964111
loss is 1.3473527431488037
loss is 1.2809985876083374
loss is 1.4272688627243042
loss is 1.255415439605713
loss is 1.3465298414230347
loss is 1.4486485719680786
loss is 1.4625109434127808
loss is 1.383612036705017
loss is 1.3419827222824097
loss is 1.4227302074432373
loss is 1.442550778388977
loss is 1.3398720026016235
loss is 1.317999005317688
loss is 1.3472660779953003
loss is 1.4923068284988403
loss is 1.3931858539581299
loss is 1.4301774501800537
loss is 1.297751784324646
loss is 1.430434226989746
loss is 1.3361790180206299
loss is 1.251010775566101
loss is 1.3593864440917969
loss is 1.4475123882293701
loss is 1.4291322231292725
loss is 1.35093092918396
loss is 1.257957100868225
loss is 1.2457503080368042
loss is 1.3210062980651855
loss is 1.3107322454452515
loss is 1.3347259759902954
loss is 1.4220538139343262
loss is 1.351378321647644
loss is 1.2915503978729248
loss is 1.3171426057815552
loss is 1.3562175035476685
loss is 1.403245210647583
loss is 1.2736964225769043
loss is 1.4210991859436035
loss is 1.3056001663208008
loss is 1.2973487377166748
loss is 1.2656073570251465
loss is 1.4429949522018433
loss is 1.3768794536590576
loss is 1.3017184734344482
loss is 1.415724754333496
loss is 1.4149686098098755
loss is 1.3966288566589355
loss is 1.4059617519378662
loss is 1.291780710220337
loss is 1.300032377243042
loss is 1.3881536722183228
loss is 1.2022311687469482
loss is 1.293013572692871
loss is 1.4880915880203247
loss is 1.4202699661254883
loss is 1.3213504552841187
loss is 1.3753857612609863
loss is 1.380056619644165
loss is 1.2458891868591309
loss is 1.3685030937194824
loss is 1.4511511325836182
loss is 1.4364275932312012
loss is 1.244216799736023
loss is 1.2839787006378174
loss is 1.383211374282837
loss is 1.4496744871139526
loss is 1.304632544517517
loss is 1.3665611743927002
loss is 1.4141088724136353
loss is 1.3976632356643677
loss is 1.3407820463180542
loss is 1.2599077224731445
loss is 1.450624704360962
loss is 1.3267309665679932
loss is 1.2196040153503418
loss is 1.367842197418213
loss is 1.4118030071258545
loss is 1.3129295110702515
loss is 1.3362298011779785
loss is 1.3666340112686157
loss is 1.4387595653533936
loss is 1.2868727445602417
loss is 1.5356903076171875
loss is 1.4100321531295776
loss is 1.4204248189926147
loss is 1.348869800567627
loss is 1.3028247356414795
loss is 1.297986626625061
loss is 1.3997846841812134
loss is 1.257635474205017
loss is 1.3467135429382324
loss is 1.3642834424972534
loss is 1.3586498498916626
loss is 1.445371150970459
loss is 1.4190441370010376
loss is 1.4002217054367065
loss is 1.4043970108032227
loss is 1.4537246227264404
loss is 1.2684657573699951
loss is 1.3644741773605347
loss is 1.414657473564148
loss is 1.3601129055023193
loss is 1.4417911767959595
loss is 1.3316709995269775
loss is 1.4136806726455688
loss is 1.4031425714492798
loss is 1.4819825887680054
loss is 1.331851840019226
loss is 1.3629893064498901
loss is 1.3854727745056152
loss is 1.389824390411377
loss is 1.3696037530899048
loss is 1.3998942375183105
loss is 1.3597729206085205
loss is 1.2213985919952393
loss is 1.284704566001892
loss is 1.3472610712051392
loss is 1.4127204418182373
loss is 1.3304249048233032
loss is 1.3414677381515503
loss is 1.3436732292175293
loss is 1.3385597467422485
loss is 1.3967269659042358
loss is 1.2715380191802979
loss is 1.433542013168335
loss is 1.4483224153518677
loss is 1.3951421976089478
loss is 1.377714991569519
loss is 1.2439664602279663
loss is 1.2858102321624756
loss is 1.376867651939392
loss is 1.4938470125198364
loss is 1.3952044248580933
loss is 1.3086727857589722
loss is 1.317665457725525
loss is 1.4037704467773438
loss is 1.260167121887207
loss is 1.3886409997940063
loss is 1.3970527648925781
loss is 1.2404892444610596
loss is 1.4506943225860596
loss is 1.281899094581604
loss is 1.3432327508926392
loss is 1.268916368484497
loss is 1.3669853210449219
loss is 1.396191120147705
loss is 1.4480403661727905
loss is 1.387566089630127
loss is 1.3394089937210083
loss is 1.474668025970459
loss is 1.3079004287719727
loss is 1.322594165802002
loss is 1.455488681793213
loss is 1.358747959136963
loss is 1.4318509101867676
loss is 1.3384345769882202
loss is 1.414317011833191
loss is 1.2945350408554077
loss is 1.309927225112915
loss is 1.3619892597198486
loss is 1.3007816076278687
loss is 1.269221544265747
loss is 1.4197708368301392
loss is 1.4095302820205688
loss is 1.3716708421707153
loss is 1.374526023864746
loss is 1.2726844549179077
loss is 1.3390932083129883
loss is 1.3553603887557983
loss is 1.2397929430007935
loss is 1.4100996255874634
loss is 1.4097405672073364
loss is 1.4234663248062134
loss is 1.3985382318496704
loss is 1.2592551708221436
loss is 1.3434500694274902
loss is 1.4304933547973633
loss is 1.2648872137069702
loss is 1.3409961462020874
loss is 1.38938307762146
loss is 1.3851051330566406
loss is 1.4808375835418701
loss is 1.3874696493148804
loss is 1.370933175086975
loss is 1.3623963594436646
loss is 1.3614017963409424
loss is 1.3160672187805176
loss is 1.456396460533142
loss is 1.2818455696105957
loss is 1.3549091815948486
loss is 1.4273074865341187
loss is 1.3963671922683716
loss is 1.3002570867538452
loss is 1.3796619176864624
loss is 1.321710467338562
loss is 1.3210421800613403
loss is 1.363998293876648
loss is 1.3669031858444214
loss is 1.3599284887313843
loss is 1.3821005821228027
loss is 1.2262253761291504
loss is 1.4102658033370972
loss is 1.3429772853851318
loss is 1.40456223487854
loss is 1.3709056377410889
loss is 1.2987949848175049
loss is 1.381902813911438
loss is 1.390968918800354
loss is 1.3160868883132935
loss is 1.3926646709442139
loss is 1.281652808189392
loss is 1.3486790657043457
loss is 1.2258578538894653
loss is 1.266913890838623
loss is 1.314030408859253
loss is 1.3611679077148438
loss is 1.2393293380737305
loss is 1.3133671283721924
loss is 1.338823676109314
loss is 1.3485034704208374
loss is 1.2649840116500854
loss is 1.3534681797027588
loss is 1.4093023538589478
loss is 1.3338801860809326
loss is 1.414350152015686
loss is 1.294303297996521
loss is 1.2941006422042847
loss is 1.3554192781448364
loss is 1.3333101272583008
loss is 1.3733233213424683
loss is 1.3434408903121948
loss is 1.3842288255691528
loss is 1.3202502727508545
loss is 1.36741304397583
loss is 1.3879112005233765
loss is 1.4243285655975342
loss is 1.3079652786254883
loss is 1.3334720134735107
loss is 1.4824082851409912
loss is 1.3358174562454224
loss is 1.323910117149353
loss is 1.3076353073120117
loss is 1.3187899589538574
loss is 1.3097267150878906
loss is 1.297402024269104
loss is 1.2948434352874756
loss is 1.421671748161316
loss is 1.321386694908142
loss is 1.3744392395019531
loss is 1.2962281703948975
loss is 1.3025280237197876
loss is 1.2861567735671997
loss is 1.3716756105422974
loss is 1.2549041509628296
loss is 1.3314841985702515
loss is 1.4711754322052002
loss is 1.2911518812179565
loss is 1.4356454610824585
loss is 1.402113437652588
loss is 1.342517375946045
loss is 1.3313456773757935
loss is 1.361643671989441
loss is 1.380213975906372
loss is 1.3513165712356567
loss is 1.366957664489746
loss is 1.3824630975723267
loss is 1.2920783758163452
loss is 1.4172353744506836
loss is 1.3260447978973389
loss is 1.3392077684402466
loss is 1.5206077098846436
loss is 1.3473680019378662
loss is 1.327099323272705
loss is 1.236626386642456
loss is 1.2097119092941284
loss is 1.3310950994491577
loss is 1.341589093208313
loss is 1.288019061088562
loss is 1.2890479564666748
loss is 1.4102520942687988
loss is 1.3397459983825684
loss is 1.3740205764770508
loss is 1.338171362876892
loss is 1.3256118297576904
loss is 1.4337034225463867
loss is 1.312666893005371
loss is 1.2439792156219482
loss is 1.2941386699676514
loss is 1.3067439794540405
loss is 1.3502713441848755
loss is 1.396299123764038
loss is 1.3598833084106445
loss is 1.3363347053527832
loss is 1.392104148864746
loss is 1.3219119310379028
loss is 1.3052078485488892
loss is 1.2503341436386108
loss is 1.2930264472961426
loss is 1.303321123123169
loss is 1.4693374633789062
loss is 1.2662373781204224
loss is 1.3093239068984985
loss is 1.2718327045440674
loss is 1.2994064092636108
loss is 1.298106074333191
loss is 1.4064948558807373
loss is 1.2615845203399658
loss is 1.4531997442245483
loss is 1.420840859413147
loss is 1.4109197854995728
loss is 1.4030381441116333
loss is 1.3529778718948364
loss is 1.295817255973816
loss is 1.2328338623046875
loss is 1.3301419019699097
loss is 1.325630784034729
loss is 1.34555983543396
loss is 1.2985165119171143
loss is 1.412113070487976
loss is 1.3581961393356323
loss is 1.3504754304885864
loss is 1.346040964126587
loss is 1.3791744709014893
loss is 1.4588558673858643
loss is 1.3955731391906738
loss is 1.2467682361602783
loss is 1.3782856464385986
loss is 1.3381474018096924
loss is 1.3490486145019531
loss is 1.3698103427886963
loss is 1.3094879388809204
loss is 1.3686467409133911
loss is 1.2620233297348022
epoch 10: train_loss = 1.351
10: {'Accuracy': 0.5147, 'Precision': 0.5213, 'Recall': 0.5098, 'F1-score': 0.5055}
epoch: 11
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:50,  1.32it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:35, 10.53it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 19.53it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:13, 27.85it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:10, 34.88it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 40.62it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 45.16it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:07, 48.66it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 51.33it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.25it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 54.69it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 55.72it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.41it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.90it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.23it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.47it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 57.63it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.81it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.90it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.89it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.86it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 57.20it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:03<00:04, 57.44it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 57.65it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 57.83it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 57.95it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.06it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.24it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 57.99it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.00it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 57.95it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.00it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 57.99it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.03it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.02it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.05it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.09it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.13it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.10it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.14it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.09it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.16it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 58.16it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.27it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.19it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.09it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.12it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.07it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.07it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.16it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.27it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:06<00:01, 58.37it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.41it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.38it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 58.48it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.40it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.48it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 58.49it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.34it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.08it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.04it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 57.63it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 57.82it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 57.93it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 51.87it/s]
loss is 1.4382107257843018
loss is 1.2862948179244995
loss is 1.239295244216919
loss is 1.3300148248672485
loss is 1.1798347234725952
loss is 1.404662013053894
loss is 1.3217434883117676
loss is 1.2973006963729858
loss is 1.2817704677581787
loss is 1.2748316526412964
loss is 1.448563575744629
loss is 1.4043073654174805
loss is 1.3240809440612793
loss is 1.275626540184021
loss is 1.3448084592819214
loss is 1.3543977737426758
loss is 1.237459421157837
loss is 1.244949221611023
loss is 1.2729812860488892
loss is 1.3381333351135254
loss is 1.4632506370544434
loss is 1.2520872354507446
loss is 1.277735710144043
loss is 1.3846380710601807
loss is 1.2888174057006836
loss is 1.3289709091186523
loss is 1.3093966245651245
loss is 1.3603003025054932
loss is 1.294387936592102
loss is 1.3820044994354248
loss is 1.2984765768051147
loss is 1.2713305950164795
loss is 1.3127104043960571
loss is 1.3346961736679077
loss is 1.3886548280715942
loss is 1.338988184928894
loss is 1.2785745859146118
loss is 1.4008917808532715
loss is 1.282554268836975
loss is 1.420713186264038
loss is 1.2790488004684448
loss is 1.3587192296981812
loss is 1.4170615673065186
loss is 1.3080041408538818
loss is 1.4250537157058716
loss is 1.3259687423706055
loss is 1.2907195091247559
loss is 1.3432226181030273
loss is 1.34865140914917
loss is 1.3626927137374878
loss is 1.4105138778686523
loss is 1.2839646339416504
loss is 1.4250001907348633
loss is 1.2156410217285156
loss is 1.2536976337432861
loss is 1.3219330310821533
loss is 1.3833993673324585
loss is 1.3291685581207275
loss is 1.2587512731552124
loss is 1.325711965560913
loss is 1.2103921175003052
loss is 1.3537933826446533
loss is 1.245904803276062
loss is 1.344293475151062
loss is 1.2879482507705688
loss is 1.2379740476608276
loss is 1.2608087062835693
loss is 1.317521333694458
loss is 1.3644311428070068
loss is 1.2396091222763062
loss is 1.3419692516326904
loss is 1.3912365436553955
loss is 1.2279356718063354
loss is 1.3044134378433228
loss is 1.3586372137069702
loss is 1.3849668502807617
loss is 1.3331637382507324
loss is 1.4043967723846436
loss is 1.293342113494873
loss is 1.2317019701004028
loss is 1.239591360092163
loss is 1.3257144689559937
loss is 1.3641901016235352
loss is 1.3950496912002563
loss is 1.2941051721572876
loss is 1.3351496458053589
loss is 1.241060733795166
loss is 1.2631641626358032
loss is 1.2182941436767578
loss is 1.4062049388885498
loss is 1.2205890417099
loss is 1.392270803451538
loss is 1.333890438079834
loss is 1.3060111999511719
loss is 1.2654047012329102
loss is 1.2509570121765137
loss is 1.285764455795288
loss is 1.239011287689209
loss is 1.3647586107254028
loss is 1.2023398876190186
loss is 1.3049780130386353
loss is 1.2401407957077026
loss is 1.3661059141159058
loss is 1.3091213703155518
loss is 1.267840027809143
loss is 1.3704652786254883
loss is 1.3060705661773682
loss is 1.273794412612915
loss is 1.1700688600540161
loss is 1.255718469619751
loss is 1.3330066204071045
loss is 1.3283692598342896
loss is 1.3434443473815918
loss is 1.3256738185882568
loss is 1.2750446796417236
loss is 1.2585887908935547
loss is 1.2564138174057007
loss is 1.3693705797195435
loss is 1.2966408729553223
loss is 1.4150913953781128
loss is 1.3468939065933228
loss is 1.2928457260131836
loss is 1.2496317625045776
loss is 1.3236420154571533
loss is 1.2875245809555054
loss is 1.2454278469085693
loss is 1.3455026149749756
loss is 1.3051508665084839
loss is 1.3486061096191406
loss is 1.3626372814178467
loss is 1.1779059171676636
loss is 1.2644717693328857
loss is 1.348151445388794
loss is 1.3143006563186646
loss is 1.2489523887634277
loss is 1.2662807703018188
loss is 1.407322645187378
loss is 1.3237278461456299
loss is 1.2635455131530762
loss is 1.3293614387512207
loss is 1.3389215469360352
loss is 1.324263572692871
loss is 1.3340836763381958
loss is 1.26856529712677
loss is 1.450676679611206
loss is 1.3471931219100952
loss is 1.3247216939926147
loss is 1.3318374156951904
loss is 1.2872741222381592
loss is 1.3302226066589355
loss is 1.354506254196167
loss is 1.3051797151565552
loss is 1.3438719511032104
loss is 1.3307095766067505
loss is 1.2716394662857056
loss is 1.2624573707580566
loss is 1.332106351852417
loss is 1.2840379476547241
loss is 1.3715099096298218
loss is 1.2789933681488037
loss is 1.2970434427261353
loss is 1.4003735780715942
loss is 1.2549591064453125
loss is 1.3048312664031982
loss is 1.3343663215637207
loss is 1.3338937759399414
loss is 1.2817015647888184
loss is 1.208726406097412
loss is 1.3514686822891235
loss is 1.3879104852676392
loss is 1.374661922454834
loss is 1.3265646696090698
loss is 1.3976848125457764
loss is 1.3252490758895874
loss is 1.268553376197815
loss is 1.330772876739502
loss is 1.2939653396606445
loss is 1.4454387426376343
loss is 1.3405591249465942
loss is 1.4145653247833252
loss is 1.3475635051727295
loss is 1.3127691745758057
loss is 1.3673677444458008
loss is 1.2190577983856201
loss is 1.2816641330718994
loss is 1.3492997884750366
loss is 1.464613676071167
loss is 1.3359382152557373
loss is 1.4205856323242188
loss is 1.2930364608764648
loss is 1.3483102321624756
loss is 1.314229130744934
loss is 1.3412824869155884
loss is 1.4181784391403198
loss is 1.3850048780441284
loss is 1.2558108568191528
loss is 1.2616536617279053
loss is 1.2803393602371216
loss is 1.3794621229171753
loss is 1.3105671405792236
loss is 1.2815252542495728
loss is 1.3176721334457397
loss is 1.3452370166778564
loss is 1.3901913166046143
loss is 1.4175500869750977
loss is 1.3144973516464233
loss is 1.1805200576782227
loss is 1.312259554862976
loss is 1.3574879169464111
loss is 1.2587332725524902
loss is 1.2822200059890747
loss is 1.2788525819778442
loss is 1.3564746379852295
loss is 1.3188295364379883
loss is 1.3052568435668945
loss is 1.2388358116149902
loss is 1.3836348056793213
loss is 1.3214538097381592
loss is 1.3697543144226074
loss is 1.3393479585647583
loss is 1.3076726198196411
loss is 1.330372929573059
loss is 1.2330164909362793
loss is 1.3633434772491455
loss is 1.236920952796936
loss is 1.3497607707977295
loss is 1.3937044143676758
loss is 1.2567880153656006
loss is 1.2714921236038208
loss is 1.2660484313964844
loss is 1.3282307386398315
loss is 1.3238433599472046
loss is 1.271461009979248
loss is 1.2827776670455933
loss is 1.2568515539169312
loss is 1.3100371360778809
loss is 1.2686790227890015
loss is 1.326285719871521
loss is 1.3292537927627563
loss is 1.1816964149475098
loss is 1.3136367797851562
loss is 1.3543117046356201
loss is 1.3057987689971924
loss is 1.3305299282073975
loss is 1.313103437423706
loss is 1.2897284030914307
loss is 1.2361947298049927
loss is 1.326505422592163
loss is 1.269533395767212
loss is 1.3958183526992798
loss is 1.3520587682724
loss is 1.2501521110534668
loss is 1.2755157947540283
loss is 1.2375050783157349
loss is 1.2491230964660645
loss is 1.3670220375061035
loss is 1.301478385925293
loss is 1.3007917404174805
loss is 1.2842607498168945
loss is 1.3652710914611816
loss is 1.2969253063201904
loss is 1.467526912689209
loss is 1.3289433717727661
loss is 1.262630820274353
loss is 1.29923677444458
loss is 1.3923038244247437
loss is 1.2133103609085083
loss is 1.2969326972961426
loss is 1.246631145477295
loss is 1.2627819776535034
loss is 1.2590527534484863
loss is 1.2482012510299683
loss is 1.2285586595535278
loss is 1.2494759559631348
loss is 1.233953833580017
loss is 1.3523657321929932
loss is 1.4638432264328003
loss is 1.1534022092819214
loss is 1.360581874847412
loss is 1.300437331199646
loss is 1.308030128479004
loss is 1.21253502368927
loss is 1.3134355545043945
loss is 1.3547955751419067
loss is 1.4033403396606445
loss is 1.3729653358459473
loss is 1.349270224571228
loss is 1.241712212562561
loss is 1.3475372791290283
loss is 1.3087748289108276
loss is 1.4076151847839355
loss is 1.2957525253295898
loss is 1.3757799863815308
loss is 1.4474040269851685
loss is 1.4267100095748901
loss is 1.408044695854187
loss is 1.3358523845672607
loss is 1.283581256866455
loss is 1.324025273323059
loss is 1.447640061378479
loss is 1.2704684734344482
loss is 1.3411033153533936
loss is 1.2517459392547607
loss is 1.4253723621368408
loss is 1.321946144104004
loss is 1.2766445875167847
loss is 1.346159815788269
loss is 1.28908371925354
loss is 1.3634761571884155
loss is 1.3800780773162842
loss is 1.3151477575302124
loss is 1.2630846500396729
loss is 1.4257397651672363
loss is 1.3251032829284668
loss is 1.3732072114944458
loss is 1.2281298637390137
loss is 1.2634820938110352
loss is 1.3998178243637085
loss is 1.2809975147247314
loss is 1.41352117061615
loss is 1.2965055704116821
loss is 1.222631573677063
loss is 1.3598872423171997
loss is 1.4408085346221924
loss is 1.3165730237960815
loss is 1.1871627569198608
loss is 1.3039367198944092
loss is 1.2992255687713623
loss is 1.3009692430496216
loss is 1.2398886680603027
loss is 1.3305342197418213
loss is 1.269890546798706
loss is 1.334879755973816
loss is 1.4064728021621704
loss is 1.2283360958099365
loss is 1.2264710664749146
loss is 1.3112431764602661
loss is 1.196699857711792
loss is 1.2482972145080566
loss is 1.216564655303955
loss is 1.3081295490264893
loss is 1.3528372049331665
loss is 1.3404110670089722
loss is 1.3256690502166748
loss is 1.3462718725204468
loss is 1.3043659925460815
loss is 1.389615774154663
loss is 1.2846020460128784
loss is 1.4409153461456299
loss is 1.3959776163101196
loss is 1.2877082824707031
loss is 1.3333908319473267
loss is 1.2820770740509033
loss is 1.1551415920257568
loss is 1.2226316928863525
loss is 1.3061163425445557
loss is 1.2568186521530151
loss is 1.3086529970169067
loss is 1.2862498760223389
loss is 1.2558236122131348
loss is 1.2880669832229614
loss is 1.1679866313934326
loss is 1.2328376770019531
loss is 1.3653522729873657
loss is 1.3473734855651855
loss is 1.3869366645812988
loss is 1.3439891338348389
loss is 1.3667107820510864
loss is 1.3154116868972778
loss is 1.3194862604141235
loss is 1.2707830667495728
loss is 1.3400386571884155
loss is 1.4044592380523682
loss is 1.251124382019043
loss is 1.2106022834777832
loss is 1.4176173210144043
loss is 1.2968413829803467
loss is 1.2199609279632568
loss is 1.3411204814910889
loss is 1.3000298738479614
loss is 1.4324591159820557
loss is 1.3299641609191895
loss is 1.3248416185379028
loss is 1.3189802169799805
epoch 11: train_loss = 1.315
11: {'Accuracy': 0.5283, 'Precision': 0.5373, 'Recall': 0.5242, 'F1-score': 0.5212}
epoch: 12
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:23,  1.45it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:33, 11.36it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:17, 20.69it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:12, 29.09it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 36.19it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 41.67it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 45.90it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 49.09it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 51.57it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.38it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 54.68it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 55.62it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.28it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.79it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.15it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.34it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 57.46it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.59it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.73it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.73it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.75it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 57.76it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 57.86it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 57.85it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 57.84it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 57.88it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 57.87it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 57.93it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 57.87it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 57.93it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 57.90it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 57.92it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 57.91it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 57.92it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 57.87it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 57.86it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 57.85it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 57.85it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 57.89it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 57.85it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 57.88it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 57.92it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 57.97it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 57.92it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 57.86it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 57.82it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 57.78it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 57.83it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 57.85it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 57.78it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 57.78it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 57.83it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 57.81it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 57.79it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 57.82it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 57.80it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 57.77it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 57.75it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 57.78it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 57.80it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 57.76it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 57.36it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 57.61it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 57.75it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.23it/s]
loss is 1.1429872512817383
loss is 1.1839988231658936
loss is 1.1573150157928467
loss is 1.3039076328277588
loss is 1.1387678384780884
loss is 1.2996091842651367
loss is 1.2389330863952637
loss is 1.1831426620483398
loss is 1.3185467720031738
loss is 1.3331305980682373
loss is 1.2544498443603516
loss is 1.271562933921814
loss is 1.130445122718811
loss is 1.1424866914749146
loss is 1.2309337854385376
loss is 1.2672358751296997
loss is 1.3170599937438965
loss is 1.2676892280578613
loss is 1.2438251972198486
loss is 1.3449842929840088
loss is 1.3193414211273193
loss is 1.2563437223434448
loss is 1.2655065059661865
loss is 1.3184205293655396
loss is 1.298814058303833
loss is 1.2466349601745605
loss is 1.2536541223526
loss is 1.3378947973251343
loss is 1.280808687210083
loss is 1.1750625371932983
loss is 1.2431275844573975
loss is 1.363986849784851
loss is 1.2930325269699097
loss is 1.2759851217269897
loss is 1.261135220527649
loss is 1.276984453201294
loss is 1.1756304502487183
loss is 1.2423620223999023
loss is 1.279844880104065
loss is 1.2454099655151367
loss is 1.233839511871338
loss is 1.208979845046997
loss is 1.2566889524459839
loss is 1.310404658317566
loss is 1.1692430973052979
loss is 1.2517576217651367
loss is 1.2524679899215698
loss is 1.3578778505325317
loss is 1.3210660219192505
loss is 1.2491179704666138
loss is 1.327008843421936
loss is 1.2541030645370483
loss is 1.2038874626159668
loss is 1.2813467979431152
loss is 1.3021039962768555
loss is 1.3023875951766968
loss is 1.235837459564209
loss is 1.1856107711791992
loss is 1.178114652633667
loss is 1.2647464275360107
loss is 1.20393967628479
loss is 1.1690428256988525
loss is 1.22989821434021
loss is 1.3854480981826782
loss is 1.2786931991577148
loss is 1.265297532081604
loss is 1.3554524183273315
loss is 1.288908839225769
loss is 1.2401509284973145
loss is 1.1469305753707886
loss is 1.2520734071731567
loss is 1.2533437013626099
loss is 1.2078722715377808
loss is 1.234694242477417
loss is 1.3127036094665527
loss is 1.342868685722351
loss is 1.2606747150421143
loss is 1.1948189735412598
loss is 1.346492886543274
loss is 1.2383583784103394
loss is 1.257003664970398
loss is 1.2863340377807617
loss is 1.2541426420211792
loss is 1.1701620817184448
loss is 1.2826852798461914
loss is 1.3177595138549805
loss is 1.168527603149414
loss is 1.2309819459915161
loss is 1.3449509143829346
loss is 1.3459793329238892
loss is 1.2848331928253174
loss is 1.3291493654251099
loss is 1.2925459146499634
loss is 1.233964204788208
loss is 1.3123756647109985
loss is 1.3035261631011963
loss is 1.2142313718795776
loss is 1.2387031316757202
loss is 1.2296245098114014
loss is 1.1461106538772583
loss is 1.2606853246688843
loss is 1.2758132219314575
loss is 1.2360703945159912
loss is 1.1817963123321533
loss is 1.286185622215271
loss is 1.1772152185440063
loss is 1.2947723865509033
loss is 1.2489033937454224
loss is 1.2711251974105835
loss is 1.2187005281448364
loss is 1.306183934211731
loss is 1.349333643913269
loss is 1.4022777080535889
loss is 1.1910772323608398
loss is 1.2399382591247559
loss is 1.270704984664917
loss is 1.312740445137024
loss is 1.2472541332244873
loss is 1.3117778301239014
loss is 1.251410961151123
loss is 1.1183290481567383
loss is 1.1608383655548096
loss is 1.2606831789016724
loss is 1.2962528467178345
loss is 1.219942331314087
loss is 1.1889060735702515
loss is 1.2549660205841064
loss is 1.2969547510147095
loss is 1.2932041883468628
loss is 1.273935079574585
loss is 1.2442708015441895
loss is 1.3348872661590576
loss is 1.2378721237182617
loss is 1.3779335021972656
loss is 1.1937636137008667
loss is 1.3225843906402588
loss is 1.3008856773376465
loss is 1.2663562297821045
loss is 1.2422149181365967
loss is 1.314475417137146
loss is 1.2802913188934326
loss is 1.2828140258789062
loss is 1.1778244972229004
loss is 1.3088220357894897
loss is 1.302880883216858
loss is 1.3104244470596313
loss is 1.2795571088790894
loss is 1.408906102180481
loss is 1.3221235275268555
loss is 1.2556380033493042
loss is 1.3001065254211426
loss is 1.3641705513000488
loss is 1.3061742782592773
loss is 1.3463345766067505
loss is 1.2810558080673218
loss is 1.338085651397705
loss is 1.222167730331421
loss is 1.201279878616333
loss is 1.4120920896530151
loss is 1.344700574874878
loss is 1.3262414932250977
loss is 1.3213123083114624
loss is 1.3082748651504517
loss is 1.3138878345489502
loss is 1.333611011505127
loss is 1.291082501411438
loss is 1.2465614080429077
loss is 1.2672324180603027
loss is 1.2489209175109863
loss is 1.2504127025604248
loss is 1.3278142213821411
loss is 1.2795466184616089
loss is 1.2530460357666016
loss is 1.2393602132797241
loss is 1.198416829109192
loss is 1.293940544128418
loss is 1.2156274318695068
loss is 1.2205829620361328
loss is 1.3988771438598633
loss is 1.3673791885375977
loss is 1.243570327758789
loss is 1.2622637748718262
loss is 1.1982768774032593
loss is 1.1942895650863647
loss is 1.285127878189087
loss is 1.2709736824035645
loss is 1.3603438138961792
loss is 1.2414315938949585
loss is 1.3188518285751343
loss is 1.2694281339645386
loss is 1.2022273540496826
loss is 1.2985832691192627
loss is 1.2828917503356934
loss is 1.2720787525177002
loss is 1.2866095304489136
loss is 1.2989780902862549
loss is 1.36692214012146
loss is 1.245719313621521
loss is 1.324149489402771
loss is 1.2939372062683105
loss is 1.3368721008300781
loss is 1.2469511032104492
loss is 1.3045246601104736
loss is 1.2100739479064941
loss is 1.248369574546814
loss is 1.3443591594696045
loss is 1.2601563930511475
loss is 1.296351671218872
loss is 1.3415788412094116
loss is 1.3523060083389282
loss is 1.2249201536178589
loss is 1.3467518091201782
loss is 1.2782597541809082
loss is 1.3060312271118164
loss is 1.3023273944854736
loss is 1.3359777927398682
loss is 1.3636606931686401
loss is 1.4043306112289429
loss is 1.170966386795044
loss is 1.2598413228988647
loss is 1.3740099668502808
loss is 1.2516945600509644
loss is 1.3138961791992188
loss is 1.3775449991226196
loss is 1.2418662309646606
loss is 1.2185965776443481
loss is 1.249830961227417
loss is 1.308878779411316
loss is 1.1579177379608154
loss is 1.2019597291946411
loss is 1.2742080688476562
loss is 1.1788221597671509
loss is 1.255948543548584
loss is 1.3157103061676025
loss is 1.306368350982666
loss is 1.2346805334091187
loss is 1.3323962688446045
loss is 1.248857021331787
loss is 1.3320302963256836
loss is 1.364967942237854
loss is 1.40285325050354
loss is 1.2964744567871094
loss is 1.2739288806915283
loss is 1.287583589553833
loss is 1.3402245044708252
loss is 1.2922924757003784
loss is 1.2231590747833252
loss is 1.2274280786514282
loss is 1.2926026582717896
loss is 1.1889872550964355
loss is 1.2218555212020874
loss is 1.3315562009811401
loss is 1.328797459602356
loss is 1.241719126701355
loss is 1.2791458368301392
loss is 1.2924503087997437
loss is 1.3185374736785889
loss is 1.1606147289276123
loss is 1.2125937938690186
loss is 1.3091448545455933
loss is 1.2492390871047974
loss is 1.3117763996124268
loss is 1.2484643459320068
loss is 1.1780856847763062
loss is 1.2744096517562866
loss is 1.2333894968032837
loss is 1.2235180139541626
loss is 1.2191212177276611
loss is 1.1977109909057617
loss is 1.3732072114944458
loss is 1.1900594234466553
loss is 1.2205744981765747
loss is 1.2270350456237793
loss is 1.3537074327468872
loss is 1.2704616785049438
loss is 1.3429138660430908
loss is 1.3145403861999512
loss is 1.2785325050354004
loss is 1.2481549978256226
loss is 1.2605048418045044
loss is 1.2359763383865356
loss is 1.3343207836151123
loss is 1.2140204906463623
loss is 1.332812786102295
loss is 1.2317200899124146
loss is 1.2906064987182617
loss is 1.3541241884231567
loss is 1.156784176826477
loss is 1.2153486013412476
loss is 1.2402633428573608
loss is 1.227608323097229
loss is 1.3218774795532227
loss is 1.2988237142562866
loss is 1.2735809087753296
loss is 1.1960349082946777
loss is 1.2865073680877686
loss is 1.259095311164856
loss is 1.2688366174697876
loss is 1.2831978797912598
loss is 1.3314275741577148
loss is 1.2247555255889893
loss is 1.231865406036377
loss is 1.3053772449493408
loss is 1.3102895021438599
loss is 1.2497426271438599
loss is 1.2935124635696411
loss is 1.2952901124954224
loss is 1.214015245437622
loss is 1.2781388759613037
loss is 1.2061744928359985
loss is 1.2574764490127563
loss is 1.3296040296554565
loss is 1.2450034618377686
loss is 1.2670693397521973
loss is 1.270094394683838
loss is 1.2677823305130005
loss is 1.2850080728530884
loss is 1.2715303897857666
loss is 1.2302879095077515
loss is 1.2211748361587524
loss is 1.3046202659606934
loss is 1.2571059465408325
loss is 1.294691801071167
loss is 1.2251254320144653
loss is 1.2228970527648926
loss is 1.317418098449707
loss is 1.2739224433898926
loss is 1.2053035497665405
loss is 1.170019268989563
loss is 1.2517346143722534
loss is 1.2600055932998657
loss is 1.2037183046340942
loss is 1.3026281595230103
loss is 1.3580143451690674
loss is 1.3539866209030151
loss is 1.3987780809402466
loss is 1.2929983139038086
loss is 1.2750645875930786
loss is 1.2795372009277344
loss is 1.2536060810089111
loss is 1.269547939300537
loss is 1.200805902481079
loss is 1.2242263555526733
loss is 1.307153344154358
loss is 1.3018379211425781
loss is 1.2664088010787964
loss is 1.3123369216918945
loss is 1.244091510772705
loss is 1.2645868062973022
loss is 1.211822271347046
loss is 1.1257668733596802
loss is 1.2454746961593628
loss is 1.2834261655807495
loss is 1.2212321758270264
loss is 1.2971551418304443
loss is 1.2203471660614014
loss is 1.2936527729034424
loss is 1.2185992002487183
loss is 1.2557810544967651
loss is 1.3441836833953857
loss is 1.1399232149124146
loss is 1.223089337348938
loss is 1.2894203662872314
loss is 1.2684115171432495
loss is 1.271377682685852
loss is 1.3659389019012451
loss is 1.2780970335006714
loss is 1.0834120512008667
loss is 1.3833949565887451
loss is 1.3189884424209595
loss is 1.3844305276870728
loss is 1.3322229385375977
loss is 1.2586071491241455
loss is 1.3303974866867065
loss is 1.2091772556304932
loss is 1.1999870538711548
loss is 1.2212955951690674
loss is 1.1750810146331787
loss is 1.2850983142852783
loss is 1.1376216411590576
loss is 1.2792134284973145
loss is 1.3715519905090332
loss is 1.258875846862793
loss is 1.211990475654602
epoch 12: train_loss = 1.269
12: {'Accuracy': 0.5367, 'Precision': 0.547, 'Recall': 0.5363, 'F1-score': 0.5318}
epoch: 13
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:38,  1.38it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:34, 10.87it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 20.07it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:12, 28.39it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:10, 35.58it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 40.64it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 45.32it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 48.87it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 51.60it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.59it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 55.00it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 56.01it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.86it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 57.39it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.79it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 58.06it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 58.22it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 58.30it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 58.28it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 58.44it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 58.40it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.41it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.48it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 58.51it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.52it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:03, 58.53it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.52it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.51it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.49it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.57it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.60it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.59it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 58.66it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.71it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.67it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.68it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.63it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.67it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.62it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.62it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.59it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.66it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 58.68it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.67it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.66it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.72it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.65it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.65it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.66it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.67it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.01it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 58.22it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.33it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.44it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 58.42it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.54it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.56it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 58.57it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.58it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.55it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.64it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:06<00:00, 58.21it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 58.45it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.60it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.51it/s]
loss is 1.2262043952941895
loss is 1.2318646907806396
loss is 1.2852956056594849
loss is 1.1923667192459106
loss is 1.2192462682724
loss is 1.221566081047058
loss is 1.1817405223846436
loss is 1.261673927307129
loss is 1.2197092771530151
loss is 1.2611172199249268
loss is 1.2146812677383423
loss is 1.1516904830932617
loss is 1.252395749092102
loss is 1.2889899015426636
loss is 1.2239413261413574
loss is 1.2292871475219727
loss is 1.2516756057739258
loss is 1.340856909751892
loss is 1.3163201808929443
loss is 1.2717530727386475
loss is 1.3629215955734253
loss is 1.307727336883545
loss is 1.2309558391571045
loss is 1.281394362449646
loss is 1.3431041240692139
loss is 1.204418659210205
loss is 1.3716378211975098
loss is 1.245366096496582
loss is 1.233879566192627
loss is 1.1727148294448853
loss is 1.1817125082015991
loss is 1.2568790912628174
loss is 1.2753621339797974
loss is 1.2528871297836304
loss is 1.2067172527313232
loss is 1.2593413591384888
loss is 1.2148841619491577
loss is 1.348883032798767
loss is 1.2663496732711792
loss is 1.2660940885543823
loss is 1.2030659914016724
loss is 1.3002979755401611
loss is 1.2876944541931152
loss is 1.2600864171981812
loss is 1.1701159477233887
loss is 1.1155225038528442
loss is 1.2942677736282349
loss is 1.0874136686325073
loss is 1.1847662925720215
loss is 1.281168818473816
loss is 1.2106521129608154
loss is 1.1680747270584106
loss is 1.1512386798858643
loss is 1.3152589797973633
loss is 1.1423755884170532
loss is 1.3514536619186401
loss is 1.2263892889022827
loss is 1.205525279045105
loss is 1.2230160236358643
loss is 1.2686039209365845
loss is 1.1216069459915161
loss is 1.1741690635681152
loss is 1.326177716255188
loss is 1.2134462594985962
loss is 1.228588581085205
loss is 1.2141166925430298
loss is 1.2303650379180908
loss is 1.1836893558502197
loss is 1.2772315740585327
loss is 1.3062939643859863
loss is 1.1249945163726807
loss is 1.255866289138794
loss is 1.230130910873413
loss is 1.20537531375885
loss is 1.1557490825653076
loss is 1.2957829236984253
loss is 1.202856183052063
loss is 1.2453619241714478
loss is 1.1913974285125732
loss is 1.3396470546722412
loss is 1.2635107040405273
loss is 1.2701771259307861
loss is 1.281522274017334
loss is 1.1566474437713623
loss is 1.2948596477508545
loss is 1.243360996246338
loss is 1.2249648571014404
loss is 1.1736712455749512
loss is 1.2400089502334595
loss is 1.112644910812378
loss is 1.4196168184280396
loss is 1.1088547706604004
loss is 1.235546588897705
loss is 1.072277307510376
loss is 1.2273800373077393
loss is 1.283585548400879
loss is 1.2258647680282593
loss is 1.1906479597091675
loss is 1.3376966714859009
loss is 1.291566014289856
loss is 1.304641842842102
loss is 1.273193597793579
loss is 1.213030219078064
loss is 1.2493726015090942
loss is 1.1460952758789062
loss is 1.2778903245925903
loss is 1.2813897132873535
loss is 1.1719393730163574
loss is 1.3535031080245972
loss is 1.2033276557922363
loss is 1.3329483270645142
loss is 1.1855852603912354
loss is 1.2941503524780273
loss is 1.1424505710601807
loss is 1.1806999444961548
loss is 1.222778081893921
loss is 1.2938963174819946
loss is 1.1871451139450073
loss is 1.1953153610229492
loss is 1.1944892406463623
loss is 1.2127692699432373
loss is 1.2048258781433105
loss is 1.1655182838439941
loss is 1.1754931211471558
loss is 1.151047945022583
loss is 1.1991955041885376
loss is 1.2185146808624268
loss is 1.241163969039917
loss is 1.2522224187850952
loss is 1.2726157903671265
loss is 1.131257176399231
loss is 1.3126623630523682
loss is 1.1803089380264282
loss is 1.3540555238723755
loss is 1.1539828777313232
loss is 1.1095757484436035
loss is 1.1611436605453491
loss is 1.26558256149292
loss is 1.1896538734436035
loss is 1.1724833250045776
loss is 1.1077301502227783
loss is 1.2990530729293823
loss is 1.121742606163025
loss is 1.3119958639144897
loss is 1.2858918905258179
loss is 1.258514642715454
loss is 1.3898953199386597
loss is 1.3454970121383667
loss is 1.2452995777130127
loss is 1.2072925567626953
loss is 1.288417100906372
loss is 1.2193503379821777
loss is 1.1919032335281372
loss is 1.246222734451294
loss is 1.238897442817688
loss is 1.2902756929397583
loss is 1.2319567203521729
loss is 1.2913012504577637
loss is 1.2102543115615845
loss is 1.173234462738037
loss is 1.186036229133606
loss is 1.2785569429397583
loss is 1.2016139030456543
loss is 1.2072654962539673
loss is 1.265081763267517
loss is 1.2912356853485107
loss is 1.3032584190368652
loss is 1.3074655532836914
loss is 1.394230842590332
loss is 1.2883599996566772
loss is 1.3410223722457886
loss is 1.337622880935669
loss is 1.1428548097610474
loss is 1.2590982913970947
loss is 1.1938589811325073
loss is 1.1792023181915283
loss is 1.266413688659668
loss is 1.2810381650924683
loss is 1.2281289100646973
loss is 1.2145880460739136
loss is 1.1293814182281494
loss is 1.3410110473632812
loss is 1.1487516164779663
loss is 1.1427099704742432
loss is 1.2541875839233398
loss is 1.1821925640106201
loss is 1.295883297920227
loss is 1.1301954984664917
loss is 1.1614025831222534
loss is 1.3066335916519165
loss is 1.2405787706375122
loss is 1.361588716506958
loss is 1.186341643333435
loss is 1.2265384197235107
loss is 1.3303173780441284
loss is 1.339137315750122
loss is 1.2279460430145264
loss is 1.2120760679244995
loss is 1.3595876693725586
loss is 1.3021483421325684
loss is 1.1428743600845337
loss is 1.3132535219192505
loss is 1.2018777132034302
loss is 1.1936118602752686
loss is 1.2568904161453247
loss is 1.2693605422973633
loss is 1.2479660511016846
loss is 1.2564533948898315
loss is 1.1500171422958374
loss is 1.327162742614746
loss is 1.2431331872940063
loss is 1.255720853805542
loss is 1.2330836057662964
loss is 1.2579532861709595
loss is 1.2195754051208496
loss is 1.3140678405761719
loss is 1.1471984386444092
loss is 1.317262887954712
loss is 1.2644133567810059
loss is 1.259170651435852
loss is 1.2393132448196411
loss is 1.3506938219070435
loss is 1.1220183372497559
loss is 1.3055088520050049
loss is 1.2682147026062012
loss is 1.2567057609558105
loss is 1.1775257587432861
loss is 1.3344180583953857
loss is 1.2456282377243042
loss is 1.2611223459243774
loss is 1.1714568138122559
loss is 1.2447410821914673
loss is 1.2187870740890503
loss is 1.1990429162979126
loss is 1.2846930027008057
loss is 1.1430118083953857
loss is 1.1277945041656494
loss is 1.1791034936904907
loss is 1.2237398624420166
loss is 1.279237985610962
loss is 1.1689974069595337
loss is 1.241438388824463
loss is 1.318486213684082
loss is 1.2494393587112427
loss is 1.2802667617797852
loss is 1.1650521755218506
loss is 1.2206138372421265
loss is 1.1474034786224365
loss is 1.2298012971878052
loss is 1.2786051034927368
loss is 1.2230215072631836
loss is 1.178806185722351
loss is 1.2851611375808716
loss is 1.3183352947235107
loss is 1.134385585784912
loss is 1.2894593477249146
loss is 1.1861706972122192
loss is 1.3100159168243408
loss is 1.2882325649261475
loss is 1.2049481868743896
loss is 1.2308918237686157
loss is 1.3338661193847656
loss is 1.202654242515564
loss is 1.3044859170913696
loss is 1.233991265296936
loss is 1.2077364921569824
loss is 1.1902525424957275
loss is 1.172666072845459
loss is 1.183308482170105
loss is 1.1765812635421753
loss is 1.254684329032898
loss is 1.3044925928115845
loss is 1.2294784784317017
loss is 1.177253246307373
loss is 1.2953174114227295
loss is 1.3107287883758545
loss is 1.275221586227417
loss is 1.2806074619293213
loss is 1.1977572441101074
loss is 1.149308681488037
loss is 1.271883249282837
loss is 1.2023147344589233
loss is 1.2820695638656616
loss is 1.1786725521087646
loss is 1.2630283832550049
loss is 1.3216030597686768
loss is 1.0321223735809326
loss is 1.177490234375
loss is 1.1941683292388916
loss is 1.398083209991455
loss is 1.2056269645690918
loss is 1.2333577871322632
loss is 1.1621803045272827
loss is 1.1295111179351807
loss is 1.2140425443649292
loss is 1.229190707206726
loss is 1.2117806673049927
loss is 1.3098140954971313
loss is 1.2848187685012817
loss is 1.301023006439209
loss is 1.1748631000518799
loss is 1.1980814933776855
loss is 1.3032668828964233
loss is 1.292771339416504
loss is 1.2121831178665161
loss is 1.1951115131378174
loss is 1.2874236106872559
loss is 1.2178242206573486
loss is 1.3988699913024902
loss is 1.1958869695663452
loss is 1.298500657081604
loss is 1.2813668251037598
loss is 1.2800403833389282
loss is 1.2170681953430176
loss is 1.2246485948562622
loss is 1.3513972759246826
loss is 1.30299973487854
loss is 1.2189382314682007
loss is 1.2270516157150269
loss is 1.2815861701965332
loss is 1.2610949277877808
loss is 1.3309108018875122
loss is 1.1934399604797363
loss is 1.3144347667694092
loss is 1.320287823677063
loss is 1.122646689414978
loss is 1.287636160850525
loss is 1.3009353876113892
loss is 1.2522265911102295
loss is 1.2950798273086548
loss is 1.278058648109436
loss is 1.3193597793579102
loss is 1.1608738899230957
loss is 1.2699085474014282
loss is 1.2246387004852295
loss is 1.305550217628479
loss is 1.367013692855835
loss is 1.2941601276397705
loss is 1.2939072847366333
loss is 1.3366150856018066
loss is 1.3093287944793701
loss is 1.1903480291366577
loss is 1.2508631944656372
loss is 1.3167505264282227
loss is 1.2282135486602783
loss is 1.2586420774459839
loss is 1.1852878332138062
loss is 1.283524513244629
loss is 1.2791426181793213
loss is 1.3173537254333496
loss is 1.2384496927261353
loss is 1.1819604635238647
loss is 1.199855923652649
loss is 1.2417654991149902
loss is 1.1839574575424194
loss is 1.2502048015594482
loss is 1.2982083559036255
loss is 1.2629115581512451
loss is 1.184158444404602
loss is 1.1607078313827515
loss is 1.2152152061462402
loss is 1.2386213541030884
loss is 1.2899519205093384
loss is 1.2077305316925049
loss is 1.214035987854004
loss is 1.2352601289749146
loss is 1.1776787042617798
loss is 1.258994221687317
loss is 1.1827601194381714
loss is 1.2993180751800537
loss is 1.200136661529541
loss is 1.2266806364059448
loss is 1.1595871448516846
loss is 1.278640866279602
loss is 1.272691249847412
loss is 1.2877436876296997
loss is 1.2578761577606201
loss is 1.2491557598114014
loss is 1.0901685953140259
loss is 1.3385024070739746
loss is 1.3335157632827759
loss is 1.2265801429748535
loss is 1.1309014558792114
loss is 1.3536376953125
epoch 13: train_loss = 1.24
13: {'Accuracy': 0.5425, 'Precision': 0.5609, 'Recall': 0.5356, 'F1-score': 0.5369}
epoch: 14
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:36,  1.39it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:34, 10.98it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 20.08it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:12, 28.52it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:10, 35.68it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 41.20it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 45.64it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 49.11it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 51.71it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.65it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 55.01it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 56.07it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.79it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 57.39it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.87it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 58.10it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 58.29it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 58.44it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 58.55it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 58.66it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 58.67it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.70it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.69it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 58.72it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.66it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:03, 58.74it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.74it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.73it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.62it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.68it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.73it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.69it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 58.74it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.75it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.73it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.69it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.65it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.68it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.71it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.67it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.66it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.65it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 58.68it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.72it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.70it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.73it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.65it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.66it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.70it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.64it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.66it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 58.71it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.73it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.71it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 58.71it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.78it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.70it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 58.59it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.62it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.61it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.64it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:06<00:00, 58.26it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 58.54it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.69it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.69it/s]
loss is 1.2135733366012573
loss is 1.2167329788208008
loss is 1.2067179679870605
loss is 1.1688742637634277
loss is 1.1765000820159912
loss is 1.1322869062423706
loss is 1.153650164604187
loss is 1.2467072010040283
loss is 1.2728958129882812
loss is 1.2363910675048828
loss is 1.3120671510696411
loss is 1.0580925941467285
loss is 1.2023773193359375
loss is 1.2445790767669678
loss is 1.1842710971832275
loss is 1.286550760269165
loss is 1.074545979499817
loss is 1.1653568744659424
loss is 1.2449792623519897
loss is 1.2455936670303345
loss is 1.2073079347610474
loss is 1.229816198348999
loss is 1.1635066270828247
loss is 1.269059658050537
loss is 1.2488062381744385
loss is 1.1943309307098389
loss is 1.2264899015426636
loss is 1.2563343048095703
loss is 1.1713272333145142
loss is 1.194960117340088
loss is 1.2771809101104736
loss is 1.1714510917663574
loss is 1.1649727821350098
loss is 1.2161316871643066
loss is 1.302428126335144
loss is 1.2675158977508545
loss is 1.1945403814315796
loss is 1.1531602144241333
loss is 1.2040140628814697
loss is 1.2251602411270142
loss is 1.2415003776550293
loss is 1.1649281978607178
loss is 1.0949006080627441
loss is 1.1987398862838745
loss is 1.2508621215820312
loss is 1.232745885848999
loss is 1.2193855047225952
loss is 1.2817922830581665
loss is 1.2816358804702759
loss is 1.2075960636138916
loss is 1.0534495115280151
loss is 1.1352041959762573
loss is 1.2220345735549927
loss is 1.2230677604675293
loss is 1.1596685647964478
loss is 1.1674612760543823
loss is 1.2169758081436157
loss is 1.2925007343292236
loss is 1.1445215940475464
loss is 1.1754478216171265
loss is 1.2175205945968628
loss is 1.1462987661361694
loss is 1.145704746246338
loss is 1.1146080493927002
loss is 1.1699702739715576
loss is 1.223687767982483
loss is 1.18717622756958
loss is 1.180202603340149
loss is 1.2333109378814697
loss is 1.194962739944458
loss is 1.1627427339553833
loss is 1.135921597480774
loss is 1.2019717693328857
loss is 1.1913247108459473
loss is 1.2861863374710083
loss is 1.100867509841919
loss is 1.269250750541687
loss is 1.1977524757385254
loss is 1.2402876615524292
loss is 1.2263988256454468
loss is 1.2399964332580566
loss is 1.1647460460662842
loss is 1.2506612539291382
loss is 1.1164376735687256
loss is 1.1383519172668457
loss is 1.2178542613983154
loss is 1.2908092737197876
loss is 1.1844301223754883
loss is 1.0910872220993042
loss is 1.1932780742645264
loss is 1.2786086797714233
loss is 1.1669856309890747
loss is 1.2968770265579224
loss is 1.2000917196273804
loss is 1.1710569858551025
loss is 1.1471011638641357
loss is 1.2239364385604858
loss is 1.2610218524932861
loss is 1.2380623817443848
loss is 1.2162208557128906
loss is 1.2653826475143433
loss is 1.2065234184265137
loss is 1.2421478033065796
loss is 1.1775697469711304
loss is 1.1787068843841553
loss is 1.230271816253662
loss is 1.2342790365219116
loss is 1.263630747795105
loss is 1.1714482307434082
loss is 1.1238833665847778
loss is 1.1237094402313232
loss is 1.0864297151565552
loss is 1.209662675857544
loss is 1.1932837963104248
loss is 1.1545801162719727
loss is 1.136783242225647
loss is 1.2541863918304443
loss is 1.1721279621124268
loss is 1.1600537300109863
loss is 1.1774957180023193
loss is 1.2171924114227295
loss is 1.1962398290634155
loss is 1.206387996673584
loss is 1.2042078971862793
loss is 1.128709316253662
loss is 1.3164135217666626
loss is 1.24176824092865
loss is 1.2088708877563477
loss is 1.194097638130188
loss is 1.2590782642364502
loss is 1.251603364944458
loss is 1.3315359354019165
loss is 1.0667884349822998
loss is 1.2063905000686646
loss is 1.2213079929351807
loss is 1.2773537635803223
loss is 1.2140713930130005
loss is 1.1489962339401245
loss is 1.2123280763626099
loss is 1.1905217170715332
loss is 1.1934845447540283
loss is 1.2608431577682495
loss is 1.2295068502426147
loss is 1.166574239730835
loss is 1.239581823348999
loss is 1.216316819190979
loss is 1.1722025871276855
loss is 1.2200145721435547
loss is 1.2140257358551025
loss is 1.204049825668335
loss is 1.1761423349380493
loss is 1.1557426452636719
loss is 1.2035576105117798
loss is 1.212846279144287
loss is 1.2555785179138184
loss is 1.1550986766815186
loss is 1.209323525428772
loss is 1.1389235258102417
loss is 1.0946996212005615
loss is 1.241490125656128
loss is 1.1429190635681152
loss is 1.2153913974761963
loss is 1.0977708101272583
loss is 1.265896201133728
loss is 1.184590220451355
loss is 1.2880473136901855
loss is 1.2525756359100342
loss is 1.2369377613067627
loss is 1.2429149150848389
loss is 1.2156586647033691
loss is 1.2207971811294556
loss is 1.1641113758087158
loss is 1.1680757999420166
loss is 1.2138923406600952
loss is 1.2179707288742065
loss is 1.326960563659668
loss is 1.205612063407898
loss is 1.2901517152786255
loss is 1.3032958507537842
loss is 1.1933033466339111
loss is 1.2876461744308472
loss is 1.1398444175720215
loss is 1.1959748268127441
loss is 1.1859192848205566
loss is 1.1388306617736816
loss is 1.2528055906295776
loss is 1.1202547550201416
loss is 1.1590690612792969
loss is 1.2888225317001343
loss is 1.1979434490203857
loss is 1.3375238180160522
loss is 1.1593154668807983
loss is 1.290767788887024
loss is 1.248407006263733
loss is 1.3270115852355957
loss is 1.227374792098999
loss is 1.3279696702957153
loss is 1.1579898595809937
loss is 1.2020692825317383
loss is 1.2343311309814453
loss is 1.1087257862091064
loss is 1.2358440160751343
loss is 1.3306872844696045
loss is 1.2942900657653809
loss is 1.166985034942627
loss is 1.16725492477417
loss is 1.1236505508422852
loss is 1.1741058826446533
loss is 1.2337770462036133
loss is 1.2338963747024536
loss is 1.2390947341918945
loss is 1.173318862915039
loss is 1.0747196674346924
loss is 1.2751768827438354
loss is 1.0961076021194458
loss is 1.2454161643981934
loss is 1.3121565580368042
loss is 1.2482199668884277
loss is 1.165909767150879
loss is 1.0691403150558472
loss is 1.2542941570281982
loss is 1.213218092918396
loss is 1.1934871673583984
loss is 1.1609466075897217
loss is 1.3107000589370728
loss is 1.2168365716934204
loss is 1.2264381647109985
loss is 1.2187621593475342
loss is 1.1203207969665527
loss is 1.246362566947937
loss is 1.280595302581787
loss is 1.1630403995513916
loss is 1.2545795440673828
loss is 1.1769208908081055
loss is 1.1641037464141846
loss is 1.1349278688430786
loss is 1.1370965242385864
loss is 1.1571288108825684
loss is 1.2303715944290161
loss is 1.298140048980713
loss is 1.2044742107391357
loss is 1.2168447971343994
loss is 1.1669467687606812
loss is 1.1902021169662476
loss is 1.3478729724884033
loss is 1.2653982639312744
loss is 1.1805459260940552
loss is 1.1841816902160645
loss is 1.3254908323287964
loss is 1.3178178071975708
loss is 1.2292513847351074
loss is 1.2964563369750977
loss is 1.163361668586731
loss is 1.2029845714569092
loss is 1.199245810508728
loss is 1.141707181930542
loss is 1.1627014875411987
loss is 1.1175047159194946
loss is 1.1597590446472168
loss is 1.191754937171936
loss is 1.0748934745788574
loss is 1.1519197225570679
loss is 1.1746337413787842
loss is 1.1512373685836792
loss is 1.233694076538086
loss is 1.3333864212036133
loss is 1.1672148704528809
loss is 1.1672883033752441
loss is 1.304376482963562
loss is 1.2151983976364136
loss is 1.2203656435012817
loss is 1.1873035430908203
loss is 1.1981676816940308
loss is 1.2814745903015137
loss is 1.1104626655578613
loss is 1.240422010421753
loss is 1.2646522521972656
loss is 1.3087973594665527
loss is 1.132165551185608
loss is 1.169094443321228
loss is 1.192620038986206
loss is 1.2055003643035889
loss is 1.2225477695465088
loss is 1.1711554527282715
loss is 1.2507928609848022
loss is 1.114067792892456
loss is 1.2811532020568848
loss is 1.118444800376892
loss is 1.1692874431610107
loss is 1.1136211156845093
loss is 1.1577842235565186
loss is 1.227320909500122
loss is 1.177799940109253
loss is 1.0882842540740967
loss is 1.1434423923492432
loss is 1.2868564128875732
loss is 1.205329418182373
loss is 1.1514300107955933
loss is 1.135439395904541
loss is 1.1894097328186035
loss is 1.0932010412216187
loss is 1.2585488557815552
loss is 1.231065273284912
loss is 1.14131498336792
loss is 1.1645985841751099
loss is 1.2609390020370483
loss is 1.21591055393219
loss is 1.1722294092178345
loss is 1.1098214387893677
loss is 1.1742298603057861
loss is 1.2149784564971924
loss is 1.206061601638794
loss is 1.2288132905960083
loss is 1.2267777919769287
loss is 1.2896572351455688
loss is 1.1591490507125854
loss is 1.2901372909545898
loss is 1.2184953689575195
loss is 1.1822361946105957
loss is 1.199188470840454
loss is 1.29581880569458
loss is 1.2407301664352417
loss is 1.1463567018508911
loss is 1.3026198148727417
loss is 1.1502858400344849
loss is 1.0680328607559204
loss is 1.1552958488464355
loss is 1.1825323104858398
loss is 1.2643160820007324
loss is 1.1279594898223877
loss is 1.0832375288009644
loss is 1.3067981004714966
loss is 1.1841343641281128
loss is 1.1385242938995361
loss is 1.236452579498291
loss is 1.2261722087860107
loss is 1.338466763496399
loss is 1.2878013849258423
loss is 1.133717656135559
loss is 1.2087610960006714
loss is 1.1339592933654785
loss is 1.1219401359558105
loss is 1.1913015842437744
loss is 1.2518101930618286
loss is 1.1591031551361084
loss is 1.2986760139465332
loss is 1.22239089012146
loss is 1.3900837898254395
loss is 1.3204096555709839
loss is 1.2677825689315796
loss is 1.194624900817871
loss is 1.1811164617538452
loss is 1.3307483196258545
loss is 1.1846562623977661
loss is 1.1598175764083862
loss is 1.284346342086792
loss is 1.2280070781707764
loss is 1.2366652488708496
loss is 1.2171710729599
loss is 1.1607133150100708
loss is 1.2043133974075317
loss is 1.3513870239257812
loss is 1.2613065242767334
loss is 1.2616263628005981
loss is 1.2172287702560425
loss is 1.252219796180725
loss is 1.219897985458374
loss is 1.1202298402786255
loss is 1.1570725440979004
loss is 1.2213845252990723
loss is 1.2525476217269897
loss is 1.204970359802246
loss is 1.1303101778030396
loss is 1.2431260347366333
loss is 1.1410374641418457
loss is 1.130853533744812
loss is 1.1603975296020508
loss is 1.1471327543258667
loss is 1.1125481128692627
loss is 1.1172001361846924
loss is 1.1639591455459595
loss is 1.1670814752578735
loss is 1.253808617591858
loss is 1.1830759048461914
epoch 14: train_loss = 1.205
14: {'Accuracy': 0.5435, 'Precision': 0.5516, 'Recall': 0.5388, 'F1-score': 0.5352}
epoch: 15
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:30,  1.42it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:33, 11.11it/s]going through batches for holmes training:   4%|▎         | 14/384 [00:00<00:17, 21.72it/s]going through batches for holmes training:   5%|▌         | 20/384 [00:01<00:12, 29.73it/s]going through batches for holmes training:   7%|▋         | 26/384 [00:01<00:09, 36.50it/s]going through batches for holmes training:   8%|▊         | 32/384 [00:01<00:08, 42.00it/s]going through batches for holmes training:  10%|▉         | 38/384 [00:01<00:07, 46.46it/s]going through batches for holmes training:  11%|█▏        | 44/384 [00:01<00:06, 49.84it/s]going through batches for holmes training:  13%|█▎        | 50/384 [00:01<00:06, 52.36it/s]going through batches for holmes training:  15%|█▍        | 56/384 [00:01<00:06, 54.13it/s]going through batches for holmes training:  16%|█▌        | 62/384 [00:01<00:05, 55.41it/s]going through batches for holmes training:  18%|█▊        | 68/384 [00:01<00:05, 56.40it/s]going through batches for holmes training:  19%|█▉        | 74/384 [00:01<00:05, 57.14it/s]going through batches for holmes training:  21%|██        | 80/384 [00:02<00:05, 57.59it/s]going through batches for holmes training:  22%|██▏       | 86/384 [00:02<00:05, 57.85it/s]going through batches for holmes training:  24%|██▍       | 92/384 [00:02<00:05, 58.10it/s]going through batches for holmes training:  26%|██▌       | 98/384 [00:02<00:04, 58.30it/s]going through batches for holmes training:  27%|██▋       | 104/384 [00:02<00:04, 58.38it/s]going through batches for holmes training:  29%|██▊       | 110/384 [00:02<00:04, 58.53it/s]going through batches for holmes training:  30%|███       | 116/384 [00:02<00:04, 58.48it/s]going through batches for holmes training:  32%|███▏      | 122/384 [00:02<00:04, 58.58it/s]going through batches for holmes training:  33%|███▎      | 128/384 [00:02<00:04, 58.60it/s]going through batches for holmes training:  35%|███▍      | 134/384 [00:02<00:04, 58.64it/s]going through batches for holmes training:  36%|███▋      | 140/384 [00:03<00:04, 58.64it/s]going through batches for holmes training:  38%|███▊      | 146/384 [00:03<00:04, 58.65it/s]going through batches for holmes training:  40%|███▉      | 152/384 [00:03<00:03, 58.65it/s]going through batches for holmes training:  41%|████      | 158/384 [00:03<00:03, 58.72it/s]going through batches for holmes training:  43%|████▎     | 164/384 [00:03<00:03, 58.69it/s]going through batches for holmes training:  44%|████▍     | 170/384 [00:03<00:03, 58.73it/s]going through batches for holmes training:  46%|████▌     | 176/384 [00:03<00:03, 58.73it/s]going through batches for holmes training:  47%|████▋     | 182/384 [00:03<00:03, 58.72it/s]going through batches for holmes training:  49%|████▉     | 188/384 [00:03<00:03, 58.67it/s]going through batches for holmes training:  51%|█████     | 194/384 [00:03<00:03, 58.72it/s]going through batches for holmes training:  52%|█████▏    | 200/384 [00:04<00:03, 58.69it/s]going through batches for holmes training:  54%|█████▎    | 206/384 [00:04<00:03, 58.74it/s]going through batches for holmes training:  55%|█████▌    | 212/384 [00:04<00:02, 58.72it/s]going through batches for holmes training:  57%|█████▋    | 218/384 [00:04<00:02, 58.70it/s]going through batches for holmes training:  58%|█████▊    | 224/384 [00:04<00:02, 58.79it/s]going through batches for holmes training:  60%|█████▉    | 230/384 [00:04<00:02, 58.73it/s]going through batches for holmes training:  61%|██████▏   | 236/384 [00:04<00:02, 58.69it/s]going through batches for holmes training:  63%|██████▎   | 242/384 [00:04<00:02, 58.66it/s]going through batches for holmes training:  65%|██████▍   | 248/384 [00:04<00:02, 58.68it/s]going through batches for holmes training:  66%|██████▌   | 254/384 [00:05<00:02, 58.71it/s]going through batches for holmes training:  68%|██████▊   | 260/384 [00:05<00:02, 58.71it/s]going through batches for holmes training:  69%|██████▉   | 266/384 [00:05<00:02, 58.70it/s]going through batches for holmes training:  71%|███████   | 272/384 [00:05<00:01, 58.67it/s]going through batches for holmes training:  72%|███████▏  | 278/384 [00:05<00:01, 58.62it/s]going through batches for holmes training:  74%|███████▍  | 284/384 [00:05<00:01, 58.67it/s]going through batches for holmes training:  76%|███████▌  | 290/384 [00:05<00:01, 58.67it/s]going through batches for holmes training:  77%|███████▋  | 296/384 [00:05<00:01, 58.62it/s]going through batches for holmes training:  79%|███████▊  | 302/384 [00:05<00:01, 58.66it/s]going through batches for holmes training:  80%|████████  | 308/384 [00:05<00:01, 58.62it/s]going through batches for holmes training:  82%|████████▏ | 314/384 [00:06<00:01, 58.70it/s]going through batches for holmes training:  83%|████████▎ | 320/384 [00:06<00:01, 58.71it/s]going through batches for holmes training:  85%|████████▍ | 326/384 [00:06<00:00, 58.66it/s]going through batches for holmes training:  86%|████████▋ | 332/384 [00:06<00:00, 58.63it/s]going through batches for holmes training:  88%|████████▊ | 338/384 [00:06<00:00, 58.69it/s]going through batches for holmes training:  90%|████████▉ | 344/384 [00:06<00:00, 58.60it/s]going through batches for holmes training:  91%|█████████ | 350/384 [00:06<00:00, 58.60it/s]going through batches for holmes training:  93%|█████████▎| 356/384 [00:06<00:00, 58.64it/s]going through batches for holmes training:  94%|█████████▍| 362/384 [00:06<00:00, 58.62it/s]going through batches for holmes training:  96%|█████████▌| 368/384 [00:06<00:00, 58.26it/s]going through batches for holmes training:  97%|█████████▋| 374/384 [00:07<00:00, 58.52it/s]going through batches for holmes training:  99%|█████████▉| 380/384 [00:07<00:00, 58.70it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.78it/s]
loss is 1.146738052368164
loss is 1.155949592590332
loss is 1.1723650693893433
loss is 1.0579395294189453
loss is 1.090508222579956
loss is 1.1442447900772095
loss is 1.1464500427246094
loss is 1.178572416305542
loss is 1.1983603239059448
loss is 1.1889134645462036
loss is 1.0752511024475098
loss is 1.089029312133789
loss is 1.2954598665237427
loss is 1.2263379096984863
loss is 1.191177487373352
loss is 1.1150282621383667
loss is 1.2461497783660889
loss is 1.1475011110305786
loss is 1.109513282775879
loss is 1.2162898778915405
loss is 1.2768787145614624
loss is 1.1261690855026245
loss is 1.1805222034454346
loss is 1.0879132747650146
loss is 1.2381685972213745
loss is 1.2581998109817505
loss is 1.1347150802612305
loss is 1.1213878393173218
loss is 1.1645052433013916
loss is 1.0829458236694336
loss is 1.119807243347168
loss is 1.1450632810592651
loss is 1.1594959497451782
loss is 1.1868888139724731
loss is 1.1676932573318481
loss is 1.1482845544815063
loss is 1.1698579788208008
loss is 1.2837803363800049
loss is 1.296207070350647
loss is 1.139343500137329
loss is 1.2166893482208252
loss is 1.110758662223816
loss is 1.1573814153671265
loss is 1.2453950643539429
loss is 1.2450162172317505
loss is 1.250271201133728
loss is 1.2202274799346924
loss is 1.128665804862976
loss is 1.2380043268203735
loss is 1.1959447860717773
loss is 1.1066045761108398
loss is 1.1026825904846191
loss is 1.281944751739502
loss is 1.2171608209609985
loss is 1.1070637702941895
loss is 1.2117453813552856
loss is 1.1555979251861572
loss is 1.2137079238891602
loss is 1.1353302001953125
loss is 1.1991230249404907
loss is 1.2043988704681396
loss is 1.0538638830184937
loss is 1.1934010982513428
loss is 1.1179039478302002
loss is 1.184310793876648
loss is 1.1718541383743286
loss is 1.1038410663604736
loss is 1.2169331312179565
loss is 1.0663431882858276
loss is 1.1758960485458374
loss is 1.139994502067566
loss is 1.0844690799713135
loss is 1.179754376411438
loss is 1.189281702041626
loss is 1.2276415824890137
loss is 1.221605658531189
loss is 1.1711256504058838
loss is 1.2120717763900757
loss is 1.2073460817337036
loss is 1.1587387323379517
loss is 1.1057127714157104
loss is 1.0293046236038208
loss is 1.1793360710144043
loss is 1.199828028678894
loss is 1.1235780715942383
loss is 1.183727741241455
loss is 1.3500088453292847
loss is 1.1757969856262207
loss is 1.2411009073257446
loss is 1.1098486185073853
loss is 1.1102677583694458
loss is 1.2346540689468384
loss is 1.152979850769043
loss is 1.0998445749282837
loss is 1.2275582551956177
loss is 1.1595722436904907
loss is 1.182139277458191
loss is 1.2257745265960693
loss is 1.2329634428024292
loss is 1.081713318824768
loss is 1.0970921516418457
loss is 1.0883433818817139
loss is 1.3084423542022705
loss is 1.2249205112457275
loss is 1.1744548082351685
loss is 1.2866398096084595
loss is 1.1701455116271973
loss is 1.1263072490692139
loss is 1.1713844537734985
loss is 1.088240146636963
loss is 1.134666919708252
loss is 1.1485542058944702
loss is 1.2868828773498535
loss is 1.1680831909179688
loss is 1.1219894886016846
loss is 1.1482847929000854
loss is 1.191291093826294
loss is 1.1304010152816772
loss is 1.0894572734832764
loss is 1.0479450225830078
loss is 1.172389030456543
loss is 1.1556710004806519
loss is 1.1840091943740845
loss is 1.1881617307662964
loss is 1.1101679801940918
loss is 1.2219653129577637
loss is 1.170771598815918
loss is 1.1010220050811768
loss is 1.2261974811553955
loss is 1.0458749532699585
loss is 1.283928394317627
loss is 1.0940109491348267
loss is 1.1203320026397705
loss is 1.2496159076690674
loss is 1.2260353565216064
loss is 1.2101771831512451
loss is 1.1936442852020264
loss is 1.155181884765625
loss is 1.1568230390548706
loss is 1.194836139678955
loss is 1.1595920324325562
loss is 1.1270203590393066
loss is 1.1112788915634155
loss is 1.1618411540985107
loss is 1.1563154458999634
loss is 1.2304978370666504
loss is 1.1235320568084717
loss is 1.200282335281372
loss is 1.1186319589614868
loss is 1.1674119234085083
loss is 1.204846978187561
loss is 1.2122769355773926
loss is 1.1868619918823242
loss is 1.1232635974884033
loss is 1.124070644378662
loss is 1.0916192531585693
loss is 1.1478281021118164
loss is 1.1852418184280396
loss is 1.1113252639770508
loss is 1.213148593902588
loss is 1.116495966911316
loss is 1.0835850238800049
loss is 1.129988193511963
loss is 1.1081726551055908
loss is 1.1157094240188599
loss is 1.1177036762237549
loss is 1.2307411432266235
loss is 1.1911325454711914
loss is 1.0239355564117432
loss is 1.2921719551086426
loss is 1.1256858110427856
loss is 1.0932908058166504
loss is 1.2115219831466675
loss is 1.0794097185134888
loss is 1.0920652151107788
loss is 1.149143099784851
loss is 1.297714352607727
loss is 1.2337840795516968
loss is 1.1508281230926514
loss is 1.213585615158081
loss is 1.2395603656768799
loss is 1.1681231260299683
loss is 1.1354832649230957
loss is 1.1917963027954102
loss is 1.0707252025604248
loss is 1.1322569847106934
loss is 1.0776559114456177
loss is 1.2777283191680908
loss is 1.177231788635254
loss is 1.1870701313018799
loss is 1.1919384002685547
loss is 1.099420189857483
loss is 1.2558948993682861
loss is 1.2213889360427856
loss is 1.1885385513305664
loss is 1.1347187757492065
loss is 1.1414085626602173
loss is 1.1375538110733032
loss is 1.3410989046096802
loss is 1.2307506799697876
loss is 1.1262770891189575
loss is 1.20296049118042
loss is 1.1874244213104248
loss is 1.1437320709228516
loss is 1.1810625791549683
loss is 1.0912874937057495
loss is 1.137169361114502
loss is 1.1653324365615845
loss is 1.0805418491363525
loss is 1.302662968635559
loss is 1.1593915224075317
loss is 1.2667286396026611
loss is 1.2846299409866333
loss is 1.1339014768600464
loss is 1.2899184226989746
loss is 1.2063926458358765
loss is 1.1926733255386353
loss is 1.2566591501235962
loss is 1.3059718608856201
loss is 1.0488215684890747
loss is 1.1985780000686646
loss is 1.1605013608932495
loss is 1.1381199359893799
loss is 1.1606664657592773
loss is 1.2994341850280762
loss is 1.1793816089630127
loss is 1.178462028503418
loss is 1.1575286388397217
loss is 1.1307820081710815
loss is 1.2135549783706665
loss is 1.191930890083313
loss is 1.2526851892471313
loss is 1.0451436042785645
loss is 1.161197304725647
loss is 1.1033650636672974
loss is 1.1406598091125488
loss is 1.175258994102478
loss is 1.193251609802246
loss is 1.1548157930374146
loss is 1.1349003314971924
loss is 1.1078009605407715
loss is 1.2088916301727295
loss is 1.2134687900543213
loss is 1.0413563251495361
loss is 1.2135906219482422
loss is 1.1493641138076782
loss is 1.0926222801208496
loss is 1.2858387231826782
loss is 1.3020412921905518
loss is 1.1688615083694458
loss is 1.1812713146209717
loss is 1.1280523538589478
loss is 1.2262017726898193
loss is 1.1176350116729736
loss is 1.2195613384246826
loss is 1.3011083602905273
loss is 1.201136589050293
loss is 1.2225269079208374
loss is 1.1845693588256836
loss is 1.260453701019287
loss is 1.1141923666000366
loss is 1.1776021718978882
loss is 1.159164309501648
loss is 1.1163698434829712
loss is 1.156664490699768
loss is 1.0893155336380005
loss is 1.1242356300354004
loss is 1.213454246520996
loss is 1.2377163171768188
loss is 1.1232671737670898
loss is 1.1760777235031128
loss is 1.184666395187378
loss is 1.18586266040802
loss is 1.072842001914978
loss is 1.2015889883041382
loss is 1.1778795719146729
loss is 1.1541482210159302
loss is 1.075508952140808
loss is 1.2832980155944824
loss is 1.1081855297088623
loss is 1.318821668624878
loss is 1.1060103178024292
loss is 1.1349401473999023
loss is 1.1894479990005493
loss is 1.2143274545669556
loss is 1.1810939311981201
loss is 1.2580504417419434
loss is 1.1799339056015015
loss is 1.184628963470459
loss is 1.2079510688781738
loss is 1.286871314048767
loss is 1.1602258682250977
loss is 1.245513677597046
loss is 1.1133865118026733
loss is 1.3029417991638184
loss is 1.1640403270721436
loss is 1.1446030139923096
loss is 1.2732845544815063
loss is 1.0798332691192627
loss is 1.0535109043121338
loss is 1.1160975694656372
loss is 1.2144781351089478
loss is 1.2637954950332642
loss is 1.158307433128357
loss is 1.3364027738571167
loss is 1.2334591150283813
loss is 1.1732100248336792
loss is 1.2133185863494873
loss is 1.3115023374557495
loss is 1.138860821723938
loss is 1.21830415725708
loss is 1.1172491312026978
loss is 1.1916214227676392
loss is 1.175295114517212
loss is 1.1416645050048828
loss is 1.2367417812347412
loss is 1.1669960021972656
loss is 1.248549222946167
loss is 1.2473864555358887
loss is 1.172717809677124
loss is 1.102351188659668
loss is 1.1769193410873413
loss is 1.0894893407821655
loss is 1.0784578323364258
loss is 1.1078296899795532
loss is 1.2165447473526
loss is 1.0309219360351562
loss is 1.1797358989715576
loss is 1.1246699094772339
loss is 1.1406611204147339
loss is 1.0972825288772583
loss is 1.1512709856033325
loss is 1.1499485969543457
loss is 1.0703965425491333
loss is 1.1572281122207642
loss is 1.1384891271591187
loss is 1.1676908731460571
loss is 1.132014513015747
loss is 1.1371914148330688
loss is 1.0674864053726196
loss is 1.2151930332183838
loss is 1.1648228168487549
loss is 1.2070029973983765
loss is 1.103474736213684
loss is 1.2317259311676025
loss is 1.242537260055542
loss is 1.2976253032684326
loss is 1.1657546758651733
loss is 1.1406333446502686
loss is 1.2227997779846191
loss is 1.0729845762252808
loss is 1.2595707178115845
loss is 1.107314109802246
loss is 1.2539578676223755
loss is 1.265512228012085
loss is 1.1484047174453735
loss is 1.206902027130127
loss is 1.2249417304992676
loss is 1.1493704319000244
loss is 1.122169852256775
loss is 1.1077141761779785
loss is 1.1969919204711914
loss is 1.21372652053833
loss is 1.1517244577407837
loss is 1.1999869346618652
loss is 1.1567836999893188
loss is 1.2340188026428223
loss is 1.2035853862762451
loss is 1.225175380706787
loss is 1.2227082252502441
loss is 1.154771327972412
loss is 1.0814310312271118
loss is 1.1521703004837036
loss is 1.1567271947860718
loss is 1.118329405784607
loss is 1.1412968635559082
loss is 1.1910533905029297
loss is 1.1932564973831177
loss is 1.2158669233322144
loss is 1.0879454612731934
loss is 1.1929163932800293
loss is 1.253318190574646
loss is 1.1966006755828857
loss is 1.2105294466018677
epoch 15: train_loss = 1.172
15: {'Accuracy': 0.5511, 'Precision': 0.5526, 'Recall': 0.5484, 'F1-score': 0.5464}
epoch: 16
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:37,  1.38it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:35, 10.75it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 19.91it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:12, 28.20it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:10, 35.27it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 40.90it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 45.27it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:07, 48.67it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 51.23it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.14it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 54.57it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 55.57it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.28it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.81it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.15it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.39it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 57.54it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.60it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.81it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.80it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.84it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 57.80it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:03<00:04, 57.88it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 57.82it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 57.86it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 57.83it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 57.86it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 57.48it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 57.52it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 57.65it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 57.64it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 57.36it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 57.25it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 57.26it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 57.08it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:03, 57.17it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 57.21it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 57.25it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 57.26it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 57.25it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 56.60it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:05<00:02, 56.75it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 56.86it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 56.93it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 56.94it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 57.00it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 57.12it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 57.20it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 57.29it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 57.14it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 57.23it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:06<00:01, 57.31it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 57.21it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 57.39it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 54.24it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 55.27it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 55.92it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 56.31it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 56.67it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 56.98it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:07<00:00, 56.77it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 56.31it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 56.89it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 56.53it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 51.30it/s]
loss is 1.0917869806289673
loss is 1.0620813369750977
loss is 1.2185436487197876
loss is 1.009035587310791
loss is 1.1351463794708252
loss is 1.1996617317199707
loss is 1.038933515548706
loss is 1.1666558980941772
loss is 1.1044665575027466
loss is 1.0912220478057861
loss is 1.1391361951828003
loss is 1.2076406478881836
loss is 1.1125980615615845
loss is 1.0633548498153687
loss is 1.2330200672149658
loss is 1.1427067518234253
loss is 1.0687874555587769
loss is 1.133520483970642
loss is 1.1160062551498413
loss is 1.1751768589019775
loss is 1.0709320306777954
loss is 1.1620299816131592
loss is 1.1376756429672241
loss is 1.1090900897979736
loss is 1.0931217670440674
loss is 1.0443741083145142
loss is 1.1795892715454102
loss is 1.1691168546676636
loss is 1.0018588304519653
loss is 1.142026424407959
loss is 1.0326685905456543
loss is 1.1524217128753662
loss is 1.0813289880752563
loss is 1.119527816772461
loss is 1.1378490924835205
loss is 1.1912345886230469
loss is 1.2850874662399292
loss is 1.263379693031311
loss is 1.1497095823287964
loss is 1.1501151323318481
loss is 1.1750327348709106
loss is 1.1674987077713013
loss is 1.1986815929412842
loss is 1.106074333190918
loss is 1.1450053453445435
loss is 1.0450224876403809
loss is 1.1368398666381836
loss is 1.1418606042861938
loss is 1.1342250108718872
loss is 1.1591153144836426
loss is 1.188991904258728
loss is 1.1515345573425293
loss is 1.113471269607544
loss is 1.132981300354004
loss is 1.160597801208496
loss is 1.1201728582382202
loss is 1.2085368633270264
loss is 1.2282006740570068
loss is 1.0793843269348145
loss is 1.203392505645752
loss is 1.0925956964492798
loss is 1.091774344444275
loss is 1.1296451091766357
loss is 1.0913243293762207
loss is 1.092697024345398
loss is 1.170409083366394
loss is 1.085543155670166
loss is 1.2174296379089355
loss is 1.0658562183380127
loss is 1.1142306327819824
loss is 1.1129952669143677
loss is 1.1565525531768799
loss is 1.1815145015716553
loss is 1.2519851922988892
loss is 1.1581907272338867
loss is 1.0986610651016235
loss is 1.0817235708236694
loss is 1.1328978538513184
loss is 1.1087933778762817
loss is 1.170840859413147
loss is 1.1944061517715454
loss is 1.1639914512634277
loss is 1.2257628440856934
loss is 1.0859767198562622
loss is 1.2382872104644775
loss is 1.0895187854766846
loss is 1.2096340656280518
loss is 1.2281384468078613
loss is 1.2629281282424927
loss is 1.1733263731002808
loss is 1.0565125942230225
loss is 1.1905559301376343
loss is 1.1252249479293823
loss is 1.2047747373580933
loss is 1.1377829313278198
loss is 1.1831927299499512
loss is 1.1663786172866821
loss is 1.1915404796600342
loss is 1.0528924465179443
loss is 1.109654426574707
loss is 1.0897729396820068
loss is 1.0800681114196777
loss is 1.0656193494796753
loss is 1.2144699096679688
loss is 1.1126078367233276
loss is 1.1578021049499512
loss is 1.1696672439575195
loss is 1.1730865240097046
loss is 1.2108261585235596
loss is 1.1577750444412231
loss is 1.1311087608337402
loss is 1.1551835536956787
loss is 1.1824336051940918
loss is 1.0979838371276855
loss is 1.1132984161376953
loss is 1.1037441492080688
loss is 1.07839834690094
loss is 1.07868230342865
loss is 1.213597297668457
loss is 1.0895100831985474
loss is 1.1462912559509277
loss is 1.1534628868103027
loss is 1.1303287744522095
loss is 1.160041332244873
loss is 1.153592586517334
loss is 1.1057097911834717
loss is 1.1613823175430298
loss is 1.120413064956665
loss is 1.0766242742538452
loss is 1.1547938585281372
loss is 1.2732678651809692
loss is 1.1900050640106201
loss is 1.124224305152893
loss is 1.0864907503128052
loss is 1.0470290184020996
loss is 1.180545687675476
loss is 1.1600922346115112
loss is 1.0604462623596191
loss is 1.1360716819763184
loss is 1.1220108270645142
loss is 1.0242563486099243
loss is 1.1803231239318848
loss is 1.1467682123184204
loss is 1.123279094696045
loss is 1.1145210266113281
loss is 1.1210979223251343
loss is 1.029960036277771
loss is 1.191429853439331
loss is 1.1101353168487549
loss is 1.1314798593521118
loss is 1.1047253608703613
loss is 1.083265781402588
loss is 1.1773226261138916
loss is 1.1950511932373047
loss is 1.1355913877487183
loss is 1.1524112224578857
loss is 1.2097885608673096
loss is 1.0095925331115723
loss is 1.0507664680480957
loss is 1.1217535734176636
loss is 1.2629001140594482
loss is 1.221280813217163
loss is 0.9982637166976929
loss is 1.2417399883270264
loss is 1.1438016891479492
loss is 1.1111462116241455
loss is 1.2868369817733765
loss is 1.2282142639160156
loss is 1.141390323638916
loss is 1.103371500968933
loss is 1.0149412155151367
loss is 1.1780606508255005
loss is 1.0876089334487915
loss is 1.0691418647766113
loss is 1.2192758321762085
loss is 1.2218753099441528
loss is 1.166599988937378
loss is 1.2091704607009888
loss is 1.1825259923934937
loss is 1.232468605041504
loss is 1.1459695100784302
loss is 1.1467424631118774
loss is 1.2051951885223389
loss is 1.1771767139434814
loss is 1.0861477851867676
loss is 1.1395400762557983
loss is 1.1310529708862305
loss is 1.3111448287963867
loss is 1.0918262004852295
loss is 1.1460262537002563
loss is 1.1148852109909058
loss is 1.2014553546905518
loss is 1.0946367979049683
loss is 1.1013599634170532
loss is 1.1520103216171265
loss is 1.1620484590530396
loss is 1.153377652168274
loss is 1.1507935523986816
loss is 1.242874264717102
loss is 1.139991283416748
loss is 1.17122483253479
loss is 1.1040022373199463
loss is 1.2082812786102295
loss is 1.1760414838790894
loss is 1.1827272176742554
loss is 1.1300287246704102
loss is 1.212645173072815
loss is 1.1861164569854736
loss is 1.1111218929290771
loss is 1.2045085430145264
loss is 1.0609396696090698
loss is 1.154261827468872
loss is 1.161322832107544
loss is 1.1707181930541992
loss is 1.16254460811615
loss is 1.1940913200378418
loss is 1.1796013116836548
loss is 1.1847413778305054
loss is 1.1934016942977905
loss is 1.1602082252502441
loss is 1.1342675685882568
loss is 1.1257600784301758
loss is 1.0812551975250244
loss is 1.1727324724197388
loss is 1.2179384231567383
loss is 1.2512928247451782
loss is 1.0754450559616089
loss is 1.1451354026794434
loss is 1.2615349292755127
loss is 1.1298096179962158
loss is 1.143216609954834
loss is 1.2744297981262207
loss is 1.1231945753097534
loss is 1.1361737251281738
loss is 1.194817304611206
loss is 1.1674224138259888
loss is 1.0718179941177368
loss is 1.1342613697052002
loss is 1.1594020128250122
loss is 1.248073935508728
loss is 1.1044127941131592
loss is 1.1700602769851685
loss is 1.173545479774475
loss is 1.1386654376983643
loss is 1.2484620809555054
loss is 1.129137396812439
loss is 1.1287556886672974
loss is 1.0847210884094238
loss is 1.1912552118301392
loss is 1.1411160230636597
loss is 1.1530083417892456
loss is 1.1778335571289062
loss is 1.2035720348358154
loss is 1.2154961824417114
loss is 1.185908555984497
loss is 1.154097557067871
loss is 1.094711422920227
loss is 1.088393211364746
loss is 1.1462304592132568
loss is 1.238044261932373
loss is 1.1570963859558105
loss is 1.194385051727295
loss is 1.1487423181533813
loss is 1.0658996105194092
loss is 1.1690179109573364
loss is 1.113114833831787
loss is 0.9980396032333374
loss is 1.2734169960021973
loss is 1.0571976900100708
loss is 1.1686416864395142
loss is 1.1147029399871826
loss is 1.0972018241882324
loss is 1.149970293045044
loss is 1.0764505863189697
loss is 1.0701303482055664
loss is 1.1606327295303345
loss is 1.1183254718780518
loss is 1.1710795164108276
loss is 1.0979610681533813
loss is 1.2343000173568726
loss is 1.171695351600647
loss is 1.0993787050247192
loss is 1.0896084308624268
loss is 1.0988246202468872
loss is 1.0838792324066162
loss is 1.2515236139297485
loss is 1.2259478569030762
loss is 1.0952993631362915
loss is 1.215327501296997
loss is 1.1600723266601562
loss is 1.1229439973831177
loss is 1.1718460321426392
loss is 1.130792498588562
loss is 1.1609009504318237
loss is 1.0508214235305786
loss is 1.1255019903182983
loss is 1.1347153186798096
loss is 1.1520832777023315
loss is 1.1259984970092773
loss is 1.2233951091766357
loss is 1.215253472328186
loss is 1.1912205219268799
loss is 1.1636888980865479
loss is 1.1632057428359985
loss is 1.1154521703720093
loss is 1.1124162673950195
loss is 1.1486575603485107
loss is 1.1100835800170898
loss is 1.1134610176086426
loss is 1.1098871231079102
loss is 1.0808169841766357
loss is 1.042421817779541
loss is 1.220676302909851
loss is 1.1747276782989502
loss is 1.1598860025405884
loss is 1.0549579858779907
loss is 1.221684217453003
loss is 1.0673011541366577
loss is 1.2195817232131958
loss is 1.1809768676757812
loss is 1.2201144695281982
loss is 1.2149070501327515
loss is 1.1551125049591064
loss is 1.0626440048217773
loss is 1.1685700416564941
loss is 1.214452862739563
loss is 1.1329602003097534
loss is 1.1854619979858398
loss is 1.191357135772705
loss is 1.1347922086715698
loss is 1.05470871925354
loss is 1.0604804754257202
loss is 1.1397333145141602
loss is 1.0569984912872314
loss is 1.2257596254348755
loss is 1.2288953065872192
loss is 1.2293028831481934
loss is 1.0470647811889648
loss is 1.1657376289367676
loss is 1.04207444190979
loss is 1.1041449308395386
loss is 1.1910818815231323
loss is 1.0218212604522705
loss is 1.3075571060180664
loss is 1.1451826095581055
loss is 1.1225947141647339
loss is 1.223205327987671
loss is 1.1176962852478027
loss is 1.0757150650024414
loss is 1.2160934209823608
loss is 1.112089991569519
loss is 1.1909093856811523
loss is 1.1658855676651
loss is 1.192065954208374
loss is 1.1558130979537964
loss is 1.1150258779525757
loss is 1.153964638710022
loss is 1.1543391942977905
loss is 1.1641484498977661
loss is 1.058896541595459
loss is 1.156794548034668
loss is 1.216331124305725
loss is 1.1955482959747314
loss is 1.2254996299743652
loss is 1.0994997024536133
loss is 1.1558833122253418
loss is 1.0636132955551147
loss is 1.090518832206726
loss is 1.0732827186584473
loss is 1.1362026929855347
loss is 1.1335476636886597
loss is 1.1962916851043701
loss is 1.1365554332733154
loss is 1.189874291419983
loss is 1.1861017942428589
loss is 1.1069269180297852
loss is 1.1129536628723145
loss is 1.1915596723556519
loss is 1.16291081905365
loss is 1.214793086051941
loss is 1.168873906135559
loss is 1.1375693082809448
loss is 1.0989103317260742
loss is 1.1675318479537964
epoch 16: train_loss = 1.146
16: {'Accuracy': 0.547, 'Precision': 0.5651, 'Recall': 0.5441, 'F1-score': 0.5437}
epoch: 17
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:14,  1.50it/s]going through batches for holmes training:   2%|▏         | 6/384 [00:00<00:37, 10.00it/s]going through batches for holmes training:   3%|▎         | 12/384 [00:00<00:18, 19.77it/s]going through batches for holmes training:   5%|▍         | 18/384 [00:00<00:12, 28.34it/s]going through batches for holmes training:   6%|▋         | 24/384 [00:01<00:10, 35.67it/s]going through batches for holmes training:   8%|▊         | 30/384 [00:01<00:08, 41.49it/s]going through batches for holmes training:   9%|▉         | 36/384 [00:01<00:07, 45.83it/s]going through batches for holmes training:  11%|█         | 42/384 [00:01<00:06, 49.20it/s]going through batches for holmes training:  12%|█▎        | 48/384 [00:01<00:06, 51.63it/s]going through batches for holmes training:  14%|█▍        | 54/384 [00:01<00:06, 53.40it/s]going through batches for holmes training:  16%|█▌        | 60/384 [00:01<00:05, 54.68it/s]going through batches for holmes training:  17%|█▋        | 66/384 [00:01<00:05, 55.64it/s]going through batches for holmes training:  19%|█▉        | 72/384 [00:01<00:05, 56.36it/s]going through batches for holmes training:  20%|██        | 78/384 [00:02<00:05, 56.77it/s]going through batches for holmes training:  22%|██▏       | 84/384 [00:02<00:05, 57.08it/s]going through batches for holmes training:  23%|██▎       | 90/384 [00:02<00:05, 57.36it/s]going through batches for holmes training:  25%|██▌       | 96/384 [00:02<00:05, 57.42it/s]going through batches for holmes training:  27%|██▋       | 102/384 [00:02<00:04, 57.54it/s]going through batches for holmes training:  28%|██▊       | 108/384 [00:02<00:04, 57.58it/s]going through batches for holmes training:  30%|██▉       | 114/384 [00:02<00:04, 57.65it/s]going through batches for holmes training:  31%|███▏      | 120/384 [00:02<00:04, 57.66it/s]going through batches for holmes training:  33%|███▎      | 126/384 [00:02<00:04, 57.63it/s]going through batches for holmes training:  34%|███▍      | 132/384 [00:02<00:04, 57.73it/s]going through batches for holmes training:  36%|███▌      | 138/384 [00:03<00:04, 57.77it/s]going through batches for holmes training:  38%|███▊      | 144/384 [00:03<00:04, 57.89it/s]going through batches for holmes training:  39%|███▉      | 150/384 [00:03<00:04, 57.82it/s]going through batches for holmes training:  41%|████      | 156/384 [00:03<00:03, 57.83it/s]going through batches for holmes training:  42%|████▏     | 162/384 [00:03<00:03, 57.82it/s]going through batches for holmes training:  44%|████▍     | 168/384 [00:03<00:03, 57.79it/s]going through batches for holmes training:  45%|████▌     | 174/384 [00:03<00:03, 57.73it/s]going through batches for holmes training:  47%|████▋     | 180/384 [00:03<00:03, 57.54it/s]going through batches for holmes training:  48%|████▊     | 186/384 [00:03<00:03, 57.68it/s]going through batches for holmes training:  50%|█████     | 192/384 [00:03<00:03, 57.75it/s]going through batches for holmes training:  52%|█████▏    | 198/384 [00:04<00:03, 57.78it/s]going through batches for holmes training:  53%|█████▎    | 204/384 [00:04<00:03, 57.89it/s]going through batches for holmes training:  55%|█████▍    | 210/384 [00:04<00:03, 57.86it/s]going through batches for holmes training:  56%|█████▋    | 216/384 [00:04<00:02, 57.86it/s]going through batches for holmes training:  58%|█████▊    | 222/384 [00:04<00:02, 57.90it/s]going through batches for holmes training:  59%|█████▉    | 228/384 [00:04<00:02, 57.98it/s]going through batches for holmes training:  61%|██████    | 234/384 [00:04<00:02, 58.01it/s]going through batches for holmes training:  62%|██████▎   | 240/384 [00:04<00:02, 57.99it/s]going through batches for holmes training:  64%|██████▍   | 246/384 [00:04<00:02, 58.01it/s]going through batches for holmes training:  66%|██████▌   | 252/384 [00:05<00:02, 57.98it/s]going through batches for holmes training:  67%|██████▋   | 258/384 [00:05<00:02, 58.03it/s]going through batches for holmes training:  69%|██████▉   | 264/384 [00:05<00:02, 58.03it/s]going through batches for holmes training:  70%|███████   | 270/384 [00:05<00:01, 57.99it/s]going through batches for holmes training:  72%|███████▏  | 276/384 [00:05<00:01, 57.97it/s]going through batches for holmes training:  73%|███████▎  | 282/384 [00:05<00:01, 57.97it/s]going through batches for holmes training:  75%|███████▌  | 288/384 [00:05<00:01, 57.97it/s]going through batches for holmes training:  77%|███████▋  | 294/384 [00:05<00:01, 58.04it/s]going through batches for holmes training:  78%|███████▊  | 300/384 [00:05<00:01, 57.93it/s]going through batches for holmes training:  80%|███████▉  | 306/384 [00:05<00:01, 57.94it/s]going through batches for holmes training:  81%|████████▏ | 312/384 [00:06<00:01, 57.81it/s]going through batches for holmes training:  83%|████████▎ | 318/384 [00:06<00:01, 57.83it/s]going through batches for holmes training:  84%|████████▍ | 324/384 [00:06<00:01, 57.96it/s]going through batches for holmes training:  86%|████████▌ | 330/384 [00:06<00:00, 57.94it/s]going through batches for holmes training:  88%|████████▊ | 336/384 [00:06<00:00, 57.96it/s]going through batches for holmes training:  89%|████████▉ | 342/384 [00:06<00:00, 57.97it/s]going through batches for holmes training:  91%|█████████ | 348/384 [00:06<00:00, 57.92it/s]going through batches for holmes training:  92%|█████████▏| 354/384 [00:06<00:00, 57.97it/s]going through batches for holmes training:  94%|█████████▍| 360/384 [00:06<00:00, 57.87it/s]going through batches for holmes training:  95%|█████████▌| 366/384 [00:06<00:00, 57.41it/s]going through batches for holmes training:  97%|█████████▋| 372/384 [00:07<00:00, 57.66it/s]going through batches for holmes training:  98%|█████████▊| 378/384 [00:07<00:00, 57.75it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 57.81it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.22it/s]
loss is 1.026710867881775
loss is 1.1407619714736938
loss is 1.1296796798706055
loss is 1.0926378965377808
loss is 1.1228991746902466
loss is 1.0614583492279053
loss is 1.1294505596160889
loss is 1.2096830606460571
loss is 1.0287959575653076
loss is 1.090698480606079
loss is 1.0897146463394165
loss is 1.2007614374160767
loss is 1.0486385822296143
loss is 1.1740753650665283
loss is 1.099288821220398
loss is 1.016433835029602
loss is 0.9667550921440125
loss is 1.1044307947158813
loss is 1.0887815952301025
loss is 1.1939647197723389
loss is 1.0773718357086182
loss is 1.1420761346817017
loss is 1.1375150680541992
loss is 1.163180947303772
loss is 1.0975011587142944
loss is 1.1148309707641602
loss is 1.0458004474639893
loss is 0.9932564496994019
loss is 1.1169230937957764
loss is 1.1166478395462036
loss is 1.0471534729003906
loss is 0.9975296258926392
loss is 1.1840978860855103
loss is 1.0494579076766968
loss is 1.150978446006775
loss is 0.9899987578392029
loss is 1.0290056467056274
loss is 1.0753459930419922
loss is 1.0492862462997437
loss is 1.1214131116867065
loss is 1.189592719078064
loss is 1.1068824529647827
loss is 1.1597926616668701
loss is 1.0730490684509277
loss is 1.139777421951294
loss is 1.103377103805542
loss is 1.2108074426651
loss is 1.1506457328796387
loss is 1.0588322877883911
loss is 1.097856044769287
loss is 1.1328331232070923
loss is 1.1349704265594482
loss is 1.0923728942871094
loss is 1.168878197669983
loss is 1.2473547458648682
loss is 1.084331750869751
loss is 1.148268699645996
loss is 1.220605492591858
loss is 1.146186113357544
loss is 1.159660816192627
loss is 1.1819798946380615
loss is 1.176090121269226
loss is 1.0775806903839111
loss is 1.0201010704040527
loss is 1.0666232109069824
loss is 1.0676766633987427
loss is 0.9646021127700806
loss is 1.1915128231048584
loss is 1.0802104473114014
loss is 1.0824183225631714
loss is 1.1551642417907715
loss is 1.1688579320907593
loss is 1.2382535934448242
loss is 1.0468753576278687
loss is 1.0133731365203857
loss is 1.1657594442367554
loss is 1.0846302509307861
loss is 1.1467965841293335
loss is 1.1886042356491089
loss is 1.2121888399124146
loss is 1.1384804248809814
loss is 1.1191519498825073
loss is 1.2798124551773071
loss is 1.0517092943191528
loss is 1.2781872749328613
loss is 1.1594887971878052
loss is 1.155397653579712
loss is 1.1251440048217773
loss is 1.1038628816604614
loss is 1.0503369569778442
loss is 1.133263349533081
loss is 1.2072888612747192
loss is 1.2334074974060059
loss is 1.1267085075378418
loss is 1.1743509769439697
loss is 1.183825969696045
loss is 1.1315433979034424
loss is 1.068894624710083
loss is 1.1262083053588867
loss is 1.1151161193847656
loss is 1.1335235834121704
loss is 1.2295790910720825
loss is 1.0999326705932617
loss is 1.2312097549438477
loss is 1.1750832796096802
loss is 1.0949989557266235
loss is 1.011122703552246
loss is 1.071750521659851
loss is 1.0863500833511353
loss is 1.0934112071990967
loss is 1.1584298610687256
loss is 1.0611683130264282
loss is 1.0919533967971802
loss is 1.1678898334503174
loss is 1.0447185039520264
loss is 1.050402283668518
loss is 1.2011349201202393
loss is 1.1127716302871704
loss is 1.1613914966583252
loss is 1.1429280042648315
loss is 1.1131961345672607
loss is 1.2078166007995605
loss is 1.0709614753723145
loss is 1.2044799327850342
loss is 1.1392968893051147
loss is 1.1442793607711792
loss is 1.1420483589172363
loss is 0.9775061011314392
loss is 1.089054822921753
loss is 1.182032585144043
loss is 1.0173375606536865
loss is 1.110430121421814
loss is 1.024109959602356
loss is 1.1379491090774536
loss is 1.0743114948272705
loss is 1.1949690580368042
loss is 1.1338660717010498
loss is 1.280026912689209
loss is 1.0599391460418701
loss is 1.2126176357269287
loss is 1.2238879203796387
loss is 1.122817873954773
loss is 1.1329913139343262
loss is 1.1437346935272217
loss is 1.1541697978973389
loss is 1.174891710281372
loss is 1.0165313482284546
loss is 1.1769614219665527
loss is 1.179355263710022
loss is 1.091472864151001
loss is 1.0660521984100342
loss is 1.0993669033050537
loss is 1.0146719217300415
loss is 1.149001955986023
loss is 1.1944442987442017
loss is 1.0801466703414917
loss is 1.1665059328079224
loss is 1.1770565509796143
loss is 1.1463814973831177
loss is 1.1167268753051758
loss is 1.2174303531646729
loss is 1.1705734729766846
loss is 1.2100642919540405
loss is 1.104922890663147
loss is 1.0273528099060059
loss is 1.0943948030471802
loss is 1.110610842704773
loss is 1.0595676898956299
loss is 1.1632049083709717
loss is 1.070180058479309
loss is 1.077673077583313
loss is 1.1344913244247437
loss is 1.0278855562210083
loss is 1.1493659019470215
loss is 1.0700982809066772
loss is 1.1770548820495605
loss is 1.0825031995773315
loss is 1.144099473953247
loss is 1.1974875926971436
loss is 1.0905057191848755
loss is 1.031943917274475
loss is 1.108662724494934
loss is 1.2072633504867554
loss is 1.071712613105774
loss is 1.0596133470535278
loss is 1.2328568696975708
loss is 1.1362173557281494
loss is 1.1773713827133179
loss is 1.1623950004577637
loss is 1.062817096710205
loss is 1.161389946937561
loss is 1.0654090642929077
loss is 1.058720350265503
loss is 1.0722897052764893
loss is 1.0817588567733765
loss is 1.0828595161437988
loss is 1.1103451251983643
loss is 1.1239838600158691
loss is 1.0787829160690308
loss is 1.1853498220443726
loss is 1.1127628087997437
loss is 1.1510803699493408
loss is 1.1852818727493286
loss is 1.1429650783538818
loss is 1.1740503311157227
loss is 1.2379920482635498
loss is 1.137217402458191
loss is 1.1411280632019043
loss is 1.1693283319473267
loss is 1.1997864246368408
loss is 1.0054330825805664
loss is 1.1126543283462524
loss is 1.2094391584396362
loss is 1.077473759651184
loss is 1.2267721891403198
loss is 1.0917540788650513
loss is 1.1539063453674316
loss is 1.1096988916397095
loss is 1.0832905769348145
loss is 1.180303692817688
loss is 1.144983172416687
loss is 1.0181512832641602
loss is 1.1324025392532349
loss is 0.9789837598800659
loss is 1.0636752843856812
loss is 1.2074755430221558
loss is 1.1243298053741455
loss is 1.077898383140564
loss is 1.1572604179382324
loss is 1.1192190647125244
loss is 1.1764150857925415
loss is 1.0536245107650757
loss is 1.129286289215088
loss is 1.1468087434768677
loss is 1.1501414775848389
loss is 1.0408023595809937
loss is 1.102147102355957
loss is 1.157303810119629
loss is 1.124416708946228
loss is 1.1457486152648926
loss is 1.1942075490951538
loss is 1.1822253465652466
loss is 1.2052667140960693
loss is 1.032160997390747
loss is 1.0572459697723389
loss is 1.0926817655563354
loss is 1.0972763299942017
loss is 1.133508324623108
loss is 1.1274200677871704
loss is 1.135909080505371
loss is 1.1724165678024292
loss is 1.161913275718689
loss is 1.1415961980819702
loss is 1.1372580528259277
loss is 1.012630820274353
loss is 1.2244813442230225
loss is 1.1228013038635254
loss is 1.144850492477417
loss is 1.1364836692810059
loss is 1.2384215593338013
loss is 1.0994583368301392
loss is 1.1809951066970825
loss is 1.0971847772598267
loss is 1.011946439743042
loss is 1.1407963037490845
loss is 1.073564052581787
loss is 1.1762179136276245
loss is 0.9686216115951538
loss is 1.1868869066238403
loss is 1.1858607530593872
loss is 1.1376991271972656
loss is 1.111392855644226
loss is 1.1329855918884277
loss is 1.0272727012634277
loss is 1.0606986284255981
loss is 1.11043119430542
loss is 1.0921703577041626
loss is 1.106282114982605
loss is 1.0714526176452637
loss is 1.1343708038330078
loss is 1.197434663772583
loss is 1.2711756229400635
loss is 1.1699165105819702
loss is 1.1314034461975098
loss is 1.1432864665985107
loss is 1.1214790344238281
loss is 1.1839255094528198
loss is 1.0353097915649414
loss is 1.0402387380599976
loss is 1.1714749336242676
loss is 1.0459234714508057
loss is 1.075193166732788
loss is 1.2054170370101929
loss is 1.1123921871185303
loss is 1.2453802824020386
loss is 1.1384069919586182
loss is 1.0364279747009277
loss is 1.0592759847640991
loss is 1.1996992826461792
loss is 1.064916729927063
loss is 1.0797415971755981
loss is 1.1834923028945923
loss is 1.0321096181869507
loss is 1.0255037546157837
loss is 1.0803179740905762
loss is 1.0247490406036377
loss is 1.0817551612854004
loss is 1.3117648363113403
loss is 1.072444200515747
loss is 1.0292890071868896
loss is 1.142982840538025
loss is 1.1086543798446655
loss is 1.1122345924377441
loss is 1.1448090076446533
loss is 1.1960393190383911
loss is 1.131273627281189
loss is 1.1455193758010864
loss is 1.039682149887085
loss is 1.2388648986816406
loss is 1.1716961860656738
loss is 1.025272250175476
loss is 1.1022249460220337
loss is 1.1778942346572876
loss is 1.0746322870254517
loss is 1.0352574586868286
loss is 1.0178062915802002
loss is 1.1328469514846802
loss is 1.071182370185852
loss is 1.033056616783142
loss is 1.2455241680145264
loss is 1.1225084066390991
loss is 1.0696333646774292
loss is 1.0730453729629517
loss is 1.1344317197799683
loss is 1.2596508264541626
loss is 1.030015468597412
loss is 1.076369047164917
loss is 1.1696107387542725
loss is 1.0923489332199097
loss is 1.1112712621688843
loss is 1.189337134361267
loss is 1.1059136390686035
loss is 1.1656100749969482
loss is 1.1246857643127441
loss is 1.1353209018707275
loss is 1.0745246410369873
loss is 1.1731702089309692
loss is 1.1522305011749268
loss is 1.1177723407745361
loss is 1.0565388202667236
loss is 1.1195191144943237
loss is 1.0823566913604736
loss is 1.1755445003509521
loss is 1.1824873685836792
loss is 1.1271491050720215
loss is 1.1429755687713623
loss is 1.1449228525161743
loss is 1.0952696800231934
loss is 1.1855149269104004
loss is 1.0434012413024902
loss is 1.0915430784225464
loss is 1.0769411325454712
loss is 1.0946223735809326
loss is 1.065030574798584
loss is 1.115067720413208
loss is 1.0652025938034058
loss is 1.0522680282592773
loss is 1.0927212238311768
loss is 1.0914280414581299
loss is 1.1389445066452026
loss is 1.0596810579299927
loss is 1.1445257663726807
loss is 1.0884373188018799
loss is 1.1292437314987183
loss is 1.2553709745407104
loss is 1.1018558740615845
loss is 1.1290638446807861
loss is 1.0773998498916626
loss is 1.1836603879928589
loss is 1.0611640214920044
loss is 1.0828704833984375
loss is 1.1020920276641846
loss is 1.1627161502838135
loss is 1.0702067613601685
epoch 17: train_loss = 1.12
17: {'Accuracy': 0.5476, 'Precision': 0.56, 'Recall': 0.5451, 'F1-score': 0.5403}
epoch: 18
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<05:17,  1.21it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:39,  9.67it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:01<00:20, 18.22it/s]going through batches for holmes training:   5%|▌         | 20/384 [00:01<00:13, 27.49it/s]going through batches for holmes training:   7%|▋         | 26/384 [00:01<00:10, 34.23it/s]going through batches for holmes training:   8%|▊         | 32/384 [00:01<00:08, 39.94it/s]going through batches for holmes training:  10%|▉         | 38/384 [00:01<00:07, 44.62it/s]going through batches for holmes training:  11%|█▏        | 44/384 [00:01<00:07, 48.32it/s]going through batches for holmes training:  13%|█▎        | 50/384 [00:01<00:06, 51.18it/s]going through batches for holmes training:  15%|█▍        | 56/384 [00:01<00:06, 53.26it/s]going through batches for holmes training:  16%|█▌        | 62/384 [00:01<00:05, 54.71it/s]going through batches for holmes training:  18%|█▊        | 68/384 [00:01<00:05, 55.86it/s]going through batches for holmes training:  19%|█▉        | 74/384 [00:02<00:05, 56.62it/s]going through batches for holmes training:  21%|██        | 80/384 [00:02<00:05, 57.18it/s]going through batches for holmes training:  22%|██▏       | 86/384 [00:02<00:05, 57.58it/s]going through batches for holmes training:  24%|██▍       | 92/384 [00:02<00:05, 57.95it/s]going through batches for holmes training:  26%|██▌       | 98/384 [00:02<00:04, 58.10it/s]going through batches for holmes training:  27%|██▋       | 104/384 [00:02<00:04, 58.32it/s]going through batches for holmes training:  29%|██▊       | 110/384 [00:02<00:04, 58.38it/s]going through batches for holmes training:  30%|███       | 116/384 [00:02<00:04, 58.45it/s]going through batches for holmes training:  32%|███▏      | 122/384 [00:02<00:04, 58.49it/s]going through batches for holmes training:  33%|███▎      | 128/384 [00:03<00:04, 58.10it/s]going through batches for holmes training:  35%|███▍      | 134/384 [00:03<00:04, 58.21it/s]going through batches for holmes training:  36%|███▋      | 140/384 [00:03<00:04, 58.29it/s]going through batches for holmes training:  38%|███▊      | 146/384 [00:03<00:04, 58.35it/s]going through batches for holmes training:  40%|███▉      | 152/384 [00:03<00:03, 58.42it/s]going through batches for holmes training:  41%|████      | 158/384 [00:03<00:03, 58.47it/s]going through batches for holmes training:  43%|████▎     | 164/384 [00:03<00:03, 58.56it/s]going through batches for holmes training:  44%|████▍     | 170/384 [00:03<00:03, 58.53it/s]going through batches for holmes training:  46%|████▌     | 176/384 [00:03<00:03, 58.56it/s]going through batches for holmes training:  47%|████▋     | 182/384 [00:03<00:03, 58.59it/s]going through batches for holmes training:  49%|████▉     | 188/384 [00:04<00:03, 58.60it/s]going through batches for holmes training:  51%|█████     | 194/384 [00:04<00:03, 58.55it/s]going through batches for holmes training:  52%|█████▏    | 200/384 [00:04<00:03, 58.63it/s]going through batches for holmes training:  54%|█████▎    | 206/384 [00:04<00:03, 58.67it/s]going through batches for holmes training:  55%|█████▌    | 212/384 [00:04<00:02, 58.67it/s]going through batches for holmes training:  57%|█████▋    | 218/384 [00:04<00:02, 58.73it/s]going through batches for holmes training:  58%|█████▊    | 224/384 [00:04<00:02, 58.63it/s]going through batches for holmes training:  60%|█████▉    | 230/384 [00:04<00:02, 56.84it/s]going through batches for holmes training:  61%|██████▏   | 236/384 [00:04<00:02, 57.40it/s]going through batches for holmes training:  63%|██████▎   | 242/384 [00:04<00:02, 57.72it/s]going through batches for holmes training:  65%|██████▍   | 248/384 [00:05<00:02, 57.95it/s]going through batches for holmes training:  66%|██████▌   | 254/384 [00:05<00:02, 58.10it/s]going through batches for holmes training:  68%|██████▊   | 260/384 [00:05<00:02, 58.03it/s]going through batches for holmes training:  69%|██████▉   | 266/384 [00:05<00:02, 57.29it/s]going through batches for holmes training:  71%|███████   | 272/384 [00:05<00:01, 57.59it/s]going through batches for holmes training:  72%|███████▏  | 278/384 [00:05<00:01, 57.79it/s]going through batches for holmes training:  74%|███████▍  | 284/384 [00:05<00:01, 57.95it/s]going through batches for holmes training:  76%|███████▌  | 290/384 [00:05<00:01, 58.10it/s]going through batches for holmes training:  77%|███████▋  | 296/384 [00:05<00:01, 58.26it/s]going through batches for holmes training:  79%|███████▊  | 302/384 [00:05<00:01, 58.07it/s]going through batches for holmes training:  80%|████████  | 308/384 [00:06<00:01, 58.22it/s]going through batches for holmes training:  82%|████████▏ | 314/384 [00:06<00:01, 58.27it/s]going through batches for holmes training:  83%|████████▎ | 320/384 [00:06<00:01, 58.31it/s]going through batches for holmes training:  85%|████████▍ | 326/384 [00:06<00:00, 58.39it/s]going through batches for holmes training:  86%|████████▋ | 332/384 [00:06<00:00, 58.46it/s]going through batches for holmes training:  88%|████████▊ | 338/384 [00:06<00:00, 58.57it/s]going through batches for holmes training:  90%|████████▉ | 344/384 [00:06<00:00, 58.54it/s]going through batches for holmes training:  91%|█████████ | 350/384 [00:06<00:00, 58.55it/s]going through batches for holmes training:  93%|█████████▎| 356/384 [00:06<00:00, 58.54it/s]going through batches for holmes training:  94%|█████████▍| 362/384 [00:07<00:00, 58.54it/s]going through batches for holmes training:  96%|█████████▌| 368/384 [00:07<00:00, 57.58it/s]going through batches for holmes training:  97%|█████████▋| 374/384 [00:07<00:00, 57.98it/s]going through batches for holmes training:  99%|█████████▉| 380/384 [00:07<00:00, 58.20it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 51.57it/s]
loss is 0.9989814162254333
loss is 1.0304439067840576
loss is 1.0952528715133667
loss is 1.2274370193481445
loss is 1.080439805984497
loss is 0.9689267873764038
loss is 1.043315052986145
loss is 1.0002590417861938
loss is 1.153019666671753
loss is 1.1680768728256226
loss is 0.9987789392471313
loss is 1.0637065172195435
loss is 0.992489218711853
loss is 1.0340454578399658
loss is 1.1012133359909058
loss is 1.0945087671279907
loss is 1.175834059715271
loss is 1.1365448236465454
loss is 1.0440815687179565
loss is 1.0132479667663574
loss is 1.1576443910598755
loss is 1.1014314889907837
loss is 1.138757586479187
loss is 1.0149403810501099
loss is 1.1877012252807617
loss is 1.028803825378418
loss is 1.0962798595428467
loss is 1.1409817934036255
loss is 1.0345045328140259
loss is 1.1710150241851807
loss is 1.0799399614334106
loss is 1.1373629570007324
loss is 1.1003838777542114
loss is 1.0268688201904297
loss is 1.112254023551941
loss is 1.0935899019241333
loss is 1.0359845161437988
loss is 1.0686854124069214
loss is 1.1226067543029785
loss is 1.0794085264205933
loss is 1.0329784154891968
loss is 1.0851308107376099
loss is 1.0278278589248657
loss is 1.1892091035842896
loss is 1.1011691093444824
loss is 1.087491512298584
loss is 1.0854065418243408
loss is 1.1248801946640015
loss is 0.957062304019928
loss is 1.0070949792861938
loss is 1.0255169868469238
loss is 1.0600290298461914
loss is 0.9682838916778564
loss is 1.1066787242889404
loss is 1.1230971813201904
loss is 1.1059030294418335
loss is 1.1739294528961182
loss is 1.1577494144439697
loss is 1.2538233995437622
loss is 1.0749645233154297
loss is 1.0661355257034302
loss is 1.1731483936309814
loss is 1.0281949043273926
loss is 1.0872995853424072
loss is 1.060096025466919
loss is 1.0333812236785889
loss is 1.0958553552627563
loss is 1.1264395713806152
loss is 1.1322251558303833
loss is 1.0445315837860107
loss is 1.0469717979431152
loss is 1.0622475147247314
loss is 1.0189628601074219
loss is 1.1115388870239258
loss is 1.0404999256134033
loss is 1.0774543285369873
loss is 1.0773606300354004
loss is 1.0840126276016235
loss is 0.9955085515975952
loss is 1.0710780620574951
loss is 1.1227617263793945
loss is 1.1293327808380127
loss is 1.0286707878112793
loss is 1.0499355792999268
loss is 1.0303657054901123
loss is 1.0008625984191895
loss is 1.0970150232315063
loss is 1.1036471128463745
loss is 1.0067849159240723
loss is 1.087215781211853
loss is 1.031103491783142
loss is 1.0544064044952393
loss is 1.0922918319702148
loss is 1.098836064338684
loss is 0.9944261908531189
loss is 1.0937820672988892
loss is 1.148823857307434
loss is 1.0193846225738525
loss is 1.0140081644058228
loss is 1.1190271377563477
loss is 1.0454192161560059
loss is 1.0491713285446167
loss is 1.1224502325057983
loss is 1.0963478088378906
loss is 1.0626484155654907
loss is 1.196794033050537
loss is 1.0153294801712036
loss is 1.1171566247940063
loss is 1.1474711894989014
loss is 1.1236205101013184
loss is 0.9985692501068115
loss is 1.107563853263855
loss is 0.9764440655708313
loss is 1.160996437072754
loss is 1.0750935077667236
loss is 1.110478401184082
loss is 1.0313782691955566
loss is 1.0912880897521973
loss is 1.1951123476028442
loss is 1.078889012336731
loss is 1.1241247653961182
loss is 1.1628135442733765
loss is 1.1753737926483154
loss is 1.2384767532348633
loss is 0.9991127848625183
loss is 1.083857774734497
loss is 1.1142536401748657
loss is 1.2063335180282593
loss is 1.0702219009399414
loss is 1.1433876752853394
loss is 1.2193384170532227
loss is 1.073767900466919
loss is 1.094032883644104
loss is 1.0528409481048584
loss is 1.0573642253875732
loss is 1.0260828733444214
loss is 1.0757777690887451
loss is 1.1272748708724976
loss is 1.1439307928085327
loss is 1.1946821212768555
loss is 1.1239292621612549
loss is 1.1244226694107056
loss is 1.1574883460998535
loss is 0.9813986420631409
loss is 1.0000797510147095
loss is 1.1345499753952026
loss is 1.1130775213241577
loss is 1.1714563369750977
loss is 1.0932247638702393
loss is 1.1815404891967773
loss is 1.1718705892562866
loss is 1.048649787902832
loss is 1.1191380023956299
loss is 1.0211819410324097
loss is 1.1511026620864868
loss is 1.0654007196426392
loss is 1.1243900060653687
loss is 1.2074103355407715
loss is 1.0673418045043945
loss is 1.215667963027954
loss is 1.2258975505828857
loss is 1.0700279474258423
loss is 1.079666256904602
loss is 0.9786200523376465
loss is 1.1955535411834717
loss is 1.1779475212097168
loss is 1.193057656288147
loss is 1.0931504964828491
loss is 1.150700569152832
loss is 1.0332127809524536
loss is 1.1663702726364136
loss is 1.0549170970916748
loss is 1.1403133869171143
loss is 1.1654233932495117
loss is 1.0497926473617554
loss is 1.2075265645980835
loss is 0.9856972694396973
loss is 1.046622633934021
loss is 1.098177433013916
loss is 1.1264673471450806
loss is 1.0972251892089844
loss is 1.1096163988113403
loss is 1.1151777505874634
loss is 1.1428169012069702
loss is 1.0638854503631592
loss is 1.168522834777832
loss is 0.9962541460990906
loss is 1.1422070264816284
loss is 1.1076982021331787
loss is 1.1422451734542847
loss is 1.0795148611068726
loss is 1.1666591167449951
loss is 1.0788127183914185
loss is 1.0702012777328491
loss is 1.1075040102005005
loss is 1.078300952911377
loss is 1.1789132356643677
loss is 1.0587131977081299
loss is 1.1130645275115967
loss is 1.0969640016555786
loss is 1.0613856315612793
loss is 1.1151721477508545
loss is 1.169506549835205
loss is 1.1061644554138184
loss is 1.0355671644210815
loss is 1.1703362464904785
loss is 1.1726579666137695
loss is 1.0913221836090088
loss is 1.0766077041625977
loss is 1.044739007949829
loss is 1.0875431299209595
loss is 1.0781657695770264
loss is 1.1946874856948853
loss is 1.0507503747940063
loss is 1.094815969467163
loss is 1.1585547924041748
loss is 1.027295470237732
loss is 1.1328017711639404
loss is 1.22394597530365
loss is 1.2065250873565674
loss is 1.0849570035934448
loss is 1.1356158256530762
loss is 1.1370819807052612
loss is 1.051054835319519
loss is 1.1258660554885864
loss is 1.0678281784057617
loss is 1.1215232610702515
loss is 1.0499770641326904
loss is 1.1060421466827393
loss is 1.1082414388656616
loss is 1.1682062149047852
loss is 0.9965181946754456
loss is 1.1374603509902954
loss is 1.0290886163711548
loss is 1.0955082178115845
loss is 1.1605825424194336
loss is 1.1053217649459839
loss is 1.2001349925994873
loss is 1.1629002094268799
loss is 1.093165397644043
loss is 1.1181206703186035
loss is 1.0781512260437012
loss is 1.0843801498413086
loss is 1.0691736936569214
loss is 1.0010135173797607
loss is 1.1767992973327637
loss is 1.0599491596221924
loss is 1.090315341949463
loss is 1.0942106246948242
loss is 1.1353940963745117
loss is 1.1360656023025513
loss is 1.0945289134979248
loss is 1.048232913017273
loss is 0.9839051961898804
loss is 1.2137904167175293
loss is 1.0258815288543701
loss is 1.0321567058563232
loss is 1.1372692584991455
loss is 1.0186970233917236
loss is 1.0876272916793823
loss is 0.9534391760826111
loss is 1.0841888189315796
loss is 1.0535783767700195
loss is 1.0892064571380615
loss is 1.0338670015335083
loss is 1.0127547979354858
loss is 1.1066302061080933
loss is 1.128801703453064
loss is 0.9839238524436951
loss is 1.1229252815246582
loss is 0.9833720922470093
loss is 1.0029075145721436
loss is 1.1277668476104736
loss is 1.03061044216156
loss is 1.126294493675232
loss is 1.1060309410095215
loss is 1.0698691606521606
loss is 1.0106300115585327
loss is 0.9977974891662598
loss is 1.0244498252868652
loss is 1.0146294832229614
loss is 1.0972892045974731
loss is 1.0784412622451782
loss is 1.1346588134765625
loss is 1.2597298622131348
loss is 1.1207195520401
loss is 1.1158266067504883
loss is 1.1485049724578857
loss is 1.0924803018569946
loss is 1.1348719596862793
loss is 1.1706737279891968
loss is 1.1225924491882324
loss is 1.1252800226211548
loss is 1.0662193298339844
loss is 1.2089736461639404
loss is 1.0280177593231201
loss is 1.1324050426483154
loss is 1.168636441230774
loss is 1.1457209587097168
loss is 1.1734318733215332
loss is 1.1284935474395752
loss is 1.0241143703460693
loss is 1.0835307836532593
loss is 1.094922423362732
loss is 1.1687120199203491
loss is 1.0033763647079468
loss is 1.0915956497192383
loss is 1.129917025566101
loss is 1.125627040863037
loss is 0.9256365895271301
loss is 1.0344349145889282
loss is 1.084689974784851
loss is 1.0609430074691772
loss is 1.1458239555358887
loss is 1.0999715328216553
loss is 1.0955256223678589
loss is 1.1304101943969727
loss is 1.0242431163787842
loss is 1.191025733947754
loss is 1.0365782976150513
loss is 1.0658941268920898
loss is 0.9836977124214172
loss is 1.15654456615448
loss is 1.1244282722473145
loss is 1.1529723405838013
loss is 1.024287223815918
loss is 1.0563280582427979
loss is 0.9914588928222656
loss is 1.109605073928833
loss is 1.0545903444290161
loss is 1.1475738286972046
loss is 1.0953494310379028
loss is 1.101223111152649
loss is 1.1391924619674683
loss is 1.1665090322494507
loss is 0.9908425807952881
loss is 1.173143982887268
loss is 1.0951855182647705
loss is 1.1165680885314941
loss is 1.176706075668335
loss is 1.154697299003601
loss is 1.1495305299758911
loss is 1.1198559999465942
loss is 1.07679283618927
loss is 1.0480808019638062
loss is 1.0653831958770752
loss is 1.0262247323989868
loss is 1.0541536808013916
loss is 1.2413358688354492
loss is 1.1142913103103638
loss is 1.0856595039367676
loss is 1.0257209539413452
loss is 1.0442728996276855
loss is 1.0845998525619507
loss is 1.1262768507003784
loss is 1.088016390800476
loss is 1.162655234336853
loss is 1.1873668432235718
loss is 1.0752602815628052
loss is 1.1429719924926758
loss is 0.9000226855278015
loss is 1.0716005563735962
loss is 1.035598635673523
loss is 1.1395736932754517
loss is 1.1124625205993652
loss is 1.2019377946853638
loss is 1.0888196229934692
loss is 1.0876798629760742
loss is 1.0652109384536743
loss is 1.0140018463134766
loss is 1.1272321939468384
loss is 1.0505633354187012
loss is 1.173740267753601
loss is 1.0905464887619019
loss is 1.1232545375823975
loss is 1.138052225112915
loss is 1.0689409971237183
loss is 1.1085695028305054
loss is 1.0465080738067627
loss is 1.1690030097961426
loss is 1.0440617799758911
loss is 1.1042448282241821
loss is 0.9941866397857666
loss is 1.0584399700164795
epoch 18: train_loss = 1.094
18: {'Accuracy': 0.5632, 'Precision': 0.5754, 'Recall': 0.5611, 'F1-score': 0.5613}
epoch: 19
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:04,  1.56it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:31, 12.05it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:17, 21.72it/s]going through batches for holmes training:   5%|▌         | 20/384 [00:00<00:11, 31.41it/s]going through batches for holmes training:   7%|▋         | 26/384 [00:01<00:09, 38.10it/s]going through batches for holmes training:   8%|▊         | 32/384 [00:01<00:08, 43.38it/s]going through batches for holmes training:  10%|▉         | 38/384 [00:01<00:07, 47.45it/s]going through batches for holmes training:  11%|█▏        | 44/384 [00:01<00:06, 50.61it/s]going through batches for holmes training:  13%|█▎        | 50/384 [00:01<00:06, 52.91it/s]going through batches for holmes training:  15%|█▍        | 56/384 [00:01<00:06, 54.58it/s]going through batches for holmes training:  16%|█▌        | 62/384 [00:01<00:05, 55.81it/s]going through batches for holmes training:  18%|█▊        | 68/384 [00:01<00:05, 56.70it/s]going through batches for holmes training:  19%|█▉        | 74/384 [00:01<00:05, 57.28it/s]going through batches for holmes training:  21%|██        | 80/384 [00:01<00:05, 57.67it/s]going through batches for holmes training:  22%|██▏       | 86/384 [00:02<00:05, 58.03it/s]going through batches for holmes training:  24%|██▍       | 92/384 [00:02<00:05, 58.24it/s]going through batches for holmes training:  26%|██▌       | 98/384 [00:02<00:04, 58.43it/s]going through batches for holmes training:  27%|██▋       | 104/384 [00:02<00:04, 58.55it/s]going through batches for holmes training:  29%|██▊       | 110/384 [00:02<00:04, 58.61it/s]going through batches for holmes training:  30%|███       | 116/384 [00:02<00:04, 58.64it/s]going through batches for holmes training:  32%|███▏      | 122/384 [00:02<00:04, 58.67it/s]going through batches for holmes training:  33%|███▎      | 128/384 [00:02<00:04, 58.62it/s]going through batches for holmes training:  35%|███▍      | 134/384 [00:02<00:04, 58.23it/s]going through batches for holmes training:  36%|███▋      | 140/384 [00:03<00:04, 58.33it/s]going through batches for holmes training:  38%|███▊      | 146/384 [00:03<00:04, 58.43it/s]going through batches for holmes training:  40%|███▉      | 152/384 [00:03<00:03, 58.42it/s]going through batches for holmes training:  41%|████      | 158/384 [00:03<00:03, 58.50it/s]going through batches for holmes training:  43%|████▎     | 164/384 [00:03<00:03, 58.51it/s]going through batches for holmes training:  44%|████▍     | 170/384 [00:03<00:03, 58.61it/s]going through batches for holmes training:  46%|████▌     | 176/384 [00:03<00:03, 58.59it/s]going through batches for holmes training:  47%|████▋     | 182/384 [00:03<00:03, 58.66it/s]going through batches for holmes training:  49%|████▉     | 188/384 [00:03<00:03, 58.79it/s]going through batches for holmes training:  51%|█████     | 194/384 [00:03<00:03, 58.86it/s]going through batches for holmes training:  52%|█████▏    | 200/384 [00:04<00:03, 58.87it/s]going through batches for holmes training:  54%|█████▎    | 206/384 [00:04<00:03, 58.74it/s]going through batches for holmes training:  55%|█████▌    | 212/384 [00:04<00:02, 58.77it/s]going through batches for holmes training:  57%|█████▋    | 218/384 [00:04<00:02, 58.81it/s]going through batches for holmes training:  58%|█████▊    | 224/384 [00:04<00:02, 58.81it/s]going through batches for holmes training:  60%|█████▉    | 230/384 [00:04<00:02, 58.84it/s]going through batches for holmes training:  61%|██████▏   | 236/384 [00:04<00:02, 58.83it/s]going through batches for holmes training:  63%|██████▎   | 242/384 [00:04<00:02, 58.75it/s]going through batches for holmes training:  65%|██████▍   | 248/384 [00:04<00:02, 58.70it/s]going through batches for holmes training:  66%|██████▌   | 254/384 [00:04<00:02, 58.76it/s]going through batches for holmes training:  68%|██████▊   | 260/384 [00:05<00:02, 58.72it/s]going through batches for holmes training:  69%|██████▉   | 266/384 [00:05<00:02, 58.34it/s]going through batches for holmes training:  71%|███████   | 272/384 [00:05<00:01, 58.43it/s]going through batches for holmes training:  72%|███████▏  | 278/384 [00:05<00:01, 58.51it/s]going through batches for holmes training:  74%|███████▍  | 284/384 [00:05<00:01, 58.57it/s]going through batches for holmes training:  76%|███████▌  | 290/384 [00:05<00:01, 58.59it/s]going through batches for holmes training:  77%|███████▋  | 296/384 [00:05<00:01, 58.62it/s]going through batches for holmes training:  79%|███████▊  | 302/384 [00:05<00:01, 58.61it/s]going through batches for holmes training:  80%|████████  | 308/384 [00:05<00:01, 58.61it/s]going through batches for holmes training:  82%|████████▏ | 314/384 [00:05<00:01, 58.67it/s]going through batches for holmes training:  83%|████████▎ | 320/384 [00:06<00:01, 58.76it/s]going through batches for holmes training:  85%|████████▍ | 326/384 [00:06<00:00, 58.71it/s]going through batches for holmes training:  86%|████████▋ | 332/384 [00:06<00:00, 58.67it/s]going through batches for holmes training:  88%|████████▊ | 338/384 [00:06<00:00, 58.64it/s]going through batches for holmes training:  90%|████████▉ | 344/384 [00:06<00:00, 58.63it/s]going through batches for holmes training:  91%|█████████ | 350/384 [00:06<00:00, 58.60it/s]going through batches for holmes training:  93%|█████████▎| 356/384 [00:06<00:00, 58.67it/s]going through batches for holmes training:  94%|█████████▍| 362/384 [00:06<00:00, 58.72it/s]going through batches for holmes training:  96%|█████████▌| 368/384 [00:06<00:00, 58.22it/s]going through batches for holmes training:  97%|█████████▋| 374/384 [00:07<00:00, 58.47it/s]going through batches for holmes training:  99%|█████████▉| 380/384 [00:07<00:00, 58.66it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 53.22it/s]
loss is 0.9310534596443176
loss is 1.076345443725586
loss is 1.0464683771133423
loss is 1.1060593128204346
loss is 1.0502221584320068
loss is 1.1414692401885986
loss is 1.0926361083984375
loss is 1.002168893814087
loss is 1.0631715059280396
loss is 0.9874166250228882
loss is 0.9803305864334106
loss is 1.015468716621399
loss is 1.0883153676986694
loss is 1.0930640697479248
loss is 1.121978998184204
loss is 1.1159765720367432
loss is 1.100764513015747
loss is 1.0490037202835083
loss is 1.0566993951797485
loss is 1.021735429763794
loss is 1.059780478477478
loss is 1.0468287467956543
loss is 1.054509162902832
loss is 1.041702389717102
loss is 1.0646010637283325
loss is 1.176208257675171
loss is 1.1480515003204346
loss is 1.0626097917556763
loss is 1.0084459781646729
loss is 0.8720047473907471
loss is 1.0354201793670654
loss is 1.1454441547393799
loss is 1.1182996034622192
loss is 1.082442045211792
loss is 1.0531797409057617
loss is 1.0459904670715332
loss is 1.1971213817596436
loss is 0.9527775645256042
loss is 1.0408811569213867
loss is 1.1617692708969116
loss is 0.926003098487854
loss is 1.0147713422775269
loss is 1.0784518718719482
loss is 0.9118030071258545
loss is 1.0080963373184204
loss is 1.092726230621338
loss is 1.0018001794815063
loss is 1.0469281673431396
loss is 1.0714036226272583
loss is 1.104895830154419
loss is 0.9619868397712708
loss is 1.1105297803878784
loss is 1.1039776802062988
loss is 1.1630581617355347
loss is 1.0214425325393677
loss is 1.1221873760223389
loss is 1.0145807266235352
loss is 1.0737296342849731
loss is 1.0454117059707642
loss is 1.0181353092193604
loss is 1.1193734407424927
loss is 1.0492370128631592
loss is 1.1610008478164673
loss is 1.0973081588745117
loss is 1.1421006917953491
loss is 1.1097824573516846
loss is 1.0235809087753296
loss is 0.9873092770576477
loss is 1.1479499340057373
loss is 1.1125725507736206
loss is 1.0294378995895386
loss is 1.0536210536956787
loss is 0.9775171875953674
loss is 1.1197936534881592
loss is 1.0651345252990723
loss is 1.0659549236297607
loss is 1.0388492345809937
loss is 0.9279876947402954
loss is 1.1185399293899536
loss is 1.0219606161117554
loss is 1.0930241346359253
loss is 1.0700434446334839
loss is 1.0976985692977905
loss is 1.0781140327453613
loss is 0.9733753800392151
loss is 0.968940258026123
loss is 1.0998092889785767
loss is 0.8999820947647095
loss is 0.9805192351341248
loss is 0.9814097881317139
loss is 1.0935827493667603
loss is 1.1402260065078735
loss is 1.0495777130126953
loss is 1.0739792585372925
loss is 1.091425895690918
loss is 0.9763796925544739
loss is 1.0471867322921753
loss is 1.1024543046951294
loss is 1.135607361793518
loss is 1.0037314891815186
loss is 1.0008705854415894
loss is 1.1344211101531982
loss is 1.0792146921157837
loss is 1.0735529661178589
loss is 0.9978113770484924
loss is 1.046034812927246
loss is 1.213220477104187
loss is 1.0334054231643677
loss is 0.9953550100326538
loss is 1.097307562828064
loss is 0.9210457801818848
loss is 0.9986340999603271
loss is 1.0394723415374756
loss is 1.0855138301849365
loss is 1.1178797483444214
loss is 1.0510127544403076
loss is 1.100278377532959
loss is 1.0745837688446045
loss is 1.1268980503082275
loss is 1.0945022106170654
loss is 1.1047965288162231
loss is 1.0511507987976074
loss is 1.0099653005599976
loss is 1.0427300930023193
loss is 1.105784296989441
loss is 1.1198790073394775
loss is 1.1316673755645752
loss is 1.1867543458938599
loss is 1.0358628034591675
loss is 1.0618375539779663
loss is 1.0604106187820435
loss is 1.1003872156143188
loss is 1.1796196699142456
loss is 0.9963384866714478
loss is 1.156485676765442
loss is 1.0776045322418213
loss is 1.0815261602401733
loss is 1.1021596193313599
loss is 1.0510149002075195
loss is 1.056369423866272
loss is 1.1541427373886108
loss is 1.1651026010513306
loss is 1.04497492313385
loss is 0.9816395044326782
loss is 1.0300683975219727
loss is 1.0101227760314941
loss is 1.0837253332138062
loss is 1.1242810487747192
loss is 1.1342592239379883
loss is 1.0180675983428955
loss is 1.1153736114501953
loss is 0.9414018392562866
loss is 1.0394065380096436
loss is 1.1216574907302856
loss is 1.0343788862228394
loss is 1.1122679710388184
loss is 1.083089828491211
loss is 1.040224552154541
loss is 1.0587782859802246
loss is 1.0581730604171753
loss is 1.1255477666854858
loss is 1.024124026298523
loss is 1.0992588996887207
loss is 1.0929954051971436
loss is 0.9774225354194641
loss is 1.1676950454711914
loss is 1.2161282300949097
loss is 1.1544493436813354
loss is 0.9801027178764343
loss is 1.1279135942459106
loss is 1.100875735282898
loss is 1.011643886566162
loss is 1.1587258577346802
loss is 1.084761142730713
loss is 1.0375534296035767
loss is 1.116005301475525
loss is 1.0723694562911987
loss is 0.9405204653739929
loss is 1.0781333446502686
loss is 1.111983060836792
loss is 1.0421289205551147
loss is 1.0937787294387817
loss is 1.0157055854797363
loss is 1.1658293008804321
loss is 1.0617680549621582
loss is 1.021976351737976
loss is 1.1483218669891357
loss is 1.0478630065917969
loss is 1.0131922960281372
loss is 0.962596595287323
loss is 1.162187099456787
loss is 1.173392415046692
loss is 1.092658519744873
loss is 1.088594913482666
loss is 1.0497541427612305
loss is 1.139251947402954
loss is 1.058050274848938
loss is 1.1473701000213623
loss is 1.0422897338867188
loss is 1.1219216585159302
loss is 1.1067065000534058
loss is 1.2568663358688354
loss is 1.011336088180542
loss is 1.0373681783676147
loss is 1.0580220222473145
loss is 1.103689193725586
loss is 1.08798086643219
loss is 1.1337487697601318
loss is 1.0911458730697632
loss is 1.1371731758117676
loss is 1.0759440660476685
loss is 1.029323935508728
loss is 0.9962841272354126
loss is 1.1003261804580688
loss is 1.1025334596633911
loss is 1.0623753070831299
loss is 1.0166999101638794
loss is 1.0561411380767822
loss is 1.1365379095077515
loss is 1.2052907943725586
loss is 1.2074940204620361
loss is 1.0878630876541138
loss is 1.0761266946792603
loss is 1.110920786857605
loss is 1.0571343898773193
loss is 0.9835414290428162
loss is 1.0492442846298218
loss is 1.1079163551330566
loss is 1.1290538311004639
loss is 1.0269938707351685
loss is 1.0787060260772705
loss is 1.2148593664169312
loss is 1.1232810020446777
loss is 1.1022753715515137
loss is 1.2431960105895996
loss is 1.0821928977966309
loss is 1.0046594142913818
loss is 1.1761530637741089
loss is 0.9807697534561157
loss is 1.0617979764938354
loss is 1.030927062034607
loss is 0.980994462966919
loss is 1.1348696947097778
loss is 1.1274921894073486
loss is 1.0447397232055664
loss is 1.0448009967803955
loss is 1.0107426643371582
loss is 1.0727630853652954
loss is 1.0938348770141602
loss is 1.0869450569152832
loss is 1.250069260597229
loss is 1.0584110021591187
loss is 1.1681362390518188
loss is 1.0827038288116455
loss is 1.0807403326034546
loss is 1.0388227701187134
loss is 0.9988473653793335
loss is 0.975495457649231
loss is 1.0852848291397095
loss is 1.017141342163086
loss is 1.193068027496338
loss is 1.0756276845932007
loss is 1.1465275287628174
loss is 1.0457453727722168
loss is 1.0312683582305908
loss is 0.9242334961891174
loss is 1.0619539022445679
loss is 1.0743316411972046
loss is 1.1180644035339355
loss is 1.0710369348526
loss is 1.0879402160644531
loss is 1.1265499591827393
loss is 1.0228554010391235
loss is 1.1345962285995483
loss is 1.1571588516235352
loss is 1.0717041492462158
loss is 1.0453815460205078
loss is 1.0226798057556152
loss is 1.0884387493133545
loss is 0.9600956439971924
loss is 1.1516051292419434
loss is 1.1109631061553955
loss is 1.078966498374939
loss is 1.1758633852005005
loss is 0.9852080345153809
loss is 1.1299529075622559
loss is 1.003772258758545
loss is 1.1907339096069336
loss is 1.087893009185791
loss is 1.028073787689209
loss is 0.9755845069885254
loss is 1.072107195854187
loss is 1.1164344549179077
loss is 1.079138994216919
loss is 1.1953825950622559
loss is 1.1326483488082886
loss is 1.048587679862976
loss is 1.080420732498169
loss is 0.9938676953315735
loss is 1.0914738178253174
loss is 1.1814810037612915
loss is 1.1009992361068726
loss is 1.216335654258728
loss is 1.1050994396209717
loss is 1.090928077697754
loss is 1.079479694366455
loss is 1.1741275787353516
loss is 1.113572120666504
loss is 1.0717889070510864
loss is 1.1294114589691162
loss is 1.0489130020141602
loss is 1.142951488494873
loss is 1.0702316761016846
loss is 1.066502332687378
loss is 1.1003749370574951
loss is 1.0822619199752808
loss is 1.030510663986206
loss is 1.1052930355072021
loss is 0.9778057932853699
loss is 1.0114245414733887
loss is 1.1240017414093018
loss is 1.1330418586730957
loss is 0.9717946648597717
loss is 0.9971073269844055
loss is 0.983479380607605
loss is 1.0909011363983154
loss is 1.0373708009719849
loss is 1.0879114866256714
loss is 1.0432782173156738
loss is 1.0934520959854126
loss is 1.0926432609558105
loss is 1.2061057090759277
loss is 1.0998927354812622
loss is 1.077994704246521
loss is 1.000299334526062
loss is 1.0109447240829468
loss is 1.1407017707824707
loss is 1.0960853099822998
loss is 1.1552953720092773
loss is 1.0193160772323608
loss is 1.0363030433654785
loss is 1.0578320026397705
loss is 1.0602266788482666
loss is 1.0916651487350464
loss is 1.0481045246124268
loss is 1.0270408391952515
loss is 0.9439356923103333
loss is 1.06001877784729
loss is 1.1069999933242798
loss is 1.1294695138931274
loss is 0.9991161823272705
loss is 1.134271264076233
loss is 1.1212890148162842
loss is 1.1182159185409546
loss is 1.0949758291244507
loss is 1.2212553024291992
loss is 1.0396287441253662
loss is 1.0965553522109985
loss is 1.0770328044891357
loss is 1.109031319618225
loss is 1.1370526552200317
loss is 1.062340497970581
loss is 1.077385425567627
loss is 1.0358073711395264
loss is 1.106307864189148
loss is 1.251139521598816
loss is 1.068532943725586
loss is 1.0721571445465088
loss is 1.2741594314575195
loss is 1.103855848312378
loss is 1.0619698762893677
loss is 1.046852946281433
loss is 1.133413314819336
loss is 0.9999097585678101
loss is 1.0649327039718628
loss is 1.0422296524047852
loss is 1.0683159828186035
loss is 1.0221675634384155
loss is 0.9990649223327637
loss is 1.021461844444275
loss is 0.9893701672554016
loss is 1.0752265453338623
loss is 1.0443840026855469
loss is 1.0762298107147217
epoch 19: train_loss = 1.074
19: {'Accuracy': 0.5595, 'Precision': 0.5647, 'Recall': 0.5599, 'F1-score': 0.5563}
epoch: 20
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:38,  1.38it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:34, 10.87it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 19.90it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:12, 28.27it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:10, 35.40it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 41.03it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 45.54it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 48.96it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 51.45it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.42it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 54.79it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 55.76it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.42it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.98it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.33it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.54it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 57.68it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.75it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.81it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.86it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.94it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.06it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:03<00:04, 58.01it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 55.73it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 56.32it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 56.84it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 57.20it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 57.45it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 57.63it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 57.73it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 57.84it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 57.85it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 57.88it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 57.85it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 57.87it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.00it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 57.95it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 57.97it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.01it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.03it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.06it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.02it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 58.07it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.01it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 57.97it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 57.97it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.02it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.05it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.10it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.11it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 57.83it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:06<00:01, 57.82it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 57.85it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 57.88it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 57.88it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 57.91it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 57.78it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 57.89it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 57.90it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 57.95it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 57.99it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 57.50it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 57.74it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 57.90it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 51.97it/s]
loss is 1.0167018175125122
loss is 0.9454151391983032
loss is 0.9688587188720703
loss is 0.8934987783432007
loss is 0.9229151010513306
loss is 1.0146114826202393
loss is 1.104460597038269
loss is 0.995137631893158
loss is 0.9667083024978638
loss is 0.9878968596458435
loss is 1.1078273057937622
loss is 1.0510903596878052
loss is 0.9396102428436279
loss is 0.9486070275306702
loss is 0.9830320477485657
loss is 0.9856969714164734
loss is 1.0720090866088867
loss is 0.9549770951271057
loss is 1.0505504608154297
loss is 1.0073628425598145
loss is 0.9300182461738586
loss is 0.9648363590240479
loss is 0.9790303111076355
loss is 1.016359806060791
loss is 1.0957539081573486
loss is 1.0346653461456299
loss is 1.1497896909713745
loss is 1.0372215509414673
loss is 1.0132062435150146
loss is 1.0355757474899292
loss is 0.9713849425315857
loss is 0.9788234233856201
loss is 1.0984138250350952
loss is 0.9406294226646423
loss is 1.015607476234436
loss is 1.0314964056015015
loss is 1.0939345359802246
loss is 1.1210955381393433
loss is 1.0135467052459717
loss is 1.015112280845642
loss is 1.1554040908813477
loss is 1.0667616128921509
loss is 1.0437159538269043
loss is 1.1533758640289307
loss is 0.9795249700546265
loss is 1.0640556812286377
loss is 1.0766571760177612
loss is 1.022766351699829
loss is 1.0938693284988403
loss is 1.054989218711853
loss is 1.0646418333053589
loss is 1.0197350978851318
loss is 1.1204086542129517
loss is 1.1296709775924683
loss is 1.0197447538375854
loss is 0.9933555722236633
loss is 1.0447851419448853
loss is 1.058463454246521
loss is 1.0335025787353516
loss is 1.0213360786437988
loss is 1.0496125221252441
loss is 1.0421925783157349
loss is 1.0700753927230835
loss is 1.0033385753631592
loss is 1.028630256652832
loss is 1.0504218339920044
loss is 1.0306658744812012
loss is 1.1162184476852417
loss is 1.0411953926086426
loss is 1.072089433670044
loss is 1.100409746170044
loss is 1.0812344551086426
loss is 1.1056596040725708
loss is 1.0687968730926514
loss is 1.0191752910614014
loss is 1.0673496723175049
loss is 0.9934831261634827
loss is 0.9824056029319763
loss is 0.9834079742431641
loss is 1.0112721920013428
loss is 1.0780696868896484
loss is 0.94813072681427
loss is 1.1056342124938965
loss is 1.1405913829803467
loss is 1.1727614402770996
loss is 0.9578437805175781
loss is 1.080796480178833
loss is 1.0829496383666992
loss is 1.0210169553756714
loss is 0.9866807460784912
loss is 1.0500258207321167
loss is 1.0404974222183228
loss is 0.96807461977005
loss is 1.1036581993103027
loss is 0.9259647130966187
loss is 0.9241862297058105
loss is 1.022375464439392
loss is 1.0224113464355469
loss is 0.9894201755523682
loss is 1.1135709285736084
loss is 1.0379558801651
loss is 1.160119652748108
loss is 1.0819748640060425
loss is 1.1110514402389526
loss is 1.0476733446121216
loss is 0.941701352596283
loss is 1.005114197731018
loss is 1.0808076858520508
loss is 1.051864504814148
loss is 0.9019067287445068
loss is 0.9851224422454834
loss is 1.2004536390304565
loss is 1.1283650398254395
loss is 1.0627052783966064
loss is 1.0124716758728027
loss is 0.9647833108901978
loss is 0.9990106225013733
loss is 1.0355725288391113
loss is 1.1531469821929932
loss is 0.9804158210754395
loss is 1.1151851415634155
loss is 0.9784875512123108
loss is 0.9466753602027893
loss is 1.1179454326629639
loss is 1.104649305343628
loss is 1.0663039684295654
loss is 1.0097063779830933
loss is 1.0328983068466187
loss is 0.91985684633255
loss is 0.9801835417747498
loss is 1.082028865814209
loss is 1.0099701881408691
loss is 1.0723284482955933
loss is 1.0033372640609741
loss is 1.0892504453659058
loss is 0.9662519097328186
loss is 1.072371482849121
loss is 1.0498738288879395
loss is 1.0257232189178467
loss is 0.9714398384094238
loss is 1.0901579856872559
loss is 1.02012300491333
loss is 1.100183367729187
loss is 1.0512256622314453
loss is 1.112747073173523
loss is 0.9396291971206665
loss is 1.0790417194366455
loss is 1.0381550788879395
loss is 1.1587733030319214
loss is 1.0819579362869263
loss is 0.9581800699234009
loss is 1.0410069227218628
loss is 1.0489085912704468
loss is 1.0205607414245605
loss is 1.2112535238265991
loss is 0.960783839225769
loss is 1.1771678924560547
loss is 0.9979465007781982
loss is 1.1405737400054932
loss is 1.068415880203247
loss is 1.0220720767974854
loss is 1.0191106796264648
loss is 1.0861256122589111
loss is 1.0551743507385254
loss is 1.0368716716766357
loss is 1.0486829280853271
loss is 0.9868907928466797
loss is 1.0699512958526611
loss is 1.122580647468567
loss is 1.0562529563903809
loss is 1.0639396905899048
loss is 1.00888192653656
loss is 1.0321041345596313
loss is 1.0502326488494873
loss is 1.1588866710662842
loss is 1.1530905961990356
loss is 1.0110678672790527
loss is 1.009841799736023
loss is 0.9368618726730347
loss is 1.0833688974380493
loss is 1.0301446914672852
loss is 1.057471513748169
loss is 0.8944990634918213
loss is 1.0112241506576538
loss is 0.9189660549163818
loss is 1.068188190460205
loss is 1.07490873336792
loss is 1.0613341331481934
loss is 1.128716230392456
loss is 1.0951963663101196
loss is 0.9091540575027466
loss is 1.0557082891464233
loss is 1.0569325685501099
loss is 1.011630892753601
loss is 1.12462317943573
loss is 1.0056581497192383
loss is 1.0969513654708862
loss is 1.082119107246399
loss is 0.9873371720314026
loss is 1.1123356819152832
loss is 1.0712052583694458
loss is 1.1393474340438843
loss is 1.061882495880127
loss is 0.9931485056877136
loss is 0.9581292867660522
loss is 1.106871247291565
loss is 1.0983827114105225
loss is 0.9923302531242371
loss is 1.0276812314987183
loss is 0.9889893531799316
loss is 1.1687860488891602
loss is 1.0412176847457886
loss is 1.0189979076385498
loss is 1.0621562004089355
loss is 1.11155366897583
loss is 0.9789910912513733
loss is 1.1461776494979858
loss is 1.053134799003601
loss is 1.0292943716049194
loss is 1.0821467638015747
loss is 1.1013362407684326
loss is 0.9695128798484802
loss is 1.208227515220642
loss is 1.018117904663086
loss is 1.0130810737609863
loss is 1.047512412071228
loss is 1.0697550773620605
loss is 1.0054324865341187
loss is 1.0109765529632568
loss is 1.0343400239944458
loss is 1.0211844444274902
loss is 1.024114966392517
loss is 1.0564192533493042
loss is 1.0486201047897339
loss is 1.2213914394378662
loss is 1.0279806852340698
loss is 0.994791567325592
loss is 1.1112487316131592
loss is 1.173412561416626
loss is 1.0447500944137573
loss is 0.9698010087013245
loss is 1.1147605180740356
loss is 0.982948362827301
loss is 1.0816086530685425
loss is 1.115465521812439
loss is 1.025559902191162
loss is 0.970329761505127
loss is 1.0838559865951538
loss is 1.1030445098876953
loss is 0.9607915282249451
loss is 1.0349878072738647
loss is 1.0510437488555908
loss is 1.057579755783081
loss is 1.1408286094665527
loss is 1.1528568267822266
loss is 1.0373623371124268
loss is 1.1073448657989502
loss is 1.0715550184249878
loss is 1.1024677753448486
loss is 1.149706244468689
loss is 1.011351227760315
loss is 1.153181791305542
loss is 0.9870234727859497
loss is 1.1030924320220947
loss is 1.0694022178649902
loss is 0.9425114989280701
loss is 0.9696758389472961
loss is 1.1034138202667236
loss is 1.0072026252746582
loss is 1.121728539466858
loss is 1.0222169160842896
loss is 1.0283697843551636
loss is 1.0869693756103516
loss is 1.0093822479248047
loss is 1.0694959163665771
loss is 1.0479931831359863
loss is 1.0803800821304321
loss is 0.9967468976974487
loss is 1.1461875438690186
loss is 1.0844215154647827
loss is 1.0625332593917847
loss is 1.019981861114502
loss is 1.148966670036316
loss is 1.0769610404968262
loss is 0.9177469611167908
loss is 1.118112325668335
loss is 1.0118809938430786
loss is 1.0577232837677002
loss is 0.9971635937690735
loss is 1.0758923292160034
loss is 1.0025603771209717
loss is 1.0960887670516968
loss is 1.0568023920059204
loss is 1.1118907928466797
loss is 1.0703388452529907
loss is 1.171138882637024
loss is 1.0037941932678223
loss is 1.0724226236343384
loss is 1.1346224546432495
loss is 1.133994460105896
loss is 1.0575966835021973
loss is 1.049413800239563
loss is 1.1164944171905518
loss is 1.1545934677124023
loss is 1.0806951522827148
loss is 1.055169939994812
loss is 1.019749641418457
loss is 1.0907834768295288
loss is 1.0687750577926636
loss is 1.074198603630066
loss is 1.073213815689087
loss is 1.0508307218551636
loss is 0.9891289472579956
loss is 1.0523837804794312
loss is 1.097778081893921
loss is 1.038499355316162
loss is 1.0446921586990356
loss is 1.0102224349975586
loss is 1.0847851037979126
loss is 1.0487264394760132
loss is 1.0911163091659546
loss is 0.9898965954780579
loss is 0.9917493462562561
loss is 0.9673612713813782
loss is 1.0238722562789917
loss is 0.9987490177154541
loss is 1.0271883010864258
loss is 1.0267399549484253
loss is 1.0079678297042847
loss is 1.0603301525115967
loss is 1.0478836297988892
loss is 0.9283223748207092
loss is 1.0821053981781006
loss is 1.0667928457260132
loss is 0.9382891654968262
loss is 0.9635389447212219
loss is 1.0652893781661987
loss is 1.139272928237915
loss is 1.0596845149993896
loss is 1.100364327430725
loss is 1.1172308921813965
loss is 1.0630335807800293
loss is 1.1420409679412842
loss is 1.0184621810913086
loss is 1.092698335647583
loss is 1.2437816858291626
loss is 1.1495332717895508
loss is 1.030921459197998
loss is 0.9818662405014038
loss is 1.068732500076294
loss is 1.0279231071472168
loss is 1.0928059816360474
loss is 1.0041449069976807
loss is 1.0399984121322632
loss is 1.0675028562545776
loss is 0.9771908521652222
loss is 1.040691614151001
loss is 0.9882960319519043
loss is 1.0958542823791504
loss is 1.078742504119873
loss is 0.9847365021705627
loss is 1.0722439289093018
loss is 1.1059411764144897
loss is 1.1676634550094604
loss is 1.0457037687301636
loss is 1.1186728477478027
loss is 0.9768968820571899
loss is 1.0170390605926514
loss is 1.0935941934585571
loss is 1.0701991319656372
loss is 1.0842500925064087
loss is 1.0038371086120605
loss is 1.0519572496414185
loss is 1.1705389022827148
loss is 1.1123356819152832
loss is 1.0557353496551514
loss is 1.027915596961975
loss is 1.028680682182312
loss is 1.1439703702926636
loss is 0.9914727210998535
loss is 1.0376687049865723
loss is 1.0894871950149536
loss is 1.0560128688812256
loss is 1.0566527843475342
epoch 20: train_loss = 1.049
20: {'Accuracy': 0.5591, 'Precision': 0.5731, 'Recall': 0.558, 'F1-score': 0.5583}
epoch: 21
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:31,  1.41it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:33, 11.12it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 20.43it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:12, 28.90it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 36.12it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 41.81it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 46.31it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 49.70it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 52.27it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 54.05it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 55.36it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 56.40it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.98it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 57.47it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.79it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 58.03it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 58.21it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 58.30it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 58.39it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 58.43it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 58.50it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.59it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.53it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 58.55it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.59it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:03, 58.55it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.51it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.47it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.43it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.50it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.48it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.49it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:03<00:03, 58.49it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.45it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.53it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.56it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.58it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.57it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.53it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.54it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.61it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.59it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 58.59it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.62it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.58it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.58it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 58.57it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.56it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.55it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.26it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.40it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 58.42it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.36it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.40it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 58.42it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 58.41it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.48it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 58.53it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.51it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.53it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.38it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:06<00:00, 57.91it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 58.17it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.37it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.62it/s]
loss is 1.0116171836853027
loss is 1.0278043746948242
loss is 1.014557123184204
loss is 0.9423884749412537
loss is 0.9806013703346252
loss is 0.9810609221458435
loss is 0.987079381942749
loss is 1.1820725202560425
loss is 1.0165247917175293
loss is 1.0022422075271606
loss is 0.9398669600486755
loss is 1.0894043445587158
loss is 0.8915271162986755
loss is 0.9918890595436096
loss is 1.0937377214431763
loss is 1.0184941291809082
loss is 0.9800912737846375
loss is 0.9528203010559082
loss is 0.965701699256897
loss is 1.0111316442489624
loss is 1.0009740591049194
loss is 0.9010058641433716
loss is 1.0773472785949707
loss is 1.112326979637146
loss is 1.0647324323654175
loss is 1.016057014465332
loss is 0.939744234085083
loss is 0.9739893078804016
loss is 0.9620927572250366
loss is 0.9480299949645996
loss is 0.9454687237739563
loss is 1.011012315750122
loss is 0.992746889591217
loss is 0.9648314714431763
loss is 0.9461272954940796
loss is 0.8940042853355408
loss is 0.9174043536186218
loss is 0.9995583891868591
loss is 1.0383198261260986
loss is 0.9547278881072998
loss is 1.0549017190933228
loss is 0.9835206866264343
loss is 1.0226938724517822
loss is 1.0347081422805786
loss is 1.0259242057800293
loss is 0.9328332543373108
loss is 0.9962848424911499
loss is 1.0070126056671143
loss is 1.0329582691192627
loss is 0.8545129895210266
loss is 1.0643644332885742
loss is 1.01870858669281
loss is 0.8939664959907532
loss is 0.9893964529037476
loss is 1.0487332344055176
loss is 0.9394081830978394
loss is 1.1063871383666992
loss is 0.9718820452690125
loss is 1.0219883918762207
loss is 0.9429466128349304
loss is 1.1170463562011719
loss is 1.0524647235870361
loss is 0.9458184838294983
loss is 1.1226584911346436
loss is 0.9864545464515686
loss is 0.9346866011619568
loss is 0.9803481101989746
loss is 1.0194084644317627
loss is 0.9638429880142212
loss is 0.9872512221336365
loss is 1.0246502161026
loss is 0.9612598419189453
loss is 1.1619130373001099
loss is 0.9765934944152832
loss is 0.963186502456665
loss is 0.9285277724266052
loss is 1.032354712486267
loss is 1.0317243337631226
loss is 0.9581202864646912
loss is 1.0362982749938965
loss is 0.9784695506095886
loss is 1.0357167720794678
loss is 0.9547762870788574
loss is 0.9235490560531616
loss is 1.0941394567489624
loss is 0.9963224530220032
loss is 1.1141804456710815
loss is 0.9927383661270142
loss is 0.986844003200531
loss is 0.9187142848968506
loss is 1.093368649482727
loss is 1.0441519021987915
loss is 0.9229803681373596
loss is 1.0958905220031738
loss is 1.0482118129730225
loss is 0.9592761397361755
loss is 0.9101104140281677
loss is 1.0725547075271606
loss is 0.898558497428894
loss is 1.0411604642868042
loss is 1.0750740766525269
loss is 0.9311182498931885
loss is 1.0237103700637817
loss is 0.9946438074111938
loss is 1.048938274383545
loss is 0.9996075630187988
loss is 1.026016354560852
loss is 0.9213215112686157
loss is 0.9511851668357849
loss is 0.9912400841712952
loss is 1.0108073949813843
loss is 1.0304961204528809
loss is 1.0376007556915283
loss is 1.0443055629730225
loss is 1.049943208694458
loss is 0.963716983795166
loss is 1.0061873197555542
loss is 1.056602120399475
loss is 1.0036139488220215
loss is 1.0808322429656982
loss is 0.9694655537605286
loss is 1.0357078313827515
loss is 1.0016225576400757
loss is 1.1244598627090454
loss is 1.124564528465271
loss is 1.1304855346679688
loss is 0.981026291847229
loss is 0.9181492328643799
loss is 1.0091053247451782
loss is 1.1277292966842651
loss is 0.8862568140029907
loss is 0.8920152187347412
loss is 1.0451099872589111
loss is 1.0072009563446045
loss is 1.0330169200897217
loss is 0.991710364818573
loss is 1.0160136222839355
loss is 0.9681934118270874
loss is 0.9922099113464355
loss is 0.9511881470680237
loss is 1.062741756439209
loss is 1.030674934387207
loss is 1.1096868515014648
loss is 1.1317994594573975
loss is 1.1018623113632202
loss is 1.0718324184417725
loss is 1.059404969215393
loss is 0.8992740511894226
loss is 1.0736701488494873
loss is 1.124234676361084
loss is 0.9420288801193237
loss is 1.0415641069412231
loss is 1.0344330072402954
loss is 0.9788566827774048
loss is 1.1298270225524902
loss is 1.0115292072296143
loss is 0.9860962629318237
loss is 1.0047025680541992
loss is 0.9674553871154785
loss is 0.9139946699142456
loss is 1.0008893013000488
loss is 1.014686107635498
loss is 0.8577983379364014
loss is 1.0638384819030762
loss is 1.0064411163330078
loss is 0.9691570401191711
loss is 1.176851749420166
loss is 1.1771762371063232
loss is 1.004704475402832
loss is 1.0593115091323853
loss is 1.1008166074752808
loss is 1.0181657075881958
loss is 1.0391749143600464
loss is 1.0979502201080322
loss is 1.0526970624923706
loss is 1.0215095281600952
loss is 0.983046293258667
loss is 1.0572797060012817
loss is 0.9455357193946838
loss is 1.0412604808807373
loss is 1.1019079685211182
loss is 1.0626813173294067
loss is 1.0740957260131836
loss is 1.0868289470672607
loss is 1.0959869623184204
loss is 0.907975435256958
loss is 0.9797574877738953
loss is 0.9261171221733093
loss is 1.015758752822876
loss is 1.0840938091278076
loss is 0.9437417387962341
loss is 1.062209129333496
loss is 1.044163703918457
loss is 0.9868218898773193
loss is 1.1222126483917236
loss is 1.0463694334030151
loss is 0.9764083623886108
loss is 0.9826194047927856
loss is 1.0948792695999146
loss is 0.9976803064346313
loss is 1.0148415565490723
loss is 1.1104334592819214
loss is 1.1235636472702026
loss is 1.055741548538208
loss is 1.032848834991455
loss is 1.0600504875183105
loss is 0.9817902445793152
loss is 1.0523443222045898
loss is 1.0533320903778076
loss is 1.104025959968567
loss is 0.9474310278892517
loss is 1.033950924873352
loss is 1.0694106817245483
loss is 1.0464699268341064
loss is 1.052611231803894
loss is 1.0219807624816895
loss is 1.0824246406555176
loss is 1.137916088104248
loss is 1.0585781335830688
loss is 1.025098443031311
loss is 1.0358996391296387
loss is 1.1334151029586792
loss is 1.1454386711120605
loss is 1.062078833580017
loss is 0.9779493808746338
loss is 1.0129296779632568
loss is 1.0007814168930054
loss is 0.9736327528953552
loss is 1.0501375198364258
loss is 0.9734083414077759
loss is 1.09453547000885
loss is 1.1453495025634766
loss is 0.9987531900405884
loss is 1.0423974990844727
loss is 1.0037537813186646
loss is 0.9159009456634521
loss is 0.9862474203109741
loss is 0.8793015480041504
loss is 0.9958646297454834
loss is 1.0551997423171997
loss is 1.09491765499115
loss is 0.9917157888412476
loss is 0.9550189971923828
loss is 0.8990729451179504
loss is 0.969279944896698
loss is 1.0466281175613403
loss is 1.039561152458191
loss is 1.0586515665054321
loss is 1.1552597284317017
loss is 1.0616954565048218
loss is 1.0371108055114746
loss is 1.0297033786773682
loss is 1.0961378812789917
loss is 1.011096477508545
loss is 0.9559272527694702
loss is 1.0849634408950806
loss is 1.0141078233718872
loss is 1.0707913637161255
loss is 0.925250232219696
loss is 1.0031673908233643
loss is 1.1157742738723755
loss is 1.1972081661224365
loss is 1.0369945764541626
loss is 1.170911431312561
loss is 1.0184141397476196
loss is 1.0422085523605347
loss is 1.0301928520202637
loss is 1.0244057178497314
loss is 1.0167176723480225
loss is 1.0937341451644897
loss is 1.092616081237793
loss is 1.0610954761505127
loss is 1.0135842561721802
loss is 1.0431307554244995
loss is 0.9996687173843384
loss is 1.1342926025390625
loss is 0.9732561707496643
loss is 1.0336750745773315
loss is 0.9240533709526062
loss is 0.9314460754394531
loss is 1.0558360815048218
loss is 0.9663903713226318
loss is 1.0253366231918335
loss is 1.0694711208343506
loss is 1.0789556503295898
loss is 1.0157896280288696
loss is 1.0715445280075073
loss is 1.1030044555664062
loss is 1.0741462707519531
loss is 1.0435476303100586
loss is 0.9703072905540466
loss is 1.0216121673583984
loss is 0.9680570960044861
loss is 1.1398521661758423
loss is 1.1257596015930176
loss is 1.0099608898162842
loss is 1.026841640472412
loss is 1.085033893585205
loss is 0.9817711710929871
loss is 0.9610977172851562
loss is 0.9708811044692993
loss is 0.9550637006759644
loss is 0.8800195455551147
loss is 1.0314116477966309
loss is 1.0277339220046997
loss is 1.148094892501831
loss is 1.061591625213623
loss is 0.9955466389656067
loss is 1.1516116857528687
loss is 1.165898323059082
loss is 1.2173235416412354
loss is 0.9387338757514954
loss is 1.0808113813400269
loss is 1.0376636981964111
loss is 1.061249017715454
loss is 1.0266742706298828
loss is 0.9929746985435486
loss is 0.95118248462677
loss is 1.093093991279602
loss is 0.8974395990371704
loss is 1.1258585453033447
loss is 1.1109082698822021
loss is 1.1239242553710938
loss is 0.9512915015220642
loss is 1.117161512374878
loss is 1.0922884941101074
loss is 0.9638296365737915
loss is 1.052586317062378
loss is 1.0430320501327515
loss is 0.9463722705841064
loss is 1.0707919597625732
loss is 1.0815708637237549
loss is 1.0195430517196655
loss is 0.9907130599021912
loss is 1.0535024404525757
loss is 0.9910085797309875
loss is 1.0944656133651733
loss is 0.99649977684021
loss is 1.066389799118042
loss is 0.9578115344047546
loss is 1.075242519378662
loss is 1.0794774293899536
loss is 0.9955035448074341
loss is 1.1152677536010742
loss is 1.1366662979125977
loss is 1.0190463066101074
loss is 1.0767754316329956
loss is 1.0328675508499146
loss is 1.0320183038711548
loss is 1.0580189228057861
loss is 1.0914119482040405
loss is 1.0900577306747437
loss is 0.9377330541610718
loss is 1.018330693244934
loss is 1.0223227739334106
loss is 1.095987319946289
loss is 1.0402497053146362
loss is 0.9806687831878662
loss is 0.9797607660293579
loss is 1.018223524093628
loss is 0.9958154559135437
loss is 1.1014659404754639
loss is 1.0680865049362183
loss is 1.0612868070602417
loss is 1.0139178037643433
loss is 1.1386913061141968
loss is 1.1769005060195923
loss is 1.0209025144577026
loss is 1.0402332544326782
loss is 1.0337060689926147
loss is 1.0439079999923706
loss is 1.0640251636505127
loss is 1.042852520942688
loss is 1.1004661321640015
loss is 1.1569629907608032
loss is 1.0909509658813477
loss is 1.021602988243103
loss is 0.9754610657691956
loss is 0.968051016330719
loss is 1.1168495416641235
loss is 0.9239657521247864
loss is 1.0523059368133545
loss is 1.0075428485870361
loss is 0.9736458659172058
epoch 21: train_loss = 1.025
21: {'Accuracy': 0.5637, 'Precision': 0.5713, 'Recall': 0.5602, 'F1-score': 0.5612}
epoch: 22
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:16,  1.49it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:32, 11.60it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:17, 21.13it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:00<00:12, 29.59it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 36.68it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 42.16it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 46.43it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 49.62it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 49.11it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 51.57it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:06, 53.42it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 54.72it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 55.68it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.35it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 56.86it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.25it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 57.45it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.64it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.70it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.81it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.88it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 57.87it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 57.90it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 57.87it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 57.87it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 57.95it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 57.88it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 57.89it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 57.90it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 57.91it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 57.85it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 57.84it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 57.96it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 58.04it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.04it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.04it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 57.99it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.01it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 57.91it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.00it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 58.08it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.01it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 58.10it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 58.12it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 58.08it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 58.03it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 57.98it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 58.03it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 57.97it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 57.96it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 57.97it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 57.96it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.01it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 58.02it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 57.43it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 57.33it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 57.49it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 57.57it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 57.67it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 57.72it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 57.50it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 57.16it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 57.56it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 57.79it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.33it/s]
loss is 0.9433323740959167
loss is 1.0653012990951538
loss is 1.0639925003051758
loss is 0.9520760178565979
loss is 1.0541884899139404
loss is 1.0061137676239014
loss is 0.8940984606742859
loss is 1.0183757543563843
loss is 0.9156125783920288
loss is 0.9907013177871704
loss is 0.9571359753608704
loss is 0.9569859504699707
loss is 0.8568584322929382
loss is 1.0908852815628052
loss is 0.9082508683204651
loss is 0.9979633092880249
loss is 0.9547159075737
loss is 0.9555122256278992
loss is 0.9958534836769104
loss is 1.1386860609054565
loss is 0.9318056702613831
loss is 0.9911318421363831
loss is 1.048693060874939
loss is 1.145668387413025
loss is 0.9719251990318298
loss is 1.0043467283248901
loss is 1.034925103187561
loss is 0.9981924295425415
loss is 0.9626148343086243
loss is 1.1106135845184326
loss is 0.9690397381782532
loss is 0.9664099812507629
loss is 1.0396286249160767
loss is 1.0100665092468262
loss is 0.8969452381134033
loss is 1.0527451038360596
loss is 0.941925585269928
loss is 1.0615617036819458
loss is 0.9478046894073486
loss is 0.9668365716934204
loss is 1.0066684484481812
loss is 0.962879478931427
loss is 1.031693458557129
loss is 1.0805754661560059
loss is 0.9527825117111206
loss is 0.9882223010063171
loss is 0.9773951172828674
loss is 0.9379370808601379
loss is 0.9627548456192017
loss is 0.8795107007026672
loss is 1.100572109222412
loss is 1.0545734167099
loss is 0.9652183055877686
loss is 0.8690138459205627
loss is 0.9809990525245667
loss is 1.0179704427719116
loss is 0.8863239884376526
loss is 0.8832491040229797
loss is 1.0097240209579468
loss is 1.019372582435608
loss is 0.994148313999176
loss is 0.9115567803382874
loss is 1.0125377178192139
loss is 1.034072756767273
loss is 0.9616137146949768
loss is 1.0490248203277588
loss is 0.9206603765487671
loss is 0.9723316431045532
loss is 0.9688655138015747
loss is 0.9454923868179321
loss is 1.0112820863723755
loss is 0.9374237656593323
loss is 0.9492173790931702
loss is 0.9785234928131104
loss is 1.0663354396820068
loss is 0.9395366907119751
loss is 0.988176167011261
loss is 0.9829378724098206
loss is 1.0481946468353271
loss is 0.8737231492996216
loss is 1.0162724256515503
loss is 0.9616808891296387
loss is 1.0731350183486938
loss is 1.023526668548584
loss is 0.9818118214607239
loss is 0.9748320579528809
loss is 0.9295668601989746
loss is 1.0230046510696411
loss is 0.9182019829750061
loss is 1.0379618406295776
loss is 0.9733707904815674
loss is 1.111606240272522
loss is 1.0258392095565796
loss is 0.8879345655441284
loss is 1.0089868307113647
loss is 0.9440404772758484
loss is 0.9132241010665894
loss is 1.0523971319198608
loss is 1.0561119318008423
loss is 1.036514401435852
loss is 0.9776937961578369
loss is 0.9995362162590027
loss is 0.9982649087905884
loss is 1.0030514001846313
loss is 0.9707862138748169
loss is 0.8995833396911621
loss is 0.9989729523658752
loss is 0.9616681933403015
loss is 1.105458378791809
loss is 0.9403066039085388
loss is 0.8953266143798828
loss is 1.0894826650619507
loss is 0.9327458739280701
loss is 0.901863157749176
loss is 1.0878233909606934
loss is 0.9439424276351929
loss is 1.0060195922851562
loss is 1.0808981657028198
loss is 1.0282378196716309
loss is 1.073595643043518
loss is 0.930143415927887
loss is 0.9616239666938782
loss is 1.0239084959030151
loss is 1.0462852716445923
loss is 1.0034596920013428
loss is 0.9829782843589783
loss is 1.0246379375457764
loss is 0.9738388061523438
loss is 1.0702781677246094
loss is 1.0800681114196777
loss is 1.0366907119750977
loss is 1.099766492843628
loss is 0.994853675365448
loss is 0.960310161113739
loss is 1.0330116748809814
loss is 1.0872822999954224
loss is 0.995211124420166
loss is 0.9777674674987793
loss is 0.9627857804298401
loss is 0.9361549615859985
loss is 1.125461220741272
loss is 0.9672704339027405
loss is 0.9775839447975159
loss is 1.0615959167480469
loss is 0.9640693068504333
loss is 0.9545992016792297
loss is 0.8764054179191589
loss is 0.9819328188896179
loss is 0.9909108877182007
loss is 0.9845284819602966
loss is 1.0287524461746216
loss is 0.9577860832214355
loss is 0.9698801636695862
loss is 0.9145068526268005
loss is 1.0272908210754395
loss is 1.0134172439575195
loss is 1.018078327178955
loss is 1.0786916017532349
loss is 1.1118680238723755
loss is 0.9563413858413696
loss is 0.9549804925918579
loss is 1.031125545501709
loss is 0.9575258493423462
loss is 1.0622516870498657
loss is 0.9154902696609497
loss is 0.9210655093193054
loss is 0.9827873706817627
loss is 0.959869921207428
loss is 1.0175666809082031
loss is 1.0155346393585205
loss is 0.9369958639144897
loss is 1.0771033763885498
loss is 1.0073620080947876
loss is 1.0800905227661133
loss is 1.0351279973983765
loss is 1.0120553970336914
loss is 1.0545183420181274
loss is 1.0185790061950684
loss is 0.905710756778717
loss is 1.0048407316207886
loss is 1.0632878541946411
loss is 0.9246942400932312
loss is 0.9878932237625122
loss is 0.9826128482818604
loss is 0.9240858554840088
loss is 0.9947091937065125
loss is 1.0075923204421997
loss is 0.9835852980613708
loss is 1.093035101890564
loss is 1.0697827339172363
loss is 0.9795958995819092
loss is 1.082324504852295
loss is 0.962599515914917
loss is 0.9757537841796875
loss is 0.9804129004478455
loss is 1.0283616781234741
loss is 0.9292174577713013
loss is 1.0544209480285645
loss is 1.1532975435256958
loss is 1.051066279411316
loss is 1.0069106817245483
loss is 1.030787467956543
loss is 1.1159977912902832
loss is 1.0261341333389282
loss is 1.0229722261428833
loss is 0.9923102855682373
loss is 0.9279516339302063
loss is 0.958723247051239
loss is 1.019453763961792
loss is 1.1451895236968994
loss is 0.9895519018173218
loss is 1.0461931228637695
loss is 0.9840744137763977
loss is 0.9770766496658325
loss is 1.0035746097564697
loss is 0.9699211120605469
loss is 1.039839267730713
loss is 0.8958759307861328
loss is 1.0754451751708984
loss is 1.0848684310913086
loss is 1.1292579174041748
loss is 1.039347529411316
loss is 1.0010044574737549
loss is 0.909761369228363
loss is 1.067560076713562
loss is 1.0169885158538818
loss is 1.0475590229034424
loss is 1.1004362106323242
loss is 1.096126675605774
loss is 0.8862602114677429
loss is 0.9178611040115356
loss is 0.9844886660575867
loss is 0.9881794452667236
loss is 1.0689692497253418
loss is 1.0650951862335205
loss is 1.0183645486831665
loss is 1.0860910415649414
loss is 0.9843673706054688
loss is 1.0101027488708496
loss is 0.9541211128234863
loss is 0.9399368762969971
loss is 0.997224748134613
loss is 1.0963239669799805
loss is 1.015091896057129
loss is 1.0578608512878418
loss is 0.9517486095428467
loss is 0.9842592477798462
loss is 0.9938520193099976
loss is 0.9634233713150024
loss is 0.9748736023902893
loss is 0.9552074670791626
loss is 1.008022665977478
loss is 0.9700338244438171
loss is 1.1359649896621704
loss is 1.0097942352294922
loss is 1.008018970489502
loss is 1.0184935331344604
loss is 1.0693328380584717
loss is 0.9261537194252014
loss is 0.9387072920799255
loss is 1.0031752586364746
loss is 1.080420732498169
loss is 1.0071001052856445
loss is 0.9298031330108643
loss is 0.9477115869522095
loss is 1.0004565715789795
loss is 0.935921311378479
loss is 0.9457046389579773
loss is 0.9675807952880859
loss is 0.9974370002746582
loss is 0.9172723889350891
loss is 1.0563514232635498
loss is 1.1104786396026611
loss is 1.019840121269226
loss is 1.009276032447815
loss is 0.9978695511817932
loss is 0.9406033158302307
loss is 1.0246176719665527
loss is 1.1104254722595215
loss is 1.0182608366012573
loss is 1.0712218284606934
loss is 0.8774703741073608
loss is 0.99112868309021
loss is 1.0777390003204346
loss is 1.074264407157898
loss is 0.9781910181045532
loss is 1.0269131660461426
loss is 0.9382376670837402
loss is 1.0564974546432495
loss is 1.0282200574874878
loss is 0.9502007961273193
loss is 0.9585154056549072
loss is 0.9573666453361511
loss is 1.0345323085784912
loss is 1.0088258981704712
loss is 1.0145394802093506
loss is 1.184029459953308
loss is 0.9348841309547424
loss is 1.0356659889221191
loss is 0.9450467824935913
loss is 1.1900246143341064
loss is 0.969317615032196
loss is 1.001979947090149
loss is 1.0942213535308838
loss is 1.0789399147033691
loss is 0.9516323804855347
loss is 1.009516954421997
loss is 1.019282579421997
loss is 1.1043106317520142
loss is 1.0591576099395752
loss is 1.050892949104309
loss is 1.057073950767517
loss is 0.9773977398872375
loss is 1.0220752954483032
loss is 1.0786899328231812
loss is 1.1727492809295654
loss is 0.9816301465034485
loss is 1.0405783653259277
loss is 1.105615258216858
loss is 0.9815873503684998
loss is 1.0373741388320923
loss is 1.0311814546585083
loss is 1.0644632577896118
loss is 1.0787616968154907
loss is 1.049858808517456
loss is 1.0640459060668945
loss is 1.08821702003479
loss is 1.1232950687408447
loss is 1.057145118713379
loss is 0.9570659399032593
loss is 0.9368390440940857
loss is 1.0614246129989624
loss is 1.0100209712982178
loss is 0.98040771484375
loss is 0.8402154445648193
loss is 0.9638480544090271
loss is 0.9571372866630554
loss is 1.0391888618469238
loss is 1.0078997611999512
loss is 1.1058021783828735
loss is 1.0350747108459473
loss is 1.0432734489440918
loss is 0.9579834938049316
loss is 1.1288318634033203
loss is 0.9844302535057068
loss is 1.0311291217803955
loss is 1.0795172452926636
loss is 0.9736936688423157
loss is 0.9793447852134705
loss is 1.060341238975525
loss is 0.9223796725273132
loss is 1.1305553913116455
loss is 1.0194636583328247
loss is 1.0605822801589966
loss is 1.0845084190368652
loss is 0.9687621593475342
loss is 0.9689679741859436
loss is 0.9815807938575745
loss is 0.9509249329566956
loss is 0.9919335842132568
loss is 0.9431605339050293
loss is 1.0677123069763184
loss is 1.0365313291549683
loss is 1.0566027164459229
loss is 1.008741855621338
loss is 0.9657121300697327
loss is 0.9853537678718567
loss is 1.088618516921997
loss is 1.0448075532913208
loss is 0.9707993865013123
loss is 0.9643770456314087
loss is 1.0791374444961548
loss is 1.0801072120666504
loss is 1.039344310760498
loss is 0.9993976354598999
loss is 1.0285054445266724
loss is 0.9060245752334595
loss is 1.0402791500091553
loss is 1.0365967750549316
loss is 0.910075843334198
loss is 1.11629056930542
loss is 1.0529366731643677
loss is 1.0222231149673462
loss is 1.0171101093292236
epoch 22: train_loss = 1.006
22: {'Accuracy': 0.5554, 'Precision': 0.5704, 'Recall': 0.5525, 'F1-score': 0.5518}
epoch: 23
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:39,  1.37it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:34, 10.86it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 19.91it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:13, 28.07it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:10, 35.10it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 40.84it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 45.27it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:07, 48.66it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 51.15it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.10it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 54.41it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 55.37it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.00it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.51it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 56.84it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.00it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:05, 57.35it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.43it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.57it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.73it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.87it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 57.88it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:03<00:04, 57.88it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 57.98it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 57.99it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 57.96it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.03it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.11it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.13it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.08it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.10it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.14it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 57.94it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 57.96it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 57.94it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 57.96it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 57.95it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 57.95it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.02it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.02it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 57.99it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 58.07it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 58.08it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 57.96it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 57.89it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 57.81it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 57.95it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 57.95it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 58.04it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 58.01it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.00it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:06<00:01, 58.06it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 57.84it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 57.91it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 57.94it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 57.98it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 58.05it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 57.94it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 57.88it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 57.63it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 57.76it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 57.36it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 57.55it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 57.60it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.01it/s]
loss is 0.9803317785263062
loss is 0.9776777625083923
loss is 1.0451762676239014
loss is 0.9671304225921631
loss is 0.9466190934181213
loss is 0.8371361494064331
loss is 0.9832651019096375
loss is 0.9736791849136353
loss is 1.1432435512542725
loss is 0.9468740224838257
loss is 0.8561360239982605
loss is 0.9546360969543457
loss is 0.9658262133598328
loss is 0.8839153051376343
loss is 0.9808176159858704
loss is 0.9904899001121521
loss is 0.957240879535675
loss is 0.8522998094558716
loss is 0.9771971106529236
loss is 0.9228280782699585
loss is 0.9776635766029358
loss is 0.9988624453544617
loss is 1.0520873069763184
loss is 0.8806361556053162
loss is 0.9678317904472351
loss is 0.9965790510177612
loss is 1.049627661705017
loss is 1.056593418121338
loss is 0.932700514793396
loss is 1.0455008745193481
loss is 0.9567446708679199
loss is 0.9524637460708618
loss is 0.9477009773254395
loss is 0.972300112247467
loss is 0.9363966584205627
loss is 1.0015060901641846
loss is 1.0379836559295654
loss is 0.9016156792640686
loss is 0.9809662699699402
loss is 0.9125930070877075
loss is 1.0362056493759155
loss is 0.8783063292503357
loss is 0.9425853490829468
loss is 0.9523592591285706
loss is 0.9444955587387085
loss is 0.9803932905197144
loss is 1.0015931129455566
loss is 1.0386934280395508
loss is 0.9619821906089783
loss is 1.013602614402771
loss is 0.9681107997894287
loss is 0.9861156940460205
loss is 0.9384332299232483
loss is 1.0149515867233276
loss is 0.8822658061981201
loss is 0.9486881494522095
loss is 1.0214416980743408
loss is 0.9230003356933594
loss is 1.0019081830978394
loss is 0.9705516695976257
loss is 0.9956420660018921
loss is 0.9540330767631531
loss is 0.9958643317222595
loss is 1.0828648805618286
loss is 0.8698262572288513
loss is 1.1187727451324463
loss is 0.8982571363449097
loss is 0.8403491377830505
loss is 0.9774777889251709
loss is 0.9321885704994202
loss is 1.0264365673065186
loss is 0.8849272131919861
loss is 0.9251191020011902
loss is 0.8882889747619629
loss is 0.9871938824653625
loss is 0.9868878722190857
loss is 1.0021721124649048
loss is 0.9469636678695679
loss is 1.0239038467407227
loss is 1.019160509109497
loss is 0.9031023979187012
loss is 0.9517318606376648
loss is 0.9716148972511292
loss is 0.9835597276687622
loss is 0.9866599440574646
loss is 0.8962056040763855
loss is 0.9464642405509949
loss is 0.9473422169685364
loss is 0.9586591124534607
loss is 0.9588801860809326
loss is 0.9419513940811157
loss is 0.9017833471298218
loss is 0.8759082555770874
loss is 0.9452297687530518
loss is 0.9518819451332092
loss is 0.9878118634223938
loss is 0.8718715906143188
loss is 0.9455375075340271
loss is 0.9486923217773438
loss is 0.9937278628349304
loss is 0.9420866966247559
loss is 1.0203499794006348
loss is 0.8867666125297546
loss is 1.0925424098968506
loss is 0.8872378468513489
loss is 0.8839004039764404
loss is 0.9293148517608643
loss is 0.9759628772735596
loss is 0.9152067303657532
loss is 1.024743676185608
loss is 0.9474862813949585
loss is 0.9703215956687927
loss is 1.0281720161437988
loss is 0.9700042605400085
loss is 1.0818947553634644
loss is 0.9347020983695984
loss is 0.9056611657142639
loss is 1.06925630569458
loss is 0.9685747027397156
loss is 1.0323777198791504
loss is 0.9491679668426514
loss is 1.0841434001922607
loss is 0.9414899349212646
loss is 1.0252711772918701
loss is 0.9436231255531311
loss is 1.0288604497909546
loss is 1.0531678199768066
loss is 0.9694114923477173
loss is 1.1221517324447632
loss is 0.9000486731529236
loss is 0.9864251613616943
loss is 0.9355382323265076
loss is 1.026261329650879
loss is 0.9251447916030884
loss is 0.9024000763893127
loss is 1.055593729019165
loss is 0.9700775742530823
loss is 1.017694354057312
loss is 0.8800162672996521
loss is 0.9711249470710754
loss is 1.0620657205581665
loss is 0.9612886309623718
loss is 1.0730060338974
loss is 1.035892367362976
loss is 0.8883911371231079
loss is 0.9472243785858154
loss is 0.9282405972480774
loss is 1.018091082572937
loss is 1.0228049755096436
loss is 1.0135473012924194
loss is 0.8348712921142578
loss is 0.9233198761940002
loss is 0.9963781237602234
loss is 0.9903348684310913
loss is 0.9050506353378296
loss is 1.001965045928955
loss is 0.881874144077301
loss is 1.1627001762390137
loss is 0.9560516476631165
loss is 0.9436012506484985
loss is 0.9931529760360718
loss is 1.078117847442627
loss is 0.990119457244873
loss is 1.077989101409912
loss is 1.000494122505188
loss is 0.9934545755386353
loss is 0.9282006025314331
loss is 1.0112926959991455
loss is 0.9381383657455444
loss is 0.9625331163406372
loss is 0.9832785129547119
loss is 0.9912706613540649
loss is 0.991638720035553
loss is 0.9898731112480164
loss is 1.0033886432647705
loss is 0.9855419397354126
loss is 1.0257234573364258
loss is 1.094183325767517
loss is 1.024742603302002
loss is 0.8321495652198792
loss is 0.9945281744003296
loss is 0.9413536190986633
loss is 0.9259698987007141
loss is 0.9234908819198608
loss is 1.0685453414916992
loss is 1.0490632057189941
loss is 1.0271971225738525
loss is 0.8317838907241821
loss is 0.9927638173103333
loss is 1.1092604398727417
loss is 0.8818703293800354
loss is 0.991454541683197
loss is 1.0395066738128662
loss is 0.9383658766746521
loss is 0.8704724311828613
loss is 1.0068752765655518
loss is 0.9768080115318298
loss is 0.9502203464508057
loss is 0.9298379421234131
loss is 0.9606882333755493
loss is 0.9551292657852173
loss is 1.0147035121917725
loss is 1.0650882720947266
loss is 0.9548761248588562
loss is 1.006401777267456
loss is 0.9405671954154968
loss is 0.9153693914413452
loss is 1.0584832429885864
loss is 0.925920844078064
loss is 1.0503473281860352
loss is 1.0270172357559204
loss is 0.8985579609870911
loss is 1.0463348627090454
loss is 0.9797824621200562
loss is 0.9375331401824951
loss is 1.0560001134872437
loss is 1.1140633821487427
loss is 0.902759850025177
loss is 0.8973236083984375
loss is 0.8395584225654602
loss is 1.0031239986419678
loss is 0.9951523542404175
loss is 1.0761278867721558
loss is 0.8988430500030518
loss is 1.0915435552597046
loss is 1.013431429862976
loss is 0.9475045800209045
loss is 0.9729395508766174
loss is 1.1061370372772217
loss is 1.1529005765914917
loss is 0.9173343777656555
loss is 1.0043214559555054
loss is 1.1104650497436523
loss is 1.0438400506973267
loss is 1.0943864583969116
loss is 0.8891170024871826
loss is 0.9070920348167419
loss is 1.0355725288391113
loss is 1.0362212657928467
loss is 1.0391333103179932
loss is 1.1081873178482056
loss is 1.0122745037078857
loss is 0.9782012701034546
loss is 1.0447064638137817
loss is 1.0689749717712402
loss is 0.9889537692070007
loss is 0.9367384314537048
loss is 1.0580061674118042
loss is 0.9113673567771912
loss is 0.9980362057685852
loss is 0.9499073624610901
loss is 0.9328083992004395
loss is 1.0799027681350708
loss is 1.0247352123260498
loss is 1.0935642719268799
loss is 0.9972291588783264
loss is 1.007246971130371
loss is 0.9889484643936157
loss is 0.9324223399162292
loss is 0.9567832946777344
loss is 1.01943838596344
loss is 1.0687460899353027
loss is 1.088240623474121
loss is 0.9358185529708862
loss is 1.0585438013076782
loss is 0.9563114047050476
loss is 1.0450228452682495
loss is 0.9127342104911804
loss is 0.9691230654716492
loss is 0.9834228754043579
loss is 0.9810270667076111
loss is 0.9056410193443298
loss is 1.0084480047225952
loss is 1.001474142074585
loss is 0.9397273063659668
loss is 1.029941439628601
loss is 1.0922409296035767
loss is 0.938438355922699
loss is 1.007540225982666
loss is 1.0236194133758545
loss is 1.0186840295791626
loss is 0.9114248752593994
loss is 0.938738226890564
loss is 0.8326738476753235
loss is 0.9678903222084045
loss is 0.9698166847229004
loss is 0.9642102718353271
loss is 0.9313313364982605
loss is 1.0585482120513916
loss is 0.922088623046875
loss is 1.0435912609100342
loss is 1.1001876592636108
loss is 0.9472004771232605
loss is 1.0302907228469849
loss is 0.9741279482841492
loss is 0.996285080909729
loss is 0.963611364364624
loss is 0.9861757159233093
loss is 0.9550756216049194
loss is 0.981354296207428
loss is 1.0612146854400635
loss is 0.995896577835083
loss is 1.0569535493850708
loss is 1.1576858758926392
loss is 0.9691792130470276
loss is 0.9143939018249512
loss is 0.9821776747703552
loss is 0.9765424132347107
loss is 0.9744068384170532
loss is 1.0303207635879517
loss is 1.0024917125701904
loss is 1.013055682182312
loss is 1.0415558815002441
loss is 0.9389858245849609
loss is 1.1445881128311157
loss is 1.097047209739685
loss is 0.9414753913879395
loss is 0.9861152172088623
loss is 0.994438111782074
loss is 0.9688618183135986
loss is 0.9392057061195374
loss is 0.8802054524421692
loss is 1.003575325012207
loss is 1.0209909677505493
loss is 1.066880464553833
loss is 0.9355853796005249
loss is 1.0258299112319946
loss is 0.9732236266136169
loss is 1.0295376777648926
loss is 0.9691563248634338
loss is 0.9797137975692749
loss is 1.034896969795227
loss is 0.9478851556777954
loss is 0.9978233575820923
loss is 1.054848313331604
loss is 0.9961973428726196
loss is 1.0248976945877075
loss is 0.9536088705062866
loss is 1.094976544380188
loss is 0.9962663054466248
loss is 0.9473356604576111
loss is 0.9856201410293579
loss is 1.098471760749817
loss is 0.9865275025367737
loss is 0.8959399461746216
loss is 0.9733636975288391
loss is 0.9906361103057861
loss is 1.0171833038330078
loss is 1.0707175731658936
loss is 0.9903008937835693
loss is 1.0841418504714966
loss is 0.9732030630111694
loss is 0.9829373955726624
loss is 1.017580270767212
loss is 0.9810754656791687
loss is 0.9493625164031982
loss is 0.9764991998672485
loss is 1.0034289360046387
loss is 1.0302824974060059
loss is 1.1873860359191895
loss is 0.9614257216453552
loss is 1.0075112581253052
loss is 0.9759112596511841
loss is 0.9255673289299011
loss is 0.935726523399353
loss is 1.1270220279693604
loss is 0.9767056107521057
loss is 0.8213842511177063
loss is 1.0011703968048096
loss is 0.9366834163665771
loss is 1.049210786819458
loss is 1.0774990320205688
loss is 1.0432769060134888
loss is 0.9953725337982178
loss is 1.013075590133667
loss is 1.0276844501495361
loss is 0.9826335310935974
loss is 1.144201636314392
loss is 0.9221032857894897
loss is 0.9917528033256531
loss is 0.8734177350997925
loss is 0.9867033958435059
loss is 0.9596691727638245
loss is 1.1684956550598145
epoch 23: train_loss = 0.984
23: {'Accuracy': 0.5698, 'Precision': 0.5747, 'Recall': 0.566, 'F1-score': 0.5627}
epoch: 24
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<03:15,  1.96it/s]going through batches for holmes training:   1%|          | 2/384 [00:00<01:49,  3.48it/s]going through batches for holmes training:   2%|▏         | 6/384 [00:00<00:32, 11.66it/s]going through batches for holmes training:   3%|▎         | 12/384 [00:00<00:16, 23.19it/s]going through batches for holmes training:   5%|▍         | 18/384 [00:00<00:11, 32.54it/s]going through batches for holmes training:   6%|▋         | 24/384 [00:01<00:09, 39.48it/s]going through batches for holmes training:   8%|▊         | 30/384 [00:01<00:07, 44.53it/s]going through batches for holmes training:   9%|▉         | 36/384 [00:01<00:07, 48.31it/s]going through batches for holmes training:  11%|█         | 42/384 [00:01<00:06, 51.05it/s]going through batches for holmes training:  12%|█▎        | 48/384 [00:01<00:06, 53.01it/s]going through batches for holmes training:  14%|█▍        | 54/384 [00:01<00:06, 54.46it/s]going through batches for holmes training:  16%|█▌        | 60/384 [00:01<00:05, 55.23it/s]going through batches for holmes training:  17%|█▋        | 66/384 [00:01<00:05, 55.94it/s]going through batches for holmes training:  19%|█▉        | 72/384 [00:01<00:05, 56.52it/s]going through batches for holmes training:  20%|██        | 78/384 [00:01<00:05, 56.98it/s]going through batches for holmes training:  22%|██▏       | 84/384 [00:02<00:05, 57.27it/s]going through batches for holmes training:  23%|██▎       | 90/384 [00:02<00:05, 57.34it/s]going through batches for holmes training:  25%|██▌       | 96/384 [00:02<00:05, 57.45it/s]going through batches for holmes training:  27%|██▋       | 102/384 [00:02<00:04, 57.59it/s]going through batches for holmes training:  28%|██▊       | 108/384 [00:02<00:04, 57.70it/s]going through batches for holmes training:  30%|██▉       | 114/384 [00:02<00:04, 57.77it/s]going through batches for holmes training:  31%|███▏      | 120/384 [00:02<00:04, 57.82it/s]going through batches for holmes training:  33%|███▎      | 126/384 [00:02<00:04, 57.87it/s]going through batches for holmes training:  34%|███▍      | 132/384 [00:02<00:04, 57.90it/s]going through batches for holmes training:  36%|███▌      | 138/384 [00:03<00:04, 57.88it/s]going through batches for holmes training:  38%|███▊      | 144/384 [00:03<00:04, 57.88it/s]going through batches for holmes training:  39%|███▉      | 150/384 [00:03<00:04, 57.93it/s]going through batches for holmes training:  41%|████      | 156/384 [00:03<00:03, 57.87it/s]going through batches for holmes training:  42%|████▏     | 162/384 [00:03<00:03, 57.97it/s]going through batches for holmes training:  44%|████▍     | 168/384 [00:03<00:03, 57.96it/s]going through batches for holmes training:  45%|████▌     | 174/384 [00:03<00:03, 57.88it/s]going through batches for holmes training:  47%|████▋     | 180/384 [00:03<00:03, 57.85it/s]going through batches for holmes training:  48%|████▊     | 186/384 [00:03<00:03, 57.83it/s]going through batches for holmes training:  50%|█████     | 192/384 [00:03<00:03, 57.90it/s]going through batches for holmes training:  52%|█████▏    | 198/384 [00:04<00:03, 57.24it/s]going through batches for holmes training:  53%|█████▎    | 204/384 [00:04<00:03, 57.41it/s]going through batches for holmes training:  55%|█████▍    | 210/384 [00:04<00:03, 57.68it/s]going through batches for holmes training:  56%|█████▋    | 216/384 [00:04<00:02, 57.63it/s]going through batches for holmes training:  58%|█████▊    | 222/384 [00:04<00:02, 57.69it/s]going through batches for holmes training:  59%|█████▉    | 228/384 [00:04<00:02, 57.72it/s]going through batches for holmes training:  61%|██████    | 234/384 [00:04<00:02, 57.75it/s]going through batches for holmes training:  62%|██████▎   | 240/384 [00:04<00:02, 57.79it/s]going through batches for holmes training:  64%|██████▍   | 246/384 [00:04<00:02, 57.79it/s]going through batches for holmes training:  66%|██████▌   | 252/384 [00:05<00:02, 57.80it/s]going through batches for holmes training:  67%|██████▋   | 258/384 [00:05<00:02, 57.78it/s]going through batches for holmes training:  69%|██████▉   | 264/384 [00:05<00:02, 57.82it/s]going through batches for holmes training:  70%|███████   | 270/384 [00:05<00:01, 57.86it/s]going through batches for holmes training:  72%|███████▏  | 276/384 [00:05<00:01, 57.81it/s]going through batches for holmes training:  73%|███████▎  | 282/384 [00:05<00:01, 57.92it/s]going through batches for holmes training:  75%|███████▌  | 288/384 [00:05<00:01, 57.90it/s]going through batches for holmes training:  77%|███████▋  | 294/384 [00:05<00:01, 57.88it/s]going through batches for holmes training:  78%|███████▊  | 300/384 [00:05<00:01, 57.89it/s]going through batches for holmes training:  80%|███████▉  | 306/384 [00:05<00:01, 57.89it/s]going through batches for holmes training:  81%|████████▏ | 312/384 [00:06<00:01, 57.91it/s]going through batches for holmes training:  83%|████████▎ | 318/384 [00:06<00:01, 57.89it/s]going through batches for holmes training:  84%|████████▍ | 324/384 [00:06<00:01, 57.88it/s]going through batches for holmes training:  86%|████████▌ | 330/384 [00:06<00:00, 57.89it/s]going through batches for holmes training:  88%|████████▊ | 336/384 [00:06<00:00, 57.99it/s]going through batches for holmes training:  89%|████████▉ | 342/384 [00:06<00:00, 57.99it/s]going through batches for holmes training:  91%|█████████ | 348/384 [00:06<00:00, 58.02it/s]going through batches for holmes training:  92%|█████████▏| 354/384 [00:06<00:00, 57.93it/s]going through batches for holmes training:  94%|█████████▍| 360/384 [00:06<00:00, 57.96it/s]going through batches for holmes training:  95%|█████████▌| 366/384 [00:06<00:00, 57.47it/s]going through batches for holmes training:  97%|█████████▋| 372/384 [00:07<00:00, 57.68it/s]going through batches for holmes training:  98%|█████████▊| 378/384 [00:07<00:00, 57.85it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 57.91it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.46it/s]
loss is 0.9366039037704468
loss is 0.9039890766143799
loss is 0.9034947156906128
loss is 0.807150661945343
loss is 0.998460590839386
loss is 1.0545622110366821
loss is 1.0536245107650757
loss is 0.8710889220237732
loss is 0.8791000247001648
loss is 0.9799118041992188
loss is 0.8794236183166504
loss is 0.8359490036964417
loss is 0.9917047023773193
loss is 0.8837137818336487
loss is 1.082735538482666
loss is 0.9569704532623291
loss is 1.0014266967773438
loss is 1.0093097686767578
loss is 0.9302265644073486
loss is 0.9543588757514954
loss is 0.9756141901016235
loss is 0.9702568054199219
loss is 0.8577251434326172
loss is 0.9457163214683533
loss is 0.8952552676200867
loss is 0.8806896209716797
loss is 0.9293817281723022
loss is 1.0499123334884644
loss is 0.9257368445396423
loss is 0.9563179612159729
loss is 0.9201968312263489
loss is 0.8704144954681396
loss is 0.9651998281478882
loss is 0.9379470348358154
loss is 0.9631535410881042
loss is 1.0359859466552734
loss is 0.9265422821044922
loss is 0.943412184715271
loss is 1.0578112602233887
loss is 0.9705515503883362
loss is 0.9701756238937378
loss is 0.9245651364326477
loss is 0.8848333954811096
loss is 0.9916711449623108
loss is 0.8787539601325989
loss is 0.9277337789535522
loss is 0.9229481220245361
loss is 0.9083603620529175
loss is 0.877126932144165
loss is 0.9967063069343567
loss is 0.9547677636146545
loss is 0.8980453610420227
loss is 1.019051194190979
loss is 0.8938261270523071
loss is 0.9682895541191101
loss is 0.9640624523162842
loss is 1.062412142753601
loss is 0.9631545543670654
loss is 1.158142328262329
loss is 0.9372021555900574
loss is 0.9406155347824097
loss is 0.9905655384063721
loss is 0.9908990263938904
loss is 1.0152140855789185
loss is 0.9325803518295288
loss is 0.938707172870636
loss is 0.9293519854545593
loss is 0.9762904644012451
loss is 0.8001344799995422
loss is 0.889707088470459
loss is 0.9562132358551025
loss is 0.9590457081794739
loss is 0.9684586524963379
loss is 0.9860464334487915
loss is 0.8830490112304688
loss is 0.9182153344154358
loss is 0.8645321726799011
loss is 0.9795136451721191
loss is 0.9692551493644714
loss is 0.9281858801841736
loss is 1.0106810331344604
loss is 1.0268737077713013
loss is 0.9209070801734924
loss is 1.0168265104293823
loss is 0.9946128129959106
loss is 0.9421993494033813
loss is 1.0480575561523438
loss is 1.0226331949234009
loss is 0.8534867167472839
loss is 0.9228944182395935
loss is 0.8986813426017761
loss is 0.9585329294204712
loss is 0.9804767370223999
loss is 1.0432028770446777
loss is 0.9263894557952881
loss is 0.8748475909233093
loss is 0.9145383238792419
loss is 0.9948918223381042
loss is 0.9263213276863098
loss is 0.9099480509757996
loss is 0.9337761402130127
loss is 1.0610804557800293
loss is 1.0091941356658936
loss is 0.9745245575904846
loss is 0.9362981915473938
loss is 0.89060378074646
loss is 1.0164151191711426
loss is 0.9755406379699707
loss is 0.953319251537323
loss is 0.9980847239494324
loss is 0.9912068247795105
loss is 1.0059984922409058
loss is 0.9437654614448547
loss is 1.0240373611450195
loss is 0.9160548448562622
loss is 0.9774311780929565
loss is 0.9504004120826721
loss is 0.9967641234397888
loss is 0.926848828792572
loss is 0.9847109913825989
loss is 1.0315603017807007
loss is 1.0563974380493164
loss is 0.9777636528015137
loss is 1.070775032043457
loss is 1.0409690141677856
loss is 1.0166093111038208
loss is 1.1779643297195435
loss is 0.8948236703872681
loss is 0.9424015283584595
loss is 1.0012974739074707
loss is 0.9042249321937561
loss is 0.9869388341903687
loss is 0.9357943534851074
loss is 1.040966510772705
loss is 0.9134665131568909
loss is 0.8951506614685059
loss is 1.0135177373886108
loss is 1.1178605556488037
loss is 0.8795173168182373
loss is 0.9859344363212585
loss is 0.9033620953559875
loss is 0.8866239786148071
loss is 1.0307430028915405
loss is 0.8284974694252014
loss is 0.8713629841804504
loss is 0.9428898692131042
loss is 0.9873641729354858
loss is 0.9043750762939453
loss is 0.9961559176445007
loss is 0.9101895689964294
loss is 0.9837088584899902
loss is 1.022716999053955
loss is 0.8954657912254333
loss is 1.0410419702529907
loss is 0.9103794693946838
loss is 0.952913761138916
loss is 0.8980519771575928
loss is 1.0070312023162842
loss is 1.0080369710922241
loss is 0.919547975063324
loss is 0.9062402844429016
loss is 0.9465248584747314
loss is 0.8779997825622559
loss is 0.9195860028266907
loss is 1.0501813888549805
loss is 1.0812166929244995
loss is 0.8942146897315979
loss is 1.0126152038574219
loss is 0.8736312389373779
loss is 0.9561503529548645
loss is 1.0873363018035889
loss is 0.9112191796302795
loss is 0.989661455154419
loss is 0.9083519577980042
loss is 0.9053038954734802
loss is 0.9650580883026123
loss is 0.9626016020774841
loss is 0.8839778900146484
loss is 0.8997464776039124
loss is 0.9847216010093689
loss is 1.0572409629821777
loss is 0.9306179285049438
loss is 1.0250273942947388
loss is 1.0067883729934692
loss is 0.9303687810897827
loss is 0.9131531715393066
loss is 1.0317507982254028
loss is 1.049851655960083
loss is 0.9438166618347168
loss is 0.9172744750976562
loss is 0.9729116559028625
loss is 0.9896569848060608
loss is 1.0990550518035889
loss is 1.0483636856079102
loss is 1.017324447631836
loss is 0.9802103638648987
loss is 0.8411799669265747
loss is 0.9870550632476807
loss is 0.9285433888435364
loss is 0.9654785990715027
loss is 1.0629303455352783
loss is 1.0051381587982178
loss is 1.0739485025405884
loss is 0.9737024903297424
loss is 0.8837552666664124
loss is 0.8735609650611877
loss is 0.9643135070800781
loss is 0.9401782155036926
loss is 0.9617385268211365
loss is 0.9676145315170288
loss is 0.9196702837944031
loss is 1.0041903257369995
loss is 0.9617719054222107
loss is 1.0744411945343018
loss is 1.0199562311172485
loss is 0.9953417778015137
loss is 0.9682376384735107
loss is 0.9038941860198975
loss is 0.9700872302055359
loss is 0.9304725527763367
loss is 1.0266228914260864
loss is 0.8656501173973083
loss is 0.9868904948234558
loss is 0.9935736656188965
loss is 0.9316217303276062
loss is 0.9309859275817871
loss is 1.0411083698272705
loss is 0.9577760100364685
loss is 1.0052722692489624
loss is 1.0169788599014282
loss is 0.9539027214050293
loss is 0.9533645510673523
loss is 0.9760286211967468
loss is 0.9548853039741516
loss is 0.9833850860595703
loss is 0.9139780402183533
loss is 1.0044986009597778
loss is 0.9032149314880371
loss is 0.9185471534729004
loss is 1.0141087770462036
loss is 1.0636358261108398
loss is 0.9653522968292236
loss is 1.0695977210998535
loss is 1.0320090055465698
loss is 1.0148780345916748
loss is 0.9622859358787537
loss is 0.9628080129623413
loss is 0.9659764170646667
loss is 0.9994177222251892
loss is 1.0376592874526978
loss is 1.0340043306350708
loss is 1.0699212551116943
loss is 0.9880973100662231
loss is 1.049780011177063
loss is 0.9693029522895813
loss is 0.9770343899726868
loss is 0.9905133843421936
loss is 0.9333325028419495
loss is 0.8973334431648254
loss is 1.0626634359359741
loss is 0.8984055519104004
loss is 0.917267382144928
loss is 0.931257963180542
loss is 0.9481874108314514
loss is 1.0337105989456177
loss is 1.0273332595825195
loss is 0.9301499128341675
loss is 0.9055267572402954
loss is 0.9475005865097046
loss is 0.9389973282814026
loss is 1.0060137510299683
loss is 0.9543406963348389
loss is 1.0156145095825195
loss is 0.8491936326026917
loss is 0.980621337890625
loss is 0.9296669960021973
loss is 1.0120161771774292
loss is 1.0121456384658813
loss is 0.9163188934326172
loss is 1.0820525884628296
loss is 0.9030142426490784
loss is 1.0364136695861816
loss is 0.9944978356361389
loss is 1.0815128087997437
loss is 0.8660885691642761
loss is 0.9623779058456421
loss is 0.8849965929985046
loss is 1.030848503112793
loss is 0.8991701602935791
loss is 1.0471510887145996
loss is 0.9406269788742065
loss is 1.0640318393707275
loss is 1.0148204565048218
loss is 0.9407814741134644
loss is 0.9277258515357971
loss is 1.0941847562789917
loss is 1.0489712953567505
loss is 0.9558824896812439
loss is 0.9516603946685791
loss is 0.9882065653800964
loss is 0.9772390127182007
loss is 0.8469707369804382
loss is 0.8925526142120361
loss is 0.8514004349708557
loss is 0.9197891354560852
loss is 1.011884093284607
loss is 1.0512851476669312
loss is 0.988110363483429
loss is 0.9462642669677734
loss is 0.7930780053138733
loss is 1.0223357677459717
loss is 0.9456125497817993
loss is 0.9894515872001648
loss is 1.0532947778701782
loss is 0.9723966121673584
loss is 0.9698476195335388
loss is 0.8423818945884705
loss is 0.9283130764961243
loss is 0.8859007358551025
loss is 1.0743800401687622
loss is 0.8858442902565002
loss is 0.9624740481376648
loss is 1.0109353065490723
loss is 1.0726853609085083
loss is 0.9666361212730408
loss is 0.9269265532493591
loss is 0.9469223618507385
loss is 0.992764949798584
loss is 1.0195527076721191
loss is 0.9572648406028748
loss is 1.0381063222885132
loss is 0.9215559363365173
loss is 1.037914752960205
loss is 0.9828035235404968
loss is 0.9335318207740784
loss is 1.0567405223846436
loss is 0.9706989526748657
loss is 0.9188998937606812
loss is 0.9681720733642578
loss is 1.0648324489593506
loss is 0.9231510162353516
loss is 0.9046292304992676
loss is 1.01824152469635
loss is 1.1277621984481812
loss is 0.9687898755073547
loss is 0.9298474192619324
loss is 1.020641803741455
loss is 1.037621259689331
loss is 1.0109968185424805
loss is 0.9180523753166199
loss is 0.9124537110328674
loss is 0.8451295495033264
loss is 0.939995527267456
loss is 1.0056215524673462
loss is 0.9884722828865051
loss is 0.9803044199943542
loss is 0.8768427968025208
loss is 0.8929495215415955
loss is 0.9943738579750061
loss is 0.9727262258529663
loss is 0.9328344464302063
loss is 0.8309669494628906
loss is 1.0253829956054688
loss is 1.0451292991638184
loss is 0.9642985463142395
loss is 1.0559322834014893
loss is 0.9927112460136414
loss is 0.9495285749435425
loss is 0.8566888570785522
loss is 0.9980891942977905
loss is 0.8644750118255615
loss is 1.0437039136886597
loss is 0.9633051156997681
loss is 0.9612948894500732
loss is 0.9880309104919434
loss is 0.9562192559242249
loss is 0.9289631843566895
loss is 1.0346070528030396
loss is 0.8646665811538696
loss is 0.9423432350158691
loss is 0.8189688324928284
loss is 1.0685113668441772
loss is 0.9824882745742798
loss is 0.9764840602874756
epoch 24: train_loss = 0.965
24: {'Accuracy': 0.573, 'Precision': 0.5765, 'Recall': 0.5691, 'F1-score': 0.5664}
epoch: 25
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<05:15,  1.21it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:38,  9.74it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:01<00:20, 18.29it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:01<00:13, 26.43it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:10, 33.38it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 39.30it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 44.04it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:07, 47.71it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 50.57it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 52.54it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 54.08it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 55.18it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:02<00:05, 55.95it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.54it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 56.86it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.13it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:05, 57.30it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.44it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.59it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.66it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 57.65it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:03<00:04, 57.66it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:03<00:04, 57.71it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 57.80it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 57.73it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 57.72it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 57.79it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 57.76it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 57.77it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 57.72it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 57.80it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:04<00:03, 57.59it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:04<00:03, 57.57it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 57.70it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 57.71it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 57.70it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 57.76it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 57.75it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 57.80it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 57.84it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 57.83it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:05<00:02, 57.87it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 57.85it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 57.88it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 57.85it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 57.84it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 57.85it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 57.82it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 57.93it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 57.83it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:06<00:01, 57.83it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:06<00:01, 57.82it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 57.77it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 57.90it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 55.50it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 56.06it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 56.46it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 56.74it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 57.05it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 57.25it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:07<00:00, 57.42it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 57.14it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 57.41it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 57.69it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 51.17it/s]
loss is 0.8697463870048523
loss is 0.923240602016449
loss is 0.8730343580245972
loss is 0.8950307369232178
loss is 0.9641674757003784
loss is 0.9399542808532715
loss is 0.8676660656929016
loss is 0.9601970911026001
loss is 0.9471086859703064
loss is 0.9417180418968201
loss is 0.9719608426094055
loss is 0.8428145051002502
loss is 0.965004026889801
loss is 0.8342375755310059
loss is 0.8470152020454407
loss is 0.8062135577201843
loss is 0.9705379605293274
loss is 0.93403160572052
loss is 0.916318953037262
loss is 0.892910361289978
loss is 0.9538601040840149
loss is 0.8721379041671753
loss is 0.9309799671173096
loss is 0.9857940673828125
loss is 0.9174520969390869
loss is 0.9318448901176453
loss is 0.9003379940986633
loss is 0.8451987504959106
loss is 0.8932136297225952
loss is 0.89559006690979
loss is 0.8561105132102966
loss is 0.9652867913246155
loss is 0.9861695170402527
loss is 0.8950315713882446
loss is 0.9214474558830261
loss is 0.9607987403869629
loss is 0.9563905596733093
loss is 0.8914582133293152
loss is 1.003233551979065
loss is 0.8509441614151001
loss is 0.9633383750915527
loss is 0.9432664513587952
loss is 0.8306103348731995
loss is 0.8896186351776123
loss is 0.8811123371124268
loss is 0.8433098793029785
loss is 0.9114503264427185
loss is 0.9345365166664124
loss is 0.9433763027191162
loss is 1.0295047760009766
loss is 0.7962825894355774
loss is 0.9996965527534485
loss is 0.8314318656921387
loss is 1.1073153018951416
loss is 0.9556221961975098
loss is 0.869357705116272
loss is 0.9342000484466553
loss is 1.028735637664795
loss is 0.891452968120575
loss is 0.9882208108901978
loss is 0.8679834008216858
loss is 0.9867671728134155
loss is 0.7793538570404053
loss is 0.9434640407562256
loss is 1.0073884725570679
loss is 1.000746250152588
loss is 0.9342223405838013
loss is 0.8760090470314026
loss is 0.9687705039978027
loss is 0.9114315509796143
loss is 0.9604201316833496
loss is 0.8465282917022705
loss is 0.8929103016853333
loss is 0.9442344903945923
loss is 0.9730933904647827
loss is 0.8870924115180969
loss is 1.016871690750122
loss is 0.9799145460128784
loss is 0.9545130133628845
loss is 0.9719882011413574
loss is 0.9710811376571655
loss is 0.9290115833282471
loss is 0.9513643383979797
loss is 1.0215548276901245
loss is 0.9044955968856812
loss is 1.038464069366455
loss is 0.9936144351959229
loss is 0.9154151082038879
loss is 0.9265178442001343
loss is 0.8763152956962585
loss is 0.8978680372238159
loss is 0.9598305225372314
loss is 1.0152783393859863
loss is 0.9682156443595886
loss is 0.9925093650817871
loss is 1.0826983451843262
loss is 0.8242318630218506
loss is 0.9998456835746765
loss is 0.9238190650939941
loss is 0.9242265224456787
loss is 1.0642571449279785
loss is 0.9621869921684265
loss is 0.9800118803977966
loss is 0.951710045337677
loss is 1.064476728439331
loss is 0.9664327502250671
loss is 0.9461959004402161
loss is 0.9405239224433899
loss is 0.9076077938079834
loss is 0.972626268863678
loss is 0.9130160808563232
loss is 0.9229441285133362
loss is 0.8985878229141235
loss is 0.8370992541313171
loss is 0.9918378591537476
loss is 0.917704701423645
loss is 0.9511653184890747
loss is 0.9081026315689087
loss is 0.829201877117157
loss is 0.9823919534683228
loss is 0.9150962829589844
loss is 0.8838839530944824
loss is 0.9219272136688232
loss is 0.9128614068031311
loss is 0.9543250799179077
loss is 0.908937931060791
loss is 0.8073477745056152
loss is 0.9717755317687988
loss is 0.9687350392341614
loss is 0.930126428604126
loss is 0.9471167922019958
loss is 1.0062519311904907
loss is 0.9385880827903748
loss is 0.8935798406600952
loss is 0.9599827527999878
loss is 0.9687966704368591
loss is 0.9002344012260437
loss is 0.9923685193061829
loss is 0.9066044688224792
loss is 0.932239830493927
loss is 1.0600272417068481
loss is 0.9150199294090271
loss is 0.9646183252334595
loss is 0.9941115379333496
loss is 0.9282796382904053
loss is 0.9401587843894958
loss is 0.9635230898857117
loss is 0.9139469861984253
loss is 1.077284812927246
loss is 0.9310572743415833
loss is 1.035546898841858
loss is 0.9888445138931274
loss is 0.9712619185447693
loss is 0.8979678153991699
loss is 0.9584006667137146
loss is 0.9527831077575684
loss is 0.9188888072967529
loss is 0.9598059058189392
loss is 1.037509799003601
loss is 0.9812018871307373
loss is 1.0977637767791748
loss is 0.9584190845489502
loss is 0.8489181399345398
loss is 0.949084460735321
loss is 0.9368520975112915
loss is 0.9038222432136536
loss is 0.9448099732398987
loss is 0.9404776692390442
loss is 0.968711256980896
loss is 0.9321801662445068
loss is 1.0659219026565552
loss is 1.020440697669983
loss is 0.9856899976730347
loss is 1.0833371877670288
loss is 0.9525054097175598
loss is 0.885658323764801
loss is 0.906291663646698
loss is 0.9367342591285706
loss is 0.8672322034835815
loss is 0.9629560112953186
loss is 0.925940990447998
loss is 0.9645354747772217
loss is 0.9228851199150085
loss is 0.8813808560371399
loss is 0.8727096319198608
loss is 1.0200859308242798
loss is 0.9336946606636047
loss is 1.006221890449524
loss is 0.969607412815094
loss is 0.9396092295646667
loss is 0.9182316660881042
loss is 0.8423461318016052
loss is 0.9808222055435181
loss is 0.8946555256843567
loss is 0.8788339495658875
loss is 0.9063795208930969
loss is 0.8379136919975281
loss is 0.8959355354309082
loss is 0.9395111203193665
loss is 0.9253752827644348
loss is 1.0270003080368042
loss is 1.0008842945098877
loss is 0.941761314868927
loss is 0.9767431616783142
loss is 1.0351841449737549
loss is 0.9524238705635071
loss is 0.9267871379852295
loss is 1.0028702020645142
loss is 0.9263575673103333
loss is 0.9287387728691101
loss is 0.9277934432029724
loss is 1.007219910621643
loss is 0.8754109144210815
loss is 0.9579696655273438
loss is 1.0179980993270874
loss is 0.9638919234275818
loss is 0.9653151035308838
loss is 1.0359386205673218
loss is 0.9840450286865234
loss is 0.848405659198761
loss is 1.005110740661621
loss is 0.9207581281661987
loss is 0.9647710919380188
loss is 0.9371250867843628
loss is 1.008467197418213
loss is 0.9246128797531128
loss is 0.9877078533172607
loss is 0.9003507494926453
loss is 0.8608411550521851
loss is 0.9202516078948975
loss is 0.9964890480041504
loss is 1.008535385131836
loss is 0.9551047682762146
loss is 0.9981238842010498
loss is 0.9926355481147766
loss is 0.9471296668052673
loss is 1.032024621963501
loss is 0.8726457953453064
loss is 0.931627094745636
loss is 1.0565807819366455
loss is 1.0009361505508423
loss is 0.8359249234199524
loss is 0.951190173625946
loss is 1.0304316282272339
loss is 0.8971805572509766
loss is 0.9134877920150757
loss is 0.9121549725532532
loss is 1.058677077293396
loss is 0.8894168138504028
loss is 0.9564608931541443
loss is 0.9469209313392639
loss is 0.9676416516304016
loss is 1.066140055656433
loss is 0.9939766526222229
loss is 0.983214259147644
loss is 0.8169012665748596
loss is 0.9060083031654358
loss is 0.9093682765960693
loss is 0.9071681499481201
loss is 0.9325164556503296
loss is 0.8993386030197144
loss is 0.9866980910301208
loss is 1.0052403211593628
loss is 0.8432469367980957
loss is 0.9451621174812317
loss is 0.9257967472076416
loss is 0.9405752420425415
loss is 1.0491286516189575
loss is 0.9206280708312988
loss is 1.0277204513549805
loss is 0.9332166314125061
loss is 1.042533040046692
loss is 0.8599398732185364
loss is 0.9488980770111084
loss is 0.9410983920097351
loss is 0.9704148769378662
loss is 1.000247836112976
loss is 1.0355724096298218
loss is 1.0081244707107544
loss is 0.9774174690246582
loss is 0.952099084854126
loss is 0.9665987491607666
loss is 0.8874121308326721
loss is 0.9465510845184326
loss is 1.0071206092834473
loss is 0.9408913254737854
loss is 0.9326400756835938
loss is 0.8866326808929443
loss is 0.9249924421310425
loss is 0.9990426898002625
loss is 0.8367813229560852
loss is 0.9830962419509888
loss is 1.000144362449646
loss is 0.9414828419685364
loss is 0.941731870174408
loss is 1.051835298538208
loss is 0.9577301144599915
loss is 0.9529001712799072
loss is 0.8923807740211487
loss is 0.961361289024353
loss is 1.0098522901535034
loss is 1.0560543537139893
loss is 1.0124799013137817
loss is 0.8987480401992798
loss is 1.0198734998703003
loss is 0.8919860124588013
loss is 0.9322798848152161
loss is 0.8805883526802063
loss is 0.953467607498169
loss is 0.9280768036842346
loss is 1.1134001016616821
loss is 0.9754886627197266
loss is 1.0397236347198486
loss is 0.9522842168807983
loss is 0.9829870462417603
loss is 1.0350654125213623
loss is 0.9667127132415771
loss is 1.0726245641708374
loss is 1.0300836563110352
loss is 0.9150189757347107
loss is 0.9024115800857544
loss is 0.9939749240875244
loss is 0.8819167613983154
loss is 0.9440542459487915
loss is 1.017218828201294
loss is 1.032151222229004
loss is 1.0580968856811523
loss is 1.0099961757659912
loss is 1.0048027038574219
loss is 0.949114203453064
loss is 0.9306118488311768
loss is 0.8637060523033142
loss is 0.9671789407730103
loss is 0.9768434762954712
loss is 1.0414892435073853
loss is 0.9558969736099243
loss is 0.9514358043670654
loss is 0.8901217579841614
loss is 0.9136962294578552
loss is 0.9369750022888184
loss is 0.9525097608566284
loss is 1.0276240110397339
loss is 0.9520683884620667
loss is 0.892055332660675
loss is 0.9049658179283142
loss is 0.8962211012840271
loss is 0.929436206817627
loss is 1.0275362730026245
loss is 0.8812143206596375
loss is 0.9065642356872559
loss is 0.9572936296463013
loss is 0.8495646119117737
loss is 0.8338047862052917
loss is 0.9070938229560852
loss is 0.8193841576576233
loss is 0.9692695140838623
loss is 0.9610135555267334
loss is 1.031007170677185
loss is 0.9886422753334045
loss is 0.922105073928833
loss is 1.0278115272521973
loss is 1.1361687183380127
loss is 0.8139305710792542
loss is 0.9355791211128235
loss is 0.9998095035552979
loss is 0.9905604720115662
loss is 1.0072104930877686
loss is 0.9789900779724121
loss is 0.9396148920059204
loss is 0.9834365248680115
loss is 0.8848596215248108
loss is 0.9243090152740479
loss is 0.9598598480224609
loss is 0.9867592453956604
loss is 0.9186132550239563
loss is 0.9159268736839294
loss is 0.8991475701332092
loss is 0.9995788335800171
loss is 1.043472409248352
loss is 0.9126293063163757
loss is 0.9389505982398987
loss is 0.9733134508132935
loss is 0.8813923001289368
loss is 0.8766263723373413
epoch 25: train_loss = 0.947
25: {'Accuracy': 0.5729, 'Precision': 0.5813, 'Recall': 0.5724, 'F1-score': 0.5724}
epoch: 26
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:08,  1.54it/s]going through batches for holmes training:   1%|▏         | 5/384 [00:00<00:44,  8.44it/s]going through batches for holmes training:   3%|▎         | 11/384 [00:00<00:19, 18.85it/s]going through batches for holmes training:   5%|▍         | 18/384 [00:00<00:12, 29.27it/s]going through batches for holmes training:   6%|▋         | 24/384 [00:01<00:09, 36.51it/s]going through batches for holmes training:   8%|▊         | 30/384 [00:01<00:08, 42.01it/s]going through batches for holmes training:   9%|▉         | 36/384 [00:01<00:07, 46.32it/s]going through batches for holmes training:  11%|█         | 42/384 [00:01<00:06, 49.70it/s]going through batches for holmes training:  12%|█▎        | 48/384 [00:01<00:06, 52.20it/s]going through batches for holmes training:  14%|█▍        | 54/384 [00:01<00:06, 54.06it/s]going through batches for holmes training:  16%|█▌        | 60/384 [00:01<00:05, 55.36it/s]going through batches for holmes training:  17%|█▋        | 66/384 [00:01<00:05, 56.31it/s]going through batches for holmes training:  19%|█▉        | 72/384 [00:01<00:05, 57.02it/s]going through batches for holmes training:  20%|██        | 78/384 [00:01<00:05, 57.36it/s]going through batches for holmes training:  22%|██▏       | 84/384 [00:02<00:05, 57.75it/s]going through batches for holmes training:  23%|██▎       | 90/384 [00:02<00:05, 58.05it/s]going through batches for holmes training:  25%|██▌       | 96/384 [00:02<00:04, 58.22it/s]going through batches for holmes training:  27%|██▋       | 102/384 [00:02<00:04, 58.26it/s]going through batches for holmes training:  28%|██▊       | 108/384 [00:02<00:04, 58.38it/s]going through batches for holmes training:  30%|██▉       | 114/384 [00:02<00:04, 58.43it/s]going through batches for holmes training:  31%|███▏      | 120/384 [00:02<00:04, 58.45it/s]going through batches for holmes training:  33%|███▎      | 126/384 [00:02<00:04, 58.54it/s]going through batches for holmes training:  34%|███▍      | 132/384 [00:02<00:04, 58.56it/s]going through batches for holmes training:  36%|███▌      | 138/384 [00:03<00:04, 58.59it/s]going through batches for holmes training:  38%|███▊      | 144/384 [00:03<00:04, 57.40it/s]going through batches for holmes training:  39%|███▉      | 150/384 [00:03<00:04, 57.74it/s]going through batches for holmes training:  41%|████      | 156/384 [00:03<00:03, 57.85it/s]going through batches for holmes training:  42%|████▏     | 162/384 [00:03<00:03, 58.06it/s]going through batches for holmes training:  44%|████▍     | 168/384 [00:03<00:03, 58.13it/s]going through batches for holmes training:  45%|████▌     | 174/384 [00:03<00:03, 58.26it/s]going through batches for holmes training:  47%|████▋     | 180/384 [00:03<00:03, 58.35it/s]going through batches for holmes training:  48%|████▊     | 186/384 [00:03<00:03, 58.39it/s]going through batches for holmes training:  50%|█████     | 192/384 [00:03<00:03, 58.34it/s]going through batches for holmes training:  52%|█████▏    | 198/384 [00:04<00:03, 58.42it/s]going through batches for holmes training:  53%|█████▎    | 204/384 [00:04<00:03, 58.51it/s]going through batches for holmes training:  55%|█████▍    | 210/384 [00:04<00:02, 58.46it/s]going through batches for holmes training:  56%|█████▋    | 216/384 [00:04<00:02, 58.56it/s]going through batches for holmes training:  58%|█████▊    | 222/384 [00:04<00:02, 58.52it/s]going through batches for holmes training:  59%|█████▉    | 228/384 [00:04<00:02, 58.52it/s]going through batches for holmes training:  61%|██████    | 234/384 [00:04<00:02, 58.51it/s]going through batches for holmes training:  62%|██████▎   | 240/384 [00:04<00:02, 58.56it/s]going through batches for holmes training:  64%|██████▍   | 246/384 [00:04<00:02, 58.56it/s]going through batches for holmes training:  66%|██████▌   | 252/384 [00:04<00:02, 58.40it/s]going through batches for holmes training:  67%|██████▋   | 258/384 [00:05<00:02, 58.43it/s]going through batches for holmes training:  69%|██████▉   | 264/384 [00:05<00:02, 58.45it/s]going through batches for holmes training:  70%|███████   | 270/384 [00:05<00:01, 58.48it/s]going through batches for holmes training:  72%|███████▏  | 276/384 [00:05<00:01, 58.53it/s]going through batches for holmes training:  73%|███████▎  | 282/384 [00:05<00:01, 58.47it/s]going through batches for holmes training:  75%|███████▌  | 288/384 [00:05<00:01, 58.49it/s]going through batches for holmes training:  77%|███████▋  | 294/384 [00:05<00:01, 58.53it/s]going through batches for holmes training:  78%|███████▊  | 300/384 [00:05<00:01, 58.26it/s]going through batches for holmes training:  80%|███████▉  | 306/384 [00:05<00:01, 58.26it/s]going through batches for holmes training:  81%|████████▏ | 312/384 [00:06<00:01, 58.31it/s]going through batches for holmes training:  83%|████████▎ | 318/384 [00:06<00:01, 58.42it/s]going through batches for holmes training:  84%|████████▍ | 324/384 [00:06<00:01, 58.42it/s]going through batches for holmes training:  86%|████████▌ | 330/384 [00:06<00:00, 58.41it/s]going through batches for holmes training:  88%|████████▊ | 336/384 [00:06<00:00, 58.46it/s]going through batches for holmes training:  89%|████████▉ | 342/384 [00:06<00:00, 58.41it/s]going through batches for holmes training:  91%|█████████ | 348/384 [00:06<00:00, 58.45it/s]going through batches for holmes training:  92%|█████████▏| 354/384 [00:06<00:00, 58.40it/s]going through batches for holmes training:  94%|█████████▍| 360/384 [00:06<00:00, 58.35it/s]going through batches for holmes training:  95%|█████████▌| 366/384 [00:06<00:00, 57.98it/s]going through batches for holmes training:  97%|█████████▋| 372/384 [00:07<00:00, 58.24it/s]going through batches for holmes training:  98%|█████████▊| 378/384 [00:07<00:00, 58.40it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 58.53it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.74it/s]
loss is 0.8944514393806458
loss is 0.9175907373428345
loss is 0.8173829913139343
loss is 0.85263991355896
loss is 0.9624440670013428
loss is 0.8392493724822998
loss is 0.9039713144302368
loss is 0.9569190144538879
loss is 0.9117414355278015
loss is 0.8905301094055176
loss is 0.9221033453941345
loss is 0.8902811408042908
loss is 0.883446216583252
loss is 0.8096235394477844
loss is 0.9424262046813965
loss is 0.8798356652259827
loss is 0.8620932698249817
loss is 0.9121057987213135
loss is 0.8661245703697205
loss is 0.9153416156768799
loss is 0.9444561004638672
loss is 0.9713765978813171
loss is 0.884003221988678
loss is 0.9853008985519409
loss is 0.9480190873146057
loss is 0.8878906965255737
loss is 0.8147440552711487
loss is 0.7822909355163574
loss is 0.8075123429298401
loss is 0.8437045216560364
loss is 0.833631157875061
loss is 0.9556301832199097
loss is 0.7660596370697021
loss is 0.8588836193084717
loss is 1.0027331113815308
loss is 0.9846552014350891
loss is 0.8929389715194702
loss is 0.9301877617835999
loss is 0.9501219987869263
loss is 0.8067496418952942
loss is 0.9002023339271545
loss is 0.8218634128570557
loss is 0.8376008868217468
loss is 0.9828919768333435
loss is 0.8918320536613464
loss is 0.8202435970306396
loss is 0.9503223896026611
loss is 0.8567942976951599
loss is 0.8516590595245361
loss is 0.9543871879577637
loss is 0.9196515083312988
loss is 0.9069788455963135
loss is 0.9361723065376282
loss is 0.8790327310562134
loss is 0.9157018065452576
loss is 0.842313289642334
loss is 0.9791314601898193
loss is 0.9432068467140198
loss is 0.8820492029190063
loss is 0.9541939496994019
loss is 0.8802176117897034
loss is 0.9788178205490112
loss is 0.905629575252533
loss is 0.9042388796806335
loss is 1.008113145828247
loss is 0.8309648036956787
loss is 0.9115538597106934
loss is 0.9203289151191711
loss is 0.9630728363990784
loss is 0.8086207509040833
loss is 0.8967491984367371
loss is 0.840562641620636
loss is 0.9574558734893799
loss is 0.8876358866691589
loss is 0.9175006747245789
loss is 0.9288060069084167
loss is 0.9759466648101807
loss is 0.9233913421630859
loss is 0.8490105271339417
loss is 0.9591521620750427
loss is 0.8817219734191895
loss is 0.9809173345565796
loss is 0.9260040521621704
loss is 0.8861870765686035
loss is 0.8925595879554749
loss is 0.8485431671142578
loss is 1.0277355909347534
loss is 0.9322370290756226
loss is 0.901878833770752
loss is 0.9630900025367737
loss is 0.944166362285614
loss is 0.9492996335029602
loss is 0.8487477898597717
loss is 0.8663444519042969
loss is 0.9841319918632507
loss is 0.8872129321098328
loss is 0.9199421405792236
loss is 0.8737061023712158
loss is 0.9597007632255554
loss is 0.9894953966140747
loss is 0.9971386790275574
loss is 0.919981062412262
loss is 0.9330261945724487
loss is 0.9522736072540283
loss is 0.8728540539741516
loss is 0.9285641312599182
loss is 0.9145932793617249
loss is 0.936044454574585
loss is 1.0009289979934692
loss is 0.8923609256744385
loss is 0.9700124263763428
loss is 0.976203203201294
loss is 1.042127251625061
loss is 0.9122938513755798
loss is 0.9553072452545166
loss is 0.9457149505615234
loss is 0.9515843987464905
loss is 0.9608656167984009
loss is 0.9705369472503662
loss is 0.9190502166748047
loss is 0.8624168634414673
loss is 0.8313235640525818
loss is 0.9026986956596375
loss is 0.9464073777198792
loss is 0.8225249648094177
loss is 0.9624361991882324
loss is 0.8975784182548523
loss is 0.931002140045166
loss is 0.8705072999000549
loss is 0.8736433982849121
loss is 0.9580342769622803
loss is 1.0138347148895264
loss is 0.8762322068214417
loss is 0.9381890892982483
loss is 0.7827012538909912
loss is 0.8740372657775879
loss is 0.7751665711402893
loss is 1.0299255847930908
loss is 0.9802604913711548
loss is 0.9301755428314209
loss is 1.0608909130096436
loss is 1.0216176509857178
loss is 0.8813427686691284
loss is 0.9674197435379028
loss is 1.0115996599197388
loss is 0.8043856620788574
loss is 1.01222562789917
loss is 0.9059211611747742
loss is 0.8384761810302734
loss is 0.9540612697601318
loss is 0.9607025384902954
loss is 0.8889510631561279
loss is 0.8851754069328308
loss is 0.9343095421791077
loss is 0.8827500343322754
loss is 0.9204479455947876
loss is 1.0361237525939941
loss is 0.8909085988998413
loss is 0.8741665482521057
loss is 0.8936734795570374
loss is 0.9329618215560913
loss is 0.9941157698631287
loss is 0.8099541664123535
loss is 0.9991214871406555
loss is 0.909014105796814
loss is 0.9563803672790527
loss is 0.9361795783042908
loss is 1.0002778768539429
loss is 0.8969306349754333
loss is 0.9333152770996094
loss is 0.9549262523651123
loss is 0.9985094666481018
loss is 0.9442343711853027
loss is 0.8807804584503174
loss is 1.0046640634536743
loss is 0.8595445156097412
loss is 0.8083993792533875
loss is 0.9193075299263
loss is 1.0113749504089355
loss is 0.9604936838150024
loss is 0.96307373046875
loss is 0.817249596118927
loss is 0.8814297318458557
loss is 0.9839419722557068
loss is 0.8814260959625244
loss is 1.0093841552734375
loss is 0.8217721581459045
loss is 0.9093465209007263
loss is 0.9199259877204895
loss is 0.9711452722549438
loss is 0.8969227075576782
loss is 1.0125116109848022
loss is 0.8992760181427002
loss is 0.8819893598556519
loss is 1.0345203876495361
loss is 0.9277039170265198
loss is 0.9916397333145142
loss is 0.8321257829666138
loss is 0.8270595073699951
loss is 0.8848757743835449
loss is 0.8507506847381592
loss is 0.9603239297866821
loss is 0.9195497632026672
loss is 0.9836022257804871
loss is 0.9354294538497925
loss is 0.9853529930114746
loss is 0.9744876623153687
loss is 0.8434311747550964
loss is 0.9223092794418335
loss is 0.899515688419342
loss is 0.8863060474395752
loss is 0.98468017578125
loss is 0.9270046949386597
loss is 1.026138186454773
loss is 0.8975282907485962
loss is 0.8774626851081848
loss is 0.8663861155509949
loss is 0.9281619191169739
loss is 0.8917369246482849
loss is 0.9638878107070923
loss is 0.9816426038742065
loss is 0.9577186703681946
loss is 0.9651657342910767
loss is 0.8674801588058472
loss is 0.8568750023841858
loss is 0.9485632181167603
loss is 0.9961485266685486
loss is 1.0109955072402954
loss is 0.9922036528587341
loss is 0.9019960165023804
loss is 0.9603917002677917
loss is 1.0219272375106812
loss is 0.779589056968689
loss is 0.8476856350898743
loss is 0.9633379578590393
loss is 1.0073477029800415
loss is 0.9039413928985596
loss is 0.8817695379257202
loss is 0.8976655602455139
loss is 0.9524549245834351
loss is 0.9132107496261597
loss is 0.8795703053474426
loss is 1.034044861793518
loss is 0.9488961100578308
loss is 0.9422613382339478
loss is 1.0870156288146973
loss is 0.8970803618431091
loss is 0.9807788133621216
loss is 0.8887279033660889
loss is 0.8264691233634949
loss is 1.0191863775253296
loss is 0.9942436814308167
loss is 0.9208006262779236
loss is 0.9849103689193726
loss is 0.9353443384170532
loss is 1.0192193984985352
loss is 0.944122850894928
loss is 0.9094244241714478
loss is 0.9543888568878174
loss is 0.8880149126052856
loss is 0.8621006608009338
loss is 0.9407860040664673
loss is 1.038295865058899
loss is 0.9278857707977295
loss is 0.9872638583183289
loss is 0.9781635999679565
loss is 0.9527242183685303
loss is 0.9737447500228882
loss is 0.9090012907981873
loss is 0.8941843509674072
loss is 0.9336442351341248
loss is 0.9180148243904114
loss is 1.0035494565963745
loss is 0.9613906741142273
loss is 0.9001308679580688
loss is 0.9450059533119202
loss is 0.952200710773468
loss is 0.8582260012626648
loss is 0.8982483148574829
loss is 0.9790933728218079
loss is 0.8582727313041687
loss is 0.8022093176841736
loss is 0.9154349565505981
loss is 0.8406481146812439
loss is 0.8922368884086609
loss is 0.9673132300376892
loss is 0.903797447681427
loss is 0.9968029856681824
loss is 1.0121524333953857
loss is 0.9753150343894958
loss is 0.9562873840332031
loss is 0.8726115226745605
loss is 0.9204428791999817
loss is 0.9489004611968994
loss is 1.003238558769226
loss is 0.932819664478302
loss is 0.9396089315414429
loss is 0.9449011087417603
loss is 0.8361276984214783
loss is 0.9367769360542297
loss is 0.8740170001983643
loss is 0.8424018621444702
loss is 0.7858698964118958
loss is 0.9586736559867859
loss is 0.9842075109481812
loss is 0.9672532677650452
loss is 0.9146711826324463
loss is 1.0149785280227661
loss is 0.9365928173065186
loss is 0.9892535209655762
loss is 1.0372220277786255
loss is 0.8747097253799438
loss is 0.956399142742157
loss is 0.9831748008728027
loss is 0.9355370402336121
loss is 0.861232578754425
loss is 0.9310256242752075
loss is 0.9761926531791687
loss is 0.9359903931617737
loss is 0.8448801636695862
loss is 0.848764955997467
loss is 0.9481095671653748
loss is 0.9377665519714355
loss is 0.9996777176856995
loss is 0.994140088558197
loss is 0.9793401956558228
loss is 0.8307737708091736
loss is 0.8434587717056274
loss is 0.9378069043159485
loss is 0.9195210337638855
loss is 1.0126289129257202
loss is 0.9573208093643188
loss is 0.9389509558677673
loss is 0.9492969512939453
loss is 1.003647804260254
loss is 0.9500119090080261
loss is 0.9623083472251892
loss is 0.8964889049530029
loss is 0.9349239468574524
loss is 0.9049748182296753
loss is 1.0306636095046997
loss is 1.0048465728759766
loss is 0.9943032264709473
loss is 0.9282852411270142
loss is 0.8547767400741577
loss is 0.8375681042671204
loss is 0.9449948072433472
loss is 0.9793441891670227
loss is 0.9705691337585449
loss is 1.008558750152588
loss is 0.9581449627876282
loss is 0.8457013964653015
loss is 0.8709578514099121
loss is 0.9564756751060486
loss is 0.9353111386299133
loss is 0.9660621881484985
loss is 1.0647382736206055
loss is 0.9833300709724426
loss is 1.0001205205917358
loss is 0.9271394610404968
loss is 1.0178937911987305
loss is 0.8761404156684875
loss is 1.0109001398086548
loss is 0.9530336260795593
loss is 0.9708996415138245
loss is 1.0017802715301514
loss is 1.0004316568374634
loss is 0.8813290596008301
loss is 0.9484617710113525
loss is 0.8930095434188843
loss is 0.9536219835281372
loss is 0.9601463079452515
loss is 0.8967177867889404
loss is 0.924832284450531
loss is 0.8934114575386047
loss is 0.9949373602867126
loss is 0.9220066070556641
loss is 1.0545810461044312
loss is 0.894320547580719
loss is 1.0139113664627075
loss is 0.9749304056167603
loss is 0.8916609287261963
loss is 0.9751899838447571
loss is 0.9606690406799316
epoch 26: train_loss = 0.927
26: {'Accuracy': 0.5634, 'Precision': 0.5695, 'Recall': 0.5595, 'F1-score': 0.5577}
epoch: 27
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:20,  1.47it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:33, 11.42it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:17, 20.84it/s]going through batches for holmes training:   5%|▍         | 19/384 [00:00<00:12, 29.33it/s]going through batches for holmes training:   7%|▋         | 25/384 [00:01<00:09, 36.42it/s]going through batches for holmes training:   8%|▊         | 31/384 [00:01<00:08, 41.97it/s]going through batches for holmes training:  10%|▉         | 37/384 [00:01<00:07, 46.30it/s]going through batches for holmes training:  11%|█         | 43/384 [00:01<00:06, 49.57it/s]going through batches for holmes training:  13%|█▎        | 49/384 [00:01<00:06, 51.94it/s]going through batches for holmes training:  14%|█▍        | 55/384 [00:01<00:06, 53.72it/s]going through batches for holmes training:  16%|█▌        | 61/384 [00:01<00:05, 54.88it/s]going through batches for holmes training:  17%|█▋        | 67/384 [00:01<00:05, 55.77it/s]going through batches for holmes training:  19%|█▉        | 73/384 [00:01<00:05, 56.47it/s]going through batches for holmes training:  21%|██        | 79/384 [00:02<00:05, 56.91it/s]going through batches for holmes training:  22%|██▏       | 85/384 [00:02<00:05, 57.21it/s]going through batches for holmes training:  24%|██▎       | 91/384 [00:02<00:05, 57.43it/s]going through batches for holmes training:  25%|██▌       | 97/384 [00:02<00:04, 57.64it/s]going through batches for holmes training:  27%|██▋       | 103/384 [00:02<00:04, 57.79it/s]going through batches for holmes training:  28%|██▊       | 109/384 [00:02<00:04, 57.91it/s]going through batches for holmes training:  30%|██▉       | 115/384 [00:02<00:04, 57.99it/s]going through batches for holmes training:  32%|███▏      | 121/384 [00:02<00:04, 58.06it/s]going through batches for holmes training:  33%|███▎      | 127/384 [00:02<00:04, 58.06it/s]going through batches for holmes training:  35%|███▍      | 133/384 [00:02<00:04, 58.00it/s]going through batches for holmes training:  36%|███▌      | 139/384 [00:03<00:04, 57.93it/s]going through batches for holmes training:  38%|███▊      | 145/384 [00:03<00:04, 58.04it/s]going through batches for holmes training:  39%|███▉      | 151/384 [00:03<00:04, 58.00it/s]going through batches for holmes training:  41%|████      | 157/384 [00:03<00:03, 58.00it/s]going through batches for holmes training:  42%|████▏     | 163/384 [00:03<00:03, 58.08it/s]going through batches for holmes training:  44%|████▍     | 169/384 [00:03<00:03, 58.09it/s]going through batches for holmes training:  46%|████▌     | 175/384 [00:03<00:03, 58.11it/s]going through batches for holmes training:  47%|████▋     | 181/384 [00:03<00:03, 58.10it/s]going through batches for holmes training:  49%|████▊     | 187/384 [00:03<00:03, 58.04it/s]going through batches for holmes training:  50%|█████     | 193/384 [00:03<00:03, 58.01it/s]going through batches for holmes training:  52%|█████▏    | 199/384 [00:04<00:03, 57.99it/s]going through batches for holmes training:  53%|█████▎    | 205/384 [00:04<00:03, 58.05it/s]going through batches for holmes training:  55%|█████▍    | 211/384 [00:04<00:02, 58.03it/s]going through batches for holmes training:  57%|█████▋    | 217/384 [00:04<00:02, 58.05it/s]going through batches for holmes training:  58%|█████▊    | 223/384 [00:04<00:02, 58.10it/s]going through batches for holmes training:  60%|█████▉    | 229/384 [00:04<00:02, 58.06it/s]going through batches for holmes training:  61%|██████    | 235/384 [00:04<00:02, 58.07it/s]going through batches for holmes training:  63%|██████▎   | 241/384 [00:04<00:02, 56.33it/s]going through batches for holmes training:  64%|██████▍   | 247/384 [00:04<00:02, 56.86it/s]going through batches for holmes training:  66%|██████▌   | 253/384 [00:05<00:02, 57.19it/s]going through batches for holmes training:  67%|██████▋   | 259/384 [00:05<00:02, 57.39it/s]going through batches for holmes training:  69%|██████▉   | 265/384 [00:05<00:02, 57.48it/s]going through batches for holmes training:  71%|███████   | 271/384 [00:05<00:01, 57.72it/s]going through batches for holmes training:  72%|███████▏  | 277/384 [00:05<00:01, 57.76it/s]going through batches for holmes training:  74%|███████▎  | 283/384 [00:05<00:01, 57.79it/s]going through batches for holmes training:  75%|███████▌  | 289/384 [00:05<00:01, 57.89it/s]going through batches for holmes training:  77%|███████▋  | 295/384 [00:05<00:01, 57.95it/s]going through batches for holmes training:  78%|███████▊  | 301/384 [00:05<00:01, 58.00it/s]going through batches for holmes training:  80%|███████▉  | 307/384 [00:05<00:01, 57.98it/s]going through batches for holmes training:  82%|████████▏ | 313/384 [00:06<00:01, 58.04it/s]going through batches for holmes training:  83%|████████▎ | 319/384 [00:06<00:01, 57.99it/s]going through batches for holmes training:  85%|████████▍ | 325/384 [00:06<00:01, 57.98it/s]going through batches for holmes training:  86%|████████▌ | 331/384 [00:06<00:00, 57.97it/s]going through batches for holmes training:  88%|████████▊ | 337/384 [00:06<00:00, 57.97it/s]going through batches for holmes training:  89%|████████▉ | 343/384 [00:06<00:00, 57.94it/s]going through batches for holmes training:  91%|█████████ | 349/384 [00:06<00:00, 58.01it/s]going through batches for holmes training:  92%|█████████▏| 355/384 [00:06<00:00, 58.01it/s]going through batches for holmes training:  94%|█████████▍| 361/384 [00:06<00:00, 58.04it/s]going through batches for holmes training:  96%|█████████▌| 367/384 [00:07<00:00, 57.68it/s]going through batches for holmes training:  97%|█████████▋| 373/384 [00:07<00:00, 57.89it/s]going through batches for holmes training:  99%|█████████▊| 379/384 [00:07<00:00, 58.04it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.35it/s]
loss is 0.9272745251655579
loss is 0.7704530954360962
loss is 0.7482972741127014
loss is 0.8366684913635254
loss is 0.9451831579208374
loss is 0.8305397033691406
loss is 0.7947365045547485
loss is 0.900631308555603
loss is 0.8993788957595825
loss is 0.7868973612785339
loss is 0.7607437372207642
loss is 0.9714223742485046
loss is 0.8894134759902954
loss is 0.9545647501945496
loss is 0.9549638628959656
loss is 0.9884289503097534
loss is 0.882988452911377
loss is 0.8533592224121094
loss is 0.8182304501533508
loss is 0.8304240703582764
loss is 0.9750680327415466
loss is 0.851895272731781
loss is 0.9003280401229858
loss is 0.9039624929428101
loss is 0.9559990167617798
loss is 0.9199435710906982
loss is 0.8090553283691406
loss is 0.9707562327384949
loss is 0.9129617214202881
loss is 0.8801280856132507
loss is 0.8507755994796753
loss is 1.0276297330856323
loss is 0.9202004075050354
loss is 0.9068406820297241
loss is 0.9100291728973389
loss is 0.8906482458114624
loss is 0.8320224285125732
loss is 0.8828517198562622
loss is 0.9416722655296326
loss is 0.9442710876464844
loss is 0.9065225720405579
loss is 0.865439772605896
loss is 0.9091663956642151
loss is 0.77826988697052
loss is 0.7881641983985901
loss is 0.8556470274925232
loss is 0.8820850253105164
loss is 0.9038625955581665
loss is 0.8955326080322266
loss is 0.9338363409042358
loss is 0.8889996409416199
loss is 1.0151944160461426
loss is 0.9658534526824951
loss is 0.8887510895729065
loss is 0.8408856987953186
loss is 0.924397349357605
loss is 0.9701968431472778
loss is 0.894012451171875
loss is 0.911777138710022
loss is 0.8560937643051147
loss is 0.8681746125221252
loss is 1.0237784385681152
loss is 0.922224760055542
loss is 0.7913366556167603
loss is 0.9941098093986511
loss is 0.9506757259368896
loss is 0.9259734153747559
loss is 0.8760552406311035
loss is 0.7647255063056946
loss is 0.9166930317878723
loss is 0.871999979019165
loss is 0.9593443274497986
loss is 0.9344954490661621
loss is 0.8691005706787109
loss is 0.8580347299575806
loss is 0.8576133847236633
loss is 0.8865947723388672
loss is 0.9578250050544739
loss is 0.8792122006416321
loss is 0.9179556369781494
loss is 0.8180433511734009
loss is 0.9803875088691711
loss is 0.9265055060386658
loss is 0.9089120626449585
loss is 0.9034930467605591
loss is 0.9480187296867371
loss is 0.8013608455657959
loss is 0.9010052680969238
loss is 0.9410602450370789
loss is 0.9883098602294922
loss is 0.9276120066642761
loss is 0.8230495452880859
loss is 0.9387853145599365
loss is 0.8998097777366638
loss is 0.9307090640068054
loss is 0.8635981678962708
loss is 0.8764838576316833
loss is 0.9014943838119507
loss is 0.8612367510795593
loss is 0.8791492581367493
loss is 0.9217497110366821
loss is 0.8073931932449341
loss is 0.8764200806617737
loss is 0.77971351146698
loss is 0.9869500994682312
loss is 0.9577482342720032
loss is 0.9065927863121033
loss is 0.9542528390884399
loss is 0.8865306377410889
loss is 0.8936048746109009
loss is 0.911276638507843
loss is 0.9363789558410645
loss is 0.949887752532959
loss is 0.8827309608459473
loss is 0.894224226474762
loss is 0.9019898772239685
loss is 0.9980436563491821
loss is 0.8128248453140259
loss is 0.8308876156806946
loss is 0.9217610359191895
loss is 0.9211126565933228
loss is 0.9157771468162537
loss is 0.8164067268371582
loss is 0.8143324255943298
loss is 0.9895282983779907
loss is 0.899911642074585
loss is 0.9028723835945129
loss is 0.9516363739967346
loss is 0.8235312104225159
loss is 0.9095421433448792
loss is 0.9042558073997498
loss is 0.8859691023826599
loss is 0.8971854448318481
loss is 0.8235462307929993
loss is 0.9472266435623169
loss is 1.0142486095428467
loss is 0.9504295587539673
loss is 0.9686903357505798
loss is 0.9203906059265137
loss is 0.9251777529716492
loss is 0.9494552612304688
loss is 1.0435981750488281
loss is 0.8948050737380981
loss is 0.8187727928161621
loss is 0.9552331566810608
loss is 0.8143462538719177
loss is 0.9264982342720032
loss is 0.9044790863990784
loss is 0.9882904291152954
loss is 0.9889291524887085
loss is 0.9831799268722534
loss is 0.8985629081726074
loss is 0.960493266582489
loss is 1.0157208442687988
loss is 1.0143418312072754
loss is 0.8560611009597778
loss is 1.0422862768173218
loss is 0.9795511364936829
loss is 0.9482383131980896
loss is 0.9830464720726013
loss is 0.8804778456687927
loss is 0.86127769947052
loss is 0.9584833383560181
loss is 0.9134804010391235
loss is 0.8845580220222473
loss is 0.9460209608078003
loss is 0.8790286779403687
loss is 0.9667211174964905
loss is 0.9125156998634338
loss is 0.9713268876075745
loss is 0.835432231426239
loss is 0.9424238801002502
loss is 0.8471892476081848
loss is 0.9641914367675781
loss is 0.9247509837150574
loss is 0.8405312299728394
loss is 0.8573839664459229
loss is 0.9682945013046265
loss is 1.0473129749298096
loss is 0.9816752672195435
loss is 0.9068276286125183
loss is 0.9067298769950867
loss is 0.8889500498771667
loss is 0.9234412312507629
loss is 0.9137660264968872
loss is 0.9927313923835754
loss is 0.8323374390602112
loss is 0.952582836151123
loss is 0.9314829111099243
loss is 0.8614550232887268
loss is 0.8928396105766296
loss is 0.8900017738342285
loss is 0.8401377201080322
loss is 0.9574378132820129
loss is 0.8776823282241821
loss is 0.9222900867462158
loss is 0.8464060425758362
loss is 0.9561535716056824
loss is 0.9789175987243652
loss is 0.8981247544288635
loss is 0.8297558426856995
loss is 0.9172755479812622
loss is 0.9155089855194092
loss is 0.8987854719161987
loss is 0.8931162357330322
loss is 0.7515064477920532
loss is 0.9398130178451538
loss is 1.1027636528015137
loss is 0.8944683074951172
loss is 0.7896709442138672
loss is 0.9327005743980408
loss is 0.8825117349624634
loss is 0.9474362134933472
loss is 0.9941697120666504
loss is 0.9801788926124573
loss is 0.93377685546875
loss is 0.915382981300354
loss is 0.845182478427887
loss is 0.9963231086730957
loss is 0.9714935421943665
loss is 0.9778270721435547
loss is 0.9356858134269714
loss is 1.0102198123931885
loss is 0.9355219006538391
loss is 0.91163170337677
loss is 0.9741175770759583
loss is 0.9291226267814636
loss is 0.8419797420501709
loss is 0.8818879723548889
loss is 0.9676421880722046
loss is 0.9050338864326477
loss is 0.9289597272872925
loss is 0.9621834754943848
loss is 1.0327255725860596
loss is 0.8920941948890686
loss is 0.8590593934059143
loss is 0.8537091612815857
loss is 0.8404425978660583
loss is 0.8974440097808838
loss is 0.8559069037437439
loss is 1.006595492362976
loss is 0.8588610887527466
loss is 0.742693305015564
loss is 0.8106608390808105
loss is 0.9920448064804077
loss is 0.9650303721427917
loss is 0.8749304413795471
loss is 1.0323134660720825
loss is 0.9851948022842407
loss is 0.9612343311309814
loss is 0.8552831411361694
loss is 1.055620789527893
loss is 0.9787428975105286
loss is 0.971469521522522
loss is 0.9871567487716675
loss is 0.9783433675765991
loss is 0.9444209933280945
loss is 0.9201167225837708
loss is 0.8985443115234375
loss is 1.0241460800170898
loss is 0.7944988012313843
loss is 0.8200963735580444
loss is 1.0621047019958496
loss is 1.0417046546936035
loss is 0.9022107720375061
loss is 0.8785293698310852
loss is 0.8028622269630432
loss is 0.9380923509597778
loss is 0.9204319715499878
loss is 0.9312458038330078
loss is 0.9535071849822998
loss is 0.9573631286621094
loss is 1.065065860748291
loss is 0.8867359161376953
loss is 0.9018341898918152
loss is 1.0881036520004272
loss is 0.9208948016166687
loss is 0.942993700504303
loss is 0.9192513227462769
loss is 0.9372872710227966
loss is 0.8776288032531738
loss is 0.8242911696434021
loss is 0.9197263121604919
loss is 1.0265443325042725
loss is 0.8912787437438965
loss is 0.8195403814315796
loss is 1.0645729303359985
loss is 0.9430348873138428
loss is 0.9210309386253357
loss is 0.9337563514709473
loss is 0.8930887579917908
loss is 0.8758995532989502
loss is 1.0197166204452515
loss is 0.880981981754303
loss is 0.8791801333427429
loss is 0.9224200248718262
loss is 0.9753074645996094
loss is 0.8816046118736267
loss is 0.9007518887519836
loss is 0.9793548583984375
loss is 0.9931349158287048
loss is 0.9019224643707275
loss is 0.9671260118484497
loss is 0.8749520182609558
loss is 0.9240052103996277
loss is 0.9121688008308411
loss is 0.9039280414581299
loss is 0.9116458892822266
loss is 0.8035275936126709
loss is 0.9002883434295654
loss is 0.8854131102561951
loss is 0.9572112560272217
loss is 0.9457327127456665
loss is 1.0917139053344727
loss is 0.8782307505607605
loss is 1.0139737129211426
loss is 0.9735509753227234
loss is 0.9243752956390381
loss is 0.9377564787864685
loss is 0.9356083869934082
loss is 0.8921651244163513
loss is 0.9506612420082092
loss is 0.8481351733207703
loss is 1.031465768814087
loss is 0.8324211835861206
loss is 0.9376424551010132
loss is 0.93626868724823
loss is 0.8165696859359741
loss is 0.9957476258277893
loss is 0.8641403317451477
loss is 0.9235038757324219
loss is 0.8641749024391174
loss is 0.9349958300590515
loss is 0.9644690752029419
loss is 0.8659672737121582
loss is 0.7805372476577759
loss is 0.8876484632492065
loss is 0.9878032803535461
loss is 1.0248202085494995
loss is 0.865329384803772
loss is 0.9388854503631592
loss is 0.9062560796737671
loss is 0.9536653757095337
loss is 0.8237901926040649
loss is 0.935874879360199
loss is 1.0228607654571533
loss is 0.9170124530792236
loss is 0.8326789736747742
loss is 0.870513379573822
loss is 0.9609337449073792
loss is 0.9609642028808594
loss is 0.954171359539032
loss is 0.9166210889816284
loss is 0.9240680932998657
loss is 0.8996075987815857
loss is 0.874544084072113
loss is 0.9068701267242432
loss is 0.9093565940856934
loss is 0.9572089910507202
loss is 0.8119019865989685
loss is 0.8869632482528687
loss is 0.9829628467559814
loss is 0.9412094354629517
loss is 0.8790504336357117
loss is 0.8658707141876221
loss is 0.9109333753585815
loss is 0.8319598436355591
loss is 0.9002203941345215
loss is 0.9633859395980835
loss is 0.8893430829048157
loss is 0.9022526741027832
loss is 0.8726487159729004
loss is 0.7949703931808472
loss is 0.8953481912612915
loss is 0.9085120558738708
loss is 0.9683010578155518
loss is 0.9597511887550354
loss is 0.9409281015396118
loss is 0.8755052089691162
loss is 0.8351731896400452
loss is 0.8431209325790405
loss is 0.8232746720314026
loss is 0.9659836292266846
loss is 0.8226378560066223
epoch 27: train_loss = 0.913
27: {'Accuracy': 0.5739, 'Precision': 0.5812, 'Recall': 0.5684, 'F1-score': 0.5678}
epoch: 28
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<03:55,  1.63it/s]going through batches for holmes training:   2%|▏         | 6/384 [00:00<00:35, 10.73it/s]going through batches for holmes training:   3%|▎         | 12/384 [00:00<00:17, 20.99it/s]going through batches for holmes training:   5%|▍         | 18/384 [00:00<00:12, 29.87it/s]going through batches for holmes training:   6%|▋         | 24/384 [00:01<00:09, 37.17it/s]going through batches for holmes training:   8%|▊         | 30/384 [00:01<00:08, 42.75it/s]going through batches for holmes training:   9%|▉         | 36/384 [00:01<00:07, 46.95it/s]going through batches for holmes training:  11%|█         | 42/384 [00:01<00:06, 50.08it/s]going through batches for holmes training:  12%|█▎        | 48/384 [00:01<00:06, 52.33it/s]going through batches for holmes training:  14%|█▍        | 54/384 [00:01<00:06, 53.95it/s]going through batches for holmes training:  16%|█▌        | 60/384 [00:01<00:05, 55.12it/s]going through batches for holmes training:  17%|█▋        | 66/384 [00:01<00:05, 55.94it/s]going through batches for holmes training:  19%|█▉        | 72/384 [00:01<00:05, 56.51it/s]going through batches for holmes training:  20%|██        | 78/384 [00:01<00:05, 56.94it/s]going through batches for holmes training:  22%|██▏       | 84/384 [00:02<00:05, 57.26it/s]going through batches for holmes training:  23%|██▎       | 90/384 [00:02<00:05, 57.48it/s]going through batches for holmes training:  25%|██▌       | 96/384 [00:02<00:04, 57.62it/s]going through batches for holmes training:  27%|██▋       | 102/384 [00:02<00:04, 57.63it/s]going through batches for holmes training:  28%|██▊       | 108/384 [00:02<00:04, 57.64it/s]going through batches for holmes training:  30%|██▉       | 114/384 [00:02<00:04, 57.80it/s]going through batches for holmes training:  31%|███▏      | 120/384 [00:02<00:04, 57.87it/s]going through batches for holmes training:  33%|███▎      | 126/384 [00:02<00:04, 57.89it/s]going through batches for holmes training:  34%|███▍      | 132/384 [00:02<00:04, 57.87it/s]going through batches for holmes training:  36%|███▌      | 138/384 [00:02<00:04, 57.94it/s]going through batches for holmes training:  38%|███▊      | 144/384 [00:03<00:04, 57.97it/s]going through batches for holmes training:  39%|███▉      | 150/384 [00:03<00:04, 57.67it/s]going through batches for holmes training:  41%|████      | 156/384 [00:03<00:03, 57.70it/s]going through batches for holmes training:  42%|████▏     | 162/384 [00:03<00:03, 57.78it/s]going through batches for holmes training:  44%|████▍     | 168/384 [00:03<00:03, 57.88it/s]going through batches for holmes training:  45%|████▌     | 174/384 [00:03<00:03, 57.84it/s]going through batches for holmes training:  47%|████▋     | 180/384 [00:03<00:03, 57.84it/s]going through batches for holmes training:  48%|████▊     | 186/384 [00:03<00:03, 57.90it/s]going through batches for holmes training:  50%|█████     | 192/384 [00:03<00:03, 57.90it/s]going through batches for holmes training:  52%|█████▏    | 198/384 [00:04<00:03, 57.84it/s]going through batches for holmes training:  53%|█████▎    | 204/384 [00:04<00:03, 57.84it/s]going through batches for holmes training:  55%|█████▍    | 210/384 [00:04<00:03, 57.84it/s]going through batches for holmes training:  56%|█████▋    | 216/384 [00:04<00:02, 57.85it/s]going through batches for holmes training:  58%|█████▊    | 222/384 [00:04<00:02, 57.82it/s]going through batches for holmes training:  59%|█████▉    | 228/384 [00:04<00:02, 57.94it/s]going through batches for holmes training:  61%|██████    | 234/384 [00:04<00:02, 57.95it/s]going through batches for holmes training:  62%|██████▎   | 240/384 [00:04<00:02, 57.88it/s]going through batches for holmes training:  64%|██████▍   | 246/384 [00:04<00:02, 57.92it/s]going through batches for holmes training:  66%|██████▌   | 252/384 [00:04<00:02, 57.88it/s]going through batches for holmes training:  67%|██████▋   | 258/384 [00:05<00:02, 57.89it/s]going through batches for holmes training:  69%|██████▉   | 264/384 [00:05<00:02, 57.83it/s]going through batches for holmes training:  70%|███████   | 270/384 [00:05<00:01, 57.87it/s]going through batches for holmes training:  72%|███████▏  | 276/384 [00:05<00:01, 57.89it/s]going through batches for holmes training:  73%|███████▎  | 282/384 [00:05<00:01, 57.73it/s]going through batches for holmes training:  75%|███████▌  | 288/384 [00:05<00:01, 57.76it/s]going through batches for holmes training:  77%|███████▋  | 294/384 [00:05<00:01, 57.79it/s]going through batches for holmes training:  78%|███████▊  | 300/384 [00:05<00:01, 57.82it/s]going through batches for holmes training:  80%|███████▉  | 306/384 [00:05<00:01, 57.74it/s]going through batches for holmes training:  81%|████████▏ | 312/384 [00:05<00:01, 57.82it/s]going through batches for holmes training:  83%|████████▎ | 318/384 [00:06<00:01, 57.94it/s]going through batches for holmes training:  84%|████████▍ | 324/384 [00:06<00:01, 57.86it/s]going through batches for holmes training:  86%|████████▌ | 330/384 [00:06<00:00, 57.82it/s]going through batches for holmes training:  88%|████████▊ | 336/384 [00:06<00:00, 57.74it/s]going through batches for holmes training:  89%|████████▉ | 342/384 [00:06<00:00, 57.74it/s]going through batches for holmes training:  91%|█████████ | 348/384 [00:06<00:00, 57.77it/s]going through batches for holmes training:  92%|█████████▏| 354/384 [00:06<00:00, 57.80it/s]going through batches for holmes training:  94%|█████████▍| 360/384 [00:06<00:00, 57.82it/s]going through batches for holmes training:  95%|█████████▌| 366/384 [00:06<00:00, 57.28it/s]going through batches for holmes training:  97%|█████████▋| 372/384 [00:07<00:00, 57.53it/s]going through batches for holmes training:  98%|█████████▊| 378/384 [00:07<00:00, 57.69it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 57.80it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.68it/s]
loss is 0.7714630961418152
loss is 0.8802450299263
loss is 0.874846875667572
loss is 0.7520943284034729
loss is 0.8243029713630676
loss is 0.9073396921157837
loss is 0.9644922614097595
loss is 0.8648768067359924
loss is 0.8980472087860107
loss is 0.9643044471740723
loss is 0.9035388231277466
loss is 0.9053733944892883
loss is 0.9691627621650696
loss is 0.9188114404678345
loss is 0.9381858706474304
loss is 0.9119890332221985
loss is 0.7401798963546753
loss is 0.811073362827301
loss is 0.8592881560325623
loss is 0.8388675451278687
loss is 0.8684346079826355
loss is 0.8682491183280945
loss is 0.8104190826416016
loss is 0.8044025301933289
loss is 0.7710748910903931
loss is 0.8046155571937561
loss is 0.8391368389129639
loss is 0.9179689288139343
loss is 0.8130943179130554
loss is 0.9006214141845703
loss is 0.8352517485618591
loss is 0.7907295227050781
loss is 0.9405432939529419
loss is 0.8587979078292847
loss is 0.992138683795929
loss is 0.8408994078636169
loss is 0.803946852684021
loss is 0.8436063528060913
loss is 0.8487676382064819
loss is 0.8461098670959473
loss is 0.8690524101257324
loss is 0.996704638004303
loss is 0.8129648566246033
loss is 0.844102680683136
loss is 0.8730957508087158
loss is 0.8425063490867615
loss is 0.8621216416358948
loss is 0.825633704662323
loss is 0.8020130395889282
loss is 0.8681449890136719
loss is 0.8644929528236389
loss is 0.8546527028083801
loss is 0.9129009246826172
loss is 0.8082185983657837
loss is 0.9190929532051086
loss is 0.843483567237854
loss is 1.010603427886963
loss is 0.992270290851593
loss is 0.7813192009925842
loss is 0.9289969801902771
loss is 0.8416491150856018
loss is 0.9368071556091309
loss is 0.8145891427993774
loss is 0.9547874927520752
loss is 0.8933185338973999
loss is 0.9312248229980469
loss is 0.8773210644721985
loss is 0.942172646522522
loss is 0.9452338218688965
loss is 0.9385421872138977
loss is 0.82207190990448
loss is 0.8808441162109375
loss is 0.8999117016792297
loss is 0.7941955327987671
loss is 1.0057158470153809
loss is 0.7810199856758118
loss is 0.9607081413269043
loss is 0.8803379535675049
loss is 0.7788597941398621
loss is 0.93255215883255
loss is 0.8799659013748169
loss is 0.9463987946510315
loss is 0.8595911264419556
loss is 0.8954508304595947
loss is 0.8163815140724182
loss is 0.8483412861824036
loss is 0.9644543528556824
loss is 0.9062116146087646
loss is 0.8867185711860657
loss is 0.8768429756164551
loss is 0.8589382767677307
loss is 0.8822488188743591
loss is 0.8632853031158447
loss is 0.9657984375953674
loss is 0.9673050045967102
loss is 0.844668984413147
loss is 0.8636904954910278
loss is 0.8275321125984192
loss is 0.8876528739929199
loss is 0.9401397109031677
loss is 0.7470866441726685
loss is 0.8571373820304871
loss is 0.9370278716087341
loss is 0.9231213331222534
loss is 1.021803617477417
loss is 0.8463428020477295
loss is 0.9244245886802673
loss is 0.8453186750411987
loss is 0.869066059589386
loss is 0.9181517958641052
loss is 0.8306024074554443
loss is 0.8773428201675415
loss is 0.8819921612739563
loss is 0.8787541389465332
loss is 0.8712097406387329
loss is 0.9631186127662659
loss is 0.998063862323761
loss is 0.8709743022918701
loss is 0.9271184802055359
loss is 0.9734326004981995
loss is 1.0584537982940674
loss is 0.8216095566749573
loss is 0.8901848793029785
loss is 0.8391274809837341
loss is 0.8890271186828613
loss is 0.9187980890274048
loss is 1.016137719154358
loss is 0.7862562537193298
loss is 0.8921212553977966
loss is 0.8628659248352051
loss is 0.8714926838874817
loss is 0.9473631381988525
loss is 0.9432408213615417
loss is 0.8792106509208679
loss is 0.9421714544296265
loss is 0.8466933369636536
loss is 0.8525279760360718
loss is 0.9234403967857361
loss is 1.0429729223251343
loss is 0.9094808101654053
loss is 0.8503673672676086
loss is 0.9189398884773254
loss is 0.8619358539581299
loss is 0.9385215640068054
loss is 0.8927042484283447
loss is 0.8801054954528809
loss is 0.908759355545044
loss is 0.943198025226593
loss is 0.9602544903755188
loss is 0.9186872243881226
loss is 0.9458490014076233
loss is 0.9605311751365662
loss is 0.8780531883239746
loss is 0.9378055334091187
loss is 0.9242913126945496
loss is 0.8789923191070557
loss is 0.9953495860099792
loss is 0.8913343548774719
loss is 0.9253637194633484
loss is 0.8495995998382568
loss is 0.8404156565666199
loss is 0.8814632296562195
loss is 0.8900623917579651
loss is 0.8606275320053101
loss is 0.7837020754814148
loss is 0.9087004065513611
loss is 0.8990544676780701
loss is 0.9215474128723145
loss is 0.814680278301239
loss is 0.9525312781333923
loss is 0.8742727041244507
loss is 0.8297061920166016
loss is 0.9307004809379578
loss is 0.8834344744682312
loss is 0.8091474175453186
loss is 0.98722243309021
loss is 0.9492403268814087
loss is 0.9190810322761536
loss is 0.8872096538543701
loss is 0.8723781108856201
loss is 0.8384965658187866
loss is 0.961474597454071
loss is 0.8370949625968933
loss is 0.9335330724716187
loss is 0.8102519512176514
loss is 0.9290814399719238
loss is 0.9706838130950928
loss is 0.8773136138916016
loss is 0.9043028354644775
loss is 0.918347954750061
loss is 0.9666217565536499
loss is 0.9245412945747375
loss is 0.9056455492973328
loss is 0.8403326272964478
loss is 0.8360881805419922
loss is 0.918266236782074
loss is 0.8588079214096069
loss is 0.91298508644104
loss is 0.8431553840637207
loss is 0.9003886580467224
loss is 0.9321545958518982
loss is 0.8095248341560364
loss is 1.0297340154647827
loss is 0.826708972454071
loss is 0.8660158514976501
loss is 0.8827601671218872
loss is 0.9252647161483765
loss is 0.8549829125404358
loss is 0.8909598588943481
loss is 0.9094505310058594
loss is 0.8830422759056091
loss is 0.940876841545105
loss is 0.8519988059997559
loss is 0.9633729457855225
loss is 0.9266687035560608
loss is 0.8681558966636658
loss is 0.8463259935379028
loss is 0.8510987758636475
loss is 0.8991716504096985
loss is 0.9109325408935547
loss is 1.011098861694336
loss is 0.973950982093811
loss is 0.938595175743103
loss is 0.9184531569480896
loss is 0.7597396373748779
loss is 0.9060993790626526
loss is 0.8659261465072632
loss is 0.767943263053894
loss is 0.9836151003837585
loss is 0.9027734994888306
loss is 0.8795182704925537
loss is 0.9799649119377136
loss is 0.9355931282043457
loss is 0.8377492427825928
loss is 0.9381923079490662
loss is 0.763709545135498
loss is 0.9421305656433105
loss is 0.7893928289413452
loss is 0.9305343627929688
loss is 0.9641503691673279
loss is 0.9807335138320923
loss is 0.9952430129051208
loss is 0.7916101217269897
loss is 0.894307017326355
loss is 0.9198245406150818
loss is 0.965353786945343
loss is 0.8888421654701233
loss is 0.8029322624206543
loss is 0.9461889863014221
loss is 0.9300177693367004
loss is 0.7465670704841614
loss is 0.8658995032310486
loss is 0.9437159895896912
loss is 0.9461107850074768
loss is 0.9494141340255737
loss is 0.8459091186523438
loss is 0.8162376880645752
loss is 0.9427554607391357
loss is 0.8859182596206665
loss is 0.8854203820228577
loss is 0.868125855922699
loss is 0.8660938739776611
loss is 0.9521446228027344
loss is 0.9460188150405884
loss is 0.9482632279396057
loss is 0.8968710899353027
loss is 0.928683340549469
loss is 0.8848547339439392
loss is 0.8138648867607117
loss is 0.8701473474502563
loss is 0.9602383375167847
loss is 0.9405329823493958
loss is 0.8801310658454895
loss is 1.037122130393982
loss is 0.9991037845611572
loss is 0.9501146078109741
loss is 0.9401999711990356
loss is 0.8328216671943665
loss is 0.9004108905792236
loss is 1.136151909828186
loss is 0.9143058061599731
loss is 0.9286975264549255
loss is 0.8628025054931641
loss is 0.89203941822052
loss is 0.9022854566574097
loss is 0.8653626441955566
loss is 0.8293232917785645
loss is 0.9440833330154419
loss is 1.0320615768432617
loss is 0.8129247426986694
loss is 0.9079065918922424
loss is 0.9069437980651855
loss is 0.8677043318748474
loss is 1.0133793354034424
loss is 0.9906572103500366
loss is 0.9032617807388306
loss is 0.918185830116272
loss is 0.857924222946167
loss is 0.8626452088356018
loss is 0.7733769416809082
loss is 0.9228363037109375
loss is 0.8565534353256226
loss is 0.8509238362312317
loss is 0.9398972988128662
loss is 0.8698882460594177
loss is 0.9593895077705383
loss is 0.955491304397583
loss is 0.9428847432136536
loss is 0.7941626906394958
loss is 0.9553497433662415
loss is 0.8916343450546265
loss is 0.9549674391746521
loss is 0.9179115295410156
loss is 0.7853503227233887
loss is 0.9761194586753845
loss is 0.8463937640190125
loss is 0.7985845804214478
loss is 0.7788113355636597
loss is 0.8971911072731018
loss is 0.8431653380393982
loss is 0.8923272490501404
loss is 0.8359246253967285
loss is 0.8781766295433044
loss is 0.8067043423652649
loss is 0.8181784749031067
loss is 0.9878840446472168
loss is 0.9425559043884277
loss is 0.9978237152099609
loss is 0.877298891544342
loss is 0.7801669836044312
loss is 0.9008643627166748
loss is 0.8689225912094116
loss is 0.9275078773498535
loss is 0.7963602542877197
loss is 0.8927839398384094
loss is 0.9190926551818848
loss is 0.9543423652648926
loss is 0.9026567339897156
loss is 0.8907027244567871
loss is 0.9576268196105957
loss is 0.8304124474525452
loss is 0.8930784463882446
loss is 1.082524299621582
loss is 0.8781189918518066
loss is 0.8625763654708862
loss is 0.9149052500724792
loss is 0.8274691700935364
loss is 0.9848114252090454
loss is 0.916440486907959
loss is 0.9886455535888672
loss is 0.9140474200248718
loss is 0.8292499780654907
loss is 0.8165351748466492
loss is 0.990465521812439
loss is 0.9575889110565186
loss is 1.001776099205017
loss is 0.9843050241470337
loss is 0.9393781423568726
loss is 0.8381574153900146
loss is 0.915896475315094
loss is 0.8896757364273071
loss is 0.9267150163650513
loss is 0.9332120418548584
loss is 1.0424262285232544
loss is 0.8060218095779419
loss is 0.9700857400894165
loss is 0.8867119550704956
loss is 0.9227115511894226
loss is 0.8465878367424011
loss is 0.8529984354972839
loss is 0.883273720741272
loss is 0.8847095370292664
loss is 0.8421388268470764
loss is 0.9491013884544373
loss is 0.9771838188171387
loss is 0.9096012115478516
loss is 0.9896451830863953
loss is 0.9309893250465393
loss is 0.8946064710617065
loss is 1.0290138721466064
loss is 0.7715034484863281
loss is 1.0305863618850708
loss is 0.8832486867904663
loss is 0.9371523857116699
epoch 28: train_loss = 0.895
28: {'Accuracy': 0.5746, 'Precision': 0.5805, 'Recall': 0.5747, 'F1-score': 0.5727}
epoch: 29
going through batches for holmes training:   0%|          | 0/384 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/384 [00:00<04:35,  1.39it/s]going through batches for holmes training:   2%|▏         | 7/384 [00:00<00:34, 10.91it/s]going through batches for holmes training:   3%|▎         | 13/384 [00:00<00:18, 20.15it/s]going through batches for holmes training:   5%|▌         | 20/384 [00:01<00:12, 29.71it/s]going through batches for holmes training:   7%|▋         | 26/384 [00:01<00:09, 36.40it/s]going through batches for holmes training:   8%|▊         | 32/384 [00:01<00:08, 41.84it/s]going through batches for holmes training:  10%|▉         | 38/384 [00:01<00:07, 46.17it/s]going through batches for holmes training:  11%|█▏        | 44/384 [00:01<00:06, 49.49it/s]going through batches for holmes training:  13%|█▎        | 50/384 [00:01<00:06, 51.99it/s]going through batches for holmes training:  15%|█▍        | 56/384 [00:01<00:06, 53.88it/s]going through batches for holmes training:  16%|█▌        | 62/384 [00:01<00:05, 55.26it/s]going through batches for holmes training:  18%|█▊        | 68/384 [00:01<00:05, 56.18it/s]going through batches for holmes training:  19%|█▉        | 74/384 [00:01<00:05, 56.87it/s]going through batches for holmes training:  21%|██        | 80/384 [00:02<00:05, 57.42it/s]going through batches for holmes training:  22%|██▏       | 86/384 [00:02<00:05, 57.71it/s]going through batches for holmes training:  24%|██▍       | 92/384 [00:02<00:05, 58.00it/s]going through batches for holmes training:  26%|██▌       | 98/384 [00:02<00:04, 58.22it/s]going through batches for holmes training:  27%|██▋       | 104/384 [00:02<00:04, 58.40it/s]going through batches for holmes training:  29%|██▊       | 110/384 [00:02<00:04, 58.50it/s]going through batches for holmes training:  30%|███       | 116/384 [00:02<00:04, 58.54it/s]going through batches for holmes training:  32%|███▏      | 122/384 [00:02<00:04, 58.54it/s]going through batches for holmes training:  33%|███▎      | 128/384 [00:02<00:04, 58.52it/s]going through batches for holmes training:  35%|███▍      | 134/384 [00:02<00:04, 58.61it/s]going through batches for holmes training:  36%|███▋      | 140/384 [00:03<00:04, 58.63it/s]going through batches for holmes training:  38%|███▊      | 146/384 [00:03<00:04, 58.58it/s]going through batches for holmes training:  40%|███▉      | 152/384 [00:03<00:04, 55.65it/s]going through batches for holmes training:  41%|████      | 158/384 [00:03<00:03, 56.57it/s]going through batches for holmes training:  43%|████▎     | 164/384 [00:03<00:03, 57.17it/s]going through batches for holmes training:  44%|████▍     | 170/384 [00:03<00:03, 57.62it/s]going through batches for holmes training:  46%|████▌     | 176/384 [00:03<00:03, 57.90it/s]going through batches for holmes training:  47%|████▋     | 182/384 [00:03<00:03, 58.18it/s]going through batches for holmes training:  49%|████▉     | 188/384 [00:03<00:03, 58.24it/s]going through batches for holmes training:  51%|█████     | 194/384 [00:04<00:03, 58.24it/s]going through batches for holmes training:  52%|█████▏    | 200/384 [00:04<00:03, 58.35it/s]going through batches for holmes training:  54%|█████▎    | 206/384 [00:04<00:03, 58.38it/s]going through batches for holmes training:  55%|█████▌    | 212/384 [00:04<00:02, 58.43it/s]going through batches for holmes training:  57%|█████▋    | 218/384 [00:04<00:02, 58.51it/s]going through batches for holmes training:  58%|█████▊    | 224/384 [00:04<00:02, 58.55it/s]going through batches for holmes training:  60%|█████▉    | 230/384 [00:04<00:02, 58.51it/s]going through batches for holmes training:  61%|██████▏   | 236/384 [00:04<00:02, 58.51it/s]going through batches for holmes training:  63%|██████▎   | 242/384 [00:04<00:02, 58.59it/s]going through batches for holmes training:  65%|██████▍   | 248/384 [00:04<00:02, 58.54it/s]going through batches for holmes training:  66%|██████▌   | 254/384 [00:05<00:02, 58.53it/s]going through batches for holmes training:  68%|██████▊   | 260/384 [00:05<00:02, 58.56it/s]going through batches for holmes training:  69%|██████▉   | 266/384 [00:05<00:02, 58.57it/s]going through batches for holmes training:  71%|███████   | 272/384 [00:05<00:01, 58.58it/s]going through batches for holmes training:  72%|███████▏  | 278/384 [00:05<00:01, 58.50it/s]going through batches for holmes training:  74%|███████▍  | 284/384 [00:05<00:01, 58.51it/s]going through batches for holmes training:  76%|███████▌  | 290/384 [00:05<00:01, 58.51it/s]going through batches for holmes training:  77%|███████▋  | 296/384 [00:05<00:01, 58.54it/s]going through batches for holmes training:  79%|███████▊  | 302/384 [00:05<00:01, 58.58it/s]going through batches for holmes training:  80%|████████  | 308/384 [00:05<00:01, 58.62it/s]going through batches for holmes training:  82%|████████▏ | 314/384 [00:06<00:01, 58.62it/s]going through batches for holmes training:  83%|████████▎ | 320/384 [00:06<00:01, 57.95it/s]going through batches for holmes training:  85%|████████▍ | 326/384 [00:06<00:00, 58.08it/s]going through batches for holmes training:  86%|████████▋ | 332/384 [00:06<00:00, 58.21it/s]going through batches for holmes training:  88%|████████▊ | 338/384 [00:06<00:00, 58.20it/s]going through batches for holmes training:  90%|████████▉ | 344/384 [00:06<00:00, 58.28it/s]going through batches for holmes training:  91%|█████████ | 350/384 [00:06<00:00, 58.40it/s]going through batches for holmes training:  93%|█████████▎| 356/384 [00:06<00:00, 58.42it/s]going through batches for holmes training:  94%|█████████▍| 362/384 [00:06<00:00, 58.42it/s]going through batches for holmes training:  96%|█████████▌| 368/384 [00:07<00:00, 57.83it/s]going through batches for holmes training:  97%|█████████▋| 374/384 [00:07<00:00, 58.09it/s]going through batches for holmes training:  99%|█████████▉| 380/384 [00:07<00:00, 58.30it/s]going through batches for holmes training: 100%|██████████| 384/384 [00:07<00:00, 52.46it/s]
loss is 0.8624826669692993
loss is 0.8423965573310852
loss is 0.872042715549469
loss is 0.8267267346382141
loss is 0.8884202837944031
loss is 0.8958163261413574
loss is 0.842214047908783
loss is 0.9728235602378845
loss is 0.8176982402801514
loss is 0.769524335861206
loss is 0.8985117077827454
loss is 0.7835086584091187
loss is 0.824327290058136
loss is 0.8395578265190125
loss is 0.8362826704978943
loss is 0.9418975710868835
loss is 0.9359435439109802
loss is 0.7861366868019104
loss is 0.8421120643615723
loss is 0.7373417615890503
loss is 0.9149001240730286
loss is 0.9186818599700928
loss is 0.8346426486968994
loss is 0.8528178930282593
loss is 0.8518822193145752
loss is 0.8940896391868591
loss is 0.7851347923278809
loss is 0.887424111366272
loss is 0.960364818572998
loss is 0.8914753198623657
loss is 0.8641115427017212
loss is 0.8894379138946533
loss is 0.856943666934967
loss is 0.7720167636871338
loss is 0.8838568925857544
loss is 0.9670535326004028
loss is 0.8415826559066772
loss is 0.8792852163314819
loss is 0.7771058678627014
loss is 0.8667394518852234
loss is 0.8871827125549316
loss is 0.9362165331840515
loss is 0.8853080868721008
loss is 0.816725492477417
loss is 0.8380477428436279
loss is 0.8585820198059082
loss is 0.8756774067878723
loss is 0.9707273244857788
loss is 0.8964242339134216
loss is 0.9392450451850891
loss is 0.8127622008323669
loss is 0.8487496376037598
loss is 0.8063603043556213
loss is 0.7529903650283813
loss is 0.9889319539070129
loss is 0.9182499647140503
loss is 0.8584696054458618
loss is 0.883036196231842
loss is 0.8456616997718811
loss is 0.9008398652076721
loss is 0.9360073208808899
loss is 0.8926936984062195
loss is 0.891025722026825
loss is 0.8357191681861877
loss is 0.8865628838539124
loss is 0.681774377822876
loss is 0.8230286240577698
loss is 0.8927509188652039
loss is 0.9247882962226868
loss is 0.835263192653656
loss is 0.9204573631286621
loss is 0.8925926089286804
loss is 0.8258953094482422
loss is 0.8833321928977966
loss is 0.9663713574409485
loss is 0.7416471242904663
loss is 0.9548277258872986
loss is 0.8792179822921753
loss is 0.8845402002334595
loss is 0.9181843400001526
loss is 0.793670117855072
loss is 0.8537613749504089
loss is 0.8916209936141968
loss is 0.8110436797142029
loss is 1.004213809967041
loss is 0.8686161637306213
loss is 0.9492253065109253
loss is 0.913962185382843
loss is 0.8420299291610718
loss is 0.8927130699157715
loss is 0.813286304473877
loss is 0.8498661518096924
loss is 0.8824296593666077
loss is 0.814067006111145
loss is 0.9357420206069946
loss is 0.9052602648735046
loss is 0.8096106052398682
loss is 0.8403638601303101
loss is 0.7120116353034973
loss is 0.8568113446235657
loss is 0.9027209281921387
loss is 0.8131160736083984
loss is 0.8987960815429688
loss is 0.934570848941803
loss is 0.7929399609565735
loss is 0.9123438000679016
loss is 0.8551843166351318
loss is 0.9142074584960938
loss is 0.8363651037216187
loss is 0.8363723754882812
loss is 0.8765435814857483
loss is 0.8216602206230164
loss is 0.8365669846534729
loss is 0.8324059844017029
loss is 0.8487451076507568
loss is 0.8651092052459717
loss is 0.9251950979232788
loss is 0.8606268167495728
loss is 0.9508768320083618
loss is 0.8245197534561157
loss is 0.8340420722961426
loss is 0.9370414018630981
loss is 0.8924375772476196
loss is 0.9349360466003418
loss is 0.7751442790031433
loss is 0.8049891591072083
loss is 0.900168240070343
loss is 0.8816696405410767
loss is 0.7875688672065735
loss is 0.8720870018005371
loss is 0.8057314157485962
loss is 0.8248304128646851
loss is 0.8570340871810913
loss is 0.8257091641426086
loss is 0.8303465843200684
loss is 0.984805166721344
loss is 0.8364385962486267
loss is 0.8987984657287598
loss is 0.7738215923309326
loss is 0.8560284972190857
loss is 0.8767178058624268
loss is 0.9426835775375366
loss is 0.8931052684783936
loss is 0.8358625769615173
loss is 0.8318787217140198
loss is 0.862891435623169
loss is 0.8045281767845154
loss is 0.8783706426620483
loss is 0.869170069694519
loss is 0.8467727899551392
loss is 0.7702498435974121
loss is 1.0124939680099487
loss is 0.8930677175521851
loss is 0.8009593486785889
loss is 0.949789822101593
loss is 0.7966459393501282
loss is 0.7700849771499634
loss is 0.879249632358551
loss is 0.943726658821106
loss is 0.9404851794242859
loss is 0.8982834815979004
loss is 0.9175254702568054
loss is 0.8748998045921326
loss is 0.925801694393158
loss is 0.8382956981658936
loss is 0.9285409450531006
loss is 0.8444010019302368
loss is 0.9446057081222534
loss is 0.7812589406967163
loss is 0.8155225515365601
loss is 0.8575573563575745
loss is 0.8163228034973145
loss is 0.850110650062561
loss is 0.9388131499290466
loss is 0.8726670145988464
loss is 0.9566241502761841
loss is 0.9979902505874634
loss is 0.8168015480041504
loss is 0.8634628057479858
loss is 0.8826321959495544
loss is 0.8667844533920288
loss is 0.8618574738502502
loss is 0.8395704627037048
loss is 0.8320407867431641
loss is 0.9037936329841614
loss is 0.8418653011322021
loss is 0.7910716533660889
loss is 0.8567966222763062
loss is 0.8406786918640137
loss is 0.9160764217376709
loss is 0.9288292527198792
loss is 0.8719882369041443
loss is 0.8799177408218384
loss is 0.8821763396263123
loss is 0.8957536220550537
loss is 0.9403536915779114
loss is 0.9050742387771606
loss is 0.9002004265785217
loss is 0.8140062093734741
loss is 0.8190706372261047
loss is 0.9252480268478394
loss is 0.9502107501029968
loss is 0.8758904337882996
loss is 0.9305142760276794
loss is 0.8789422512054443
loss is 0.9605506062507629
loss is 0.8450368642807007
loss is 0.9315651655197144
loss is 0.8946941494941711
loss is 0.9286007881164551
loss is 0.9165318012237549
loss is 0.8901429772377014
loss is 0.9596925377845764
loss is 0.8321176767349243
loss is 0.9251040816307068
loss is 0.8540570139884949
loss is 0.9600555300712585
loss is 0.9352043271064758
loss is 0.8309391140937805
loss is 0.7960106730461121
loss is 0.9137505888938904
loss is 0.9385071396827698
loss is 0.7824037671089172
loss is 0.9065495133399963
loss is 0.8743476867675781
loss is 1.0145764350891113
loss is 0.8655003905296326
loss is 0.855652928352356
loss is 0.7878234386444092
loss is 0.8142403364181519
loss is 0.7595029473304749
loss is 0.8981464505195618
loss is 0.8583935499191284
loss is 0.8738610744476318
loss is 0.8612712025642395
loss is 0.8876058459281921
loss is 1.0274699926376343
loss is 0.8401492834091187
loss is 0.8895296454429626
loss is 0.8513826131820679
loss is 0.8474194407463074
loss is 0.9007134437561035
loss is 0.7356414794921875
loss is 0.8832809925079346
loss is 1.014878511428833
loss is 0.9139900207519531
loss is 0.9515811204910278
loss is 1.0001890659332275
loss is 0.8488589525222778
loss is 0.8379104137420654
loss is 0.8611786365509033
loss is 0.8742642998695374
loss is 0.8929601311683655
loss is 0.8087599277496338
loss is 0.8637976050376892
loss is 0.8861324787139893
loss is 0.8249893188476562
loss is 0.9755821228027344
loss is 0.9366680383682251
loss is 0.9201016426086426
loss is 0.9523587822914124
loss is 0.8558697700500488
loss is 0.8890855312347412
loss is 0.820344090461731
loss is 0.8839449882507324
loss is 0.8685446977615356
loss is 0.9784301519393921
loss is 0.9118273854255676
loss is 0.942196249961853
loss is 0.8394922614097595
loss is 0.8622679114341736
loss is 0.829569399356842
loss is 0.909974217414856
loss is 0.897348165512085
loss is 0.8266375660896301
loss is 0.9125555157661438
loss is 0.9149617552757263
loss is 0.9410966634750366
loss is 0.8475121855735779
loss is 1.0196473598480225
loss is 0.8489229083061218
loss is 0.7679972648620605
loss is 0.9082504510879517
loss is 0.935992956161499
loss is 0.8613799810409546
loss is 0.7764198184013367
loss is 0.974517285823822
loss is 0.8835910558700562
loss is 0.9332097768783569
loss is 0.8613237142562866
loss is 0.8764991760253906
loss is 0.8999865651130676
loss is 0.8807504177093506
loss is 0.8740441799163818
loss is 0.8584627509117126
loss is 0.8516649603843689
loss is 0.9537851810455322
loss is 0.8567441701889038
loss is 0.7984623908996582
loss is 0.8933145403862
loss is 0.8635910749435425
loss is 0.9084717035293579
loss is 0.7733871936798096
loss is 0.8001121282577515
loss is 0.7972562313079834
loss is 0.9237760305404663
loss is 0.9183730483055115
loss is 0.8544261455535889
loss is 0.7656832933425903
loss is 0.8694942593574524
loss is 0.7800723314285278
loss is 0.8385540246963501
loss is 0.9320618510246277
loss is 0.8881617784500122
loss is 0.7837710380554199
loss is 0.9080687165260315
loss is 0.8889740705490112
loss is 0.9313700199127197
loss is 0.8322651386260986
loss is 0.7743320465087891
loss is 0.8542177081108093
loss is 0.8933306336402893
loss is 0.8955223560333252
loss is 0.9159677028656006
loss is 0.7598187923431396
loss is 0.9267094135284424
loss is 0.9125780463218689
loss is 0.9678592085838318
loss is 0.8290340900421143
loss is 0.8799469470977783
loss is 0.994422435760498
loss is 0.8732002973556519
loss is 0.9687560200691223
loss is 0.8760313391685486
loss is 0.8201014995574951
loss is 0.9284721612930298
loss is 0.8724074363708496
loss is 0.8319401741027832
loss is 0.9279147982597351
loss is 0.8499153852462769
loss is 0.836093008518219
loss is 0.9499603509902954
loss is 0.9730368256568909
loss is 0.8485594987869263
loss is 0.8526090979576111
loss is 0.90961754322052
loss is 0.9845182299613953
loss is 0.91429603099823
loss is 0.7343507409095764
loss is 0.909848690032959
loss is 0.9270918965339661
loss is 0.834104061126709
loss is 0.8691219091415405
loss is 0.9543603658676147
loss is 0.8627008199691772
loss is 0.8428525328636169
loss is 0.903063952922821
loss is 0.890379786491394
loss is 0.9539192914962769
loss is 0.8696165680885315
loss is 0.9414806365966797
loss is 0.9891109466552734
loss is 0.9166406393051147
loss is 0.8745035529136658
loss is 0.8978543281555176
loss is 0.9835536479949951
loss is 0.9677345752716064
loss is 0.9082876443862915
loss is 0.805446982383728
loss is 0.863965630531311
loss is 0.8074716925621033
loss is 1.0076278448104858
loss is 0.9102319478988647
loss is 1.005718469619751
loss is 0.939008355140686
loss is 1.0221196413040161
loss is 0.8683899641036987
loss is 0.9292291402816772
loss is 0.9250061511993408
loss is 0.9095914959907532
loss is 0.9381054043769836
loss is 0.9240667223930359
loss is 0.8306427001953125
loss is 0.8594040870666504
epoch 29: train_loss = 0.877
29: {'Accuracy': 0.5722, 'Precision': 0.5751, 'Recall': 0.5712, 'F1-score': 0.5661}
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Valid: X=torch.Size([8550, 1, 2, 1000]), y=torch.Size([8550])
num_classes: 15
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:07<01:50,  7.89s/it] 13%|█▎        | 2/15 [00:15<01:39,  7.62s/it] 20%|██        | 3/15 [00:22<01:30,  7.55s/it] 27%|██▋       | 4/15 [00:30<01:22,  7.52s/it] 33%|███▎      | 5/15 [00:37<01:15,  7.51s/it] 40%|████      | 6/15 [00:45<01:07,  7.50s/it] 47%|████▋     | 7/15 [00:52<00:59,  7.50s/it] 53%|█████▎    | 8/15 [01:00<00:52,  7.49s/it] 60%|██████    | 9/15 [01:07<00:44,  7.49s/it] 67%|██████▋   | 10/15 [01:15<00:37,  7.48s/it] 73%|███████▎  | 11/15 [01:22<00:29,  7.47s/it] 80%|████████  | 12/15 [01:30<00:22,  7.47s/it] 87%|████████▋ | 13/15 [01:37<00:14,  7.47s/it] 93%|█████████▎| 14/15 [01:44<00:07,  7.46s/it]100%|██████████| 15/15 [01:52<00:00,  7.46s/it]100%|██████████| 15/15 [01:52<00:00,  7.50s/it]
shape of attr_values: (15, 1000)
starting data augmentation script for train
  0%|          | 0/76950 [00:00<?, ?it/s]  1%|          | 801/76950 [00:00<00:09, 8003.10it/s]  2%|▏         | 1621/76950 [00:00<00:09, 8114.58it/s]  3%|▎         | 2443/76950 [00:00<00:09, 8158.96it/s]  4%|▍         | 3266/76950 [00:00<00:09, 8186.24it/s]  5%|▌         | 4088/76950 [00:00<00:08, 8197.59it/s]  6%|▋         | 4908/76950 [00:00<00:08, 8195.54it/s]  7%|▋         | 5729/76950 [00:00<00:08, 8197.23it/s]  9%|▊         | 6549/76950 [00:00<00:08, 8197.68it/s] 10%|▉         | 7378/76950 [00:00<00:08, 8223.44it/s] 11%|█         | 8203/76950 [00:01<00:08, 8229.98it/s] 12%|█▏        | 9027/76950 [00:01<00:08, 8232.65it/s] 13%|█▎        | 9851/76950 [00:01<00:08, 8224.19it/s] 14%|█▍        | 10675/76950 [00:01<00:08, 8226.24it/s] 15%|█▍        | 11502/76950 [00:01<00:07, 8237.32it/s] 16%|█▌        | 12331/76950 [00:01<00:07, 8251.23it/s] 17%|█▋        | 13161/76950 [00:01<00:07, 8263.23it/s] 18%|█▊        | 13993/76950 [00:01<00:07, 8279.59it/s] 19%|█▉        | 14821/76950 [00:01<00:07, 8278.09it/s] 20%|██        | 15653/76950 [00:01<00:07, 8288.03it/s] 21%|██▏       | 16482/76950 [00:02<00:07, 8287.80it/s] 22%|██▏       | 17311/76950 [00:02<00:07, 8220.05it/s] 24%|██▎       | 18140/76950 [00:02<00:07, 8240.40it/s] 25%|██▍       | 18965/76950 [00:02<00:07, 8236.97it/s] 26%|██▌       | 19789/76950 [00:02<00:07, 8159.74it/s] 27%|██▋       | 20618/76950 [00:02<00:06, 8197.03it/s] 28%|██▊       | 21438/76950 [00:02<00:06, 8135.67it/s] 29%|██▉       | 22262/76950 [00:02<00:06, 8165.61it/s] 30%|██▉       | 23079/76950 [00:02<00:06, 8118.21it/s] 31%|███       | 23908/76950 [00:02<00:06, 8168.30it/s] 32%|███▏      | 24730/76950 [00:03<00:06, 8181.81it/s] 33%|███▎      | 25549/76950 [00:03<00:06, 8132.22it/s] 34%|███▍      | 26376/76950 [00:03<00:06, 8173.14it/s] 35%|███▌      | 27194/76950 [00:03<00:06, 8149.63it/s] 36%|███▋      | 28013/76950 [00:03<00:05, 8159.27it/s] 37%|███▋      | 28829/76950 [00:03<00:05, 8132.15it/s] 39%|███▊      | 29654/76950 [00:03<00:05, 8166.87it/s] 40%|███▉      | 30471/76950 [00:03<00:05, 8131.29it/s] 41%|████      | 31294/76950 [00:03<00:05, 8158.78it/s] 42%|████▏     | 32124/76950 [00:03<00:05, 8198.45it/s] 43%|████▎     | 32944/76950 [00:04<00:05, 8158.24it/s] 44%|████▍     | 33773/76950 [00:04<00:05, 8195.87it/s] 45%|████▍     | 34593/76950 [00:04<00:05, 8137.28it/s] 46%|████▌     | 35418/76950 [00:04<00:05, 8168.53it/s] 47%|████▋     | 36235/76950 [00:04<00:05, 8124.45it/s] 48%|████▊     | 37064/76950 [00:04<00:04, 8173.62it/s] 49%|████▉     | 37888/76950 [00:04<00:04, 8193.09it/s] 50%|█████     | 38708/76950 [00:04<00:04, 8157.71it/s] 51%|█████▏    | 39532/76950 [00:04<00:04, 8180.44it/s] 52%|█████▏    | 40351/76950 [00:04<00:04, 8151.13it/s] 54%|█████▎    | 41178/76950 [00:05<00:04, 8184.81it/s] 55%|█████▍    | 41997/76950 [00:05<00:04, 8145.10it/s] 56%|█████▌    | 42826/76950 [00:05<00:04, 8185.21it/s] 57%|█████▋    | 43659/76950 [00:05<00:04, 8227.47it/s] 58%|█████▊    | 44482/76950 [00:05<00:04, 7217.03it/s] 59%|█████▉    | 45310/76950 [00:05<00:04, 7505.71it/s] 60%|█████▉    | 46117/76950 [00:05<00:04, 7662.11it/s] 61%|██████    | 46946/76950 [00:05<00:03, 7839.03it/s] 62%|██████▏   | 47754/76950 [00:05<00:03, 7907.84it/s] 63%|██████▎   | 48587/76950 [00:05<00:03, 8028.87it/s] 64%|██████▍   | 49415/76950 [00:06<00:03, 8100.70it/s] 65%|██████▌   | 50230/76950 [00:06<00:03, 8080.85it/s] 66%|██████▋   | 51053/76950 [00:06<00:03, 8122.04it/s] 67%|██████▋   | 51868/76950 [00:06<00:03, 8115.00it/s] 68%|██████▊   | 52701/76950 [00:06<00:02, 8175.95it/s] 70%|██████▉   | 53520/76950 [00:06<00:02, 8146.40it/s] 71%|███████   | 54343/76950 [00:06<00:02, 8170.25it/s] 72%|███████▏  | 55174/76950 [00:06<00:02, 8210.42it/s] 73%|███████▎  | 55996/76950 [00:06<00:02, 8153.71it/s] 74%|███████▍  | 56822/76950 [00:06<00:02, 8183.62it/s] 75%|███████▍  | 57641/76950 [00:07<00:02, 8140.11it/s] 76%|███████▌  | 58470/76950 [00:07<00:02, 8182.99it/s] 77%|███████▋  | 59289/76950 [00:07<00:02, 8136.95it/s] 78%|███████▊  | 60117/76950 [00:07<00:02, 8176.81it/s] 79%|███████▉  | 60935/76950 [00:07<00:01, 8137.08it/s] 80%|████████  | 61764/76950 [00:07<00:01, 8181.71it/s] 81%|████████▏ | 62586/76950 [00:07<00:01, 8190.20it/s] 82%|████████▏ | 63406/76950 [00:07<00:01, 8153.30it/s] 83%|████████▎ | 64227/76950 [00:07<00:01, 8167.62it/s] 85%|████████▍ | 65044/76950 [00:07<00:01, 8140.78it/s] 86%|████████▌ | 65871/76950 [00:08<00:01, 8178.05it/s] 87%|████████▋ | 66689/76950 [00:08<00:01, 8152.37it/s] 88%|████████▊ | 67511/76950 [00:08<00:01, 8171.13it/s] 89%|████████▉ | 68332/76950 [00:08<00:01, 8181.26it/s] 90%|████████▉ | 69151/76950 [00:08<00:00, 8144.47it/s] 91%|█████████ | 69978/76950 [00:08<00:00, 8181.83it/s] 92%|█████████▏| 70797/76950 [00:08<00:00, 8105.62it/s] 93%|█████████▎| 71619/76950 [00:08<00:00, 8139.32it/s] 94%|█████████▍| 72434/76950 [00:08<00:00, 8111.43it/s] 95%|█████████▌| 73260/76950 [00:08<00:00, 8153.98it/s] 96%|█████████▋| 74078/76950 [00:09<00:00, 8159.87it/s] 97%|█████████▋| 74895/76950 [00:09<00:00, 8128.87it/s] 98%|█████████▊| 75725/76950 [00:09<00:00, 8178.43it/s] 99%|█████████▉| 76543/76950 [00:09<00:00, 8161.54it/s]100%|██████████| 76950/76950 [00:09<00:00, 8141.71it/s]

Size Analysis:
--------------
Total raw size: 17.20 GB
Estimated compressed size: 8.60 GB
Overhead: 1.50 KB

Size per array:
X: 17.20 GB
y: 1.76 MB
Generate /home/kka151/scratch/holmes/datasets/Tik_Tok_30/aug_train.npz done.
finished data augmentation script for train
starting data augmentation script for valid
  0%|          | 0/8550 [00:00<?, ?it/s]  9%|▉         | 771/8550 [00:00<00:01, 7702.94it/s] 18%|█▊        | 1559/8550 [00:00<00:00, 7804.98it/s] 27%|██▋       | 2340/8550 [00:00<00:00, 7697.54it/s] 36%|███▋      | 3110/8550 [00:00<00:00, 7666.40it/s] 45%|████▌     | 3877/8550 [00:00<00:00, 7646.31it/s] 54%|█████▍    | 4643/8550 [00:00<00:00, 7650.53it/s] 63%|██████▎   | 5420/8550 [00:00<00:00, 7686.91it/s] 72%|███████▏  | 6189/8550 [00:00<00:00, 7673.45it/s] 81%|████████▏ | 6960/8550 [00:00<00:00, 7682.89it/s] 91%|█████████ | 7741/8550 [00:01<00:00, 7721.85it/s]100%|█████████▉| 8514/8550 [00:01<00:00, 7670.12it/s]100%|██████████| 8550/8550 [00:01<00:00, 7676.52it/s]

Size Analysis:
--------------
Total raw size: 1.91 GB
Estimated compressed size: 978.57 MB
Overhead: 1.50 KB

Size per array:
X: 1.91 GB
y: 200.39 KB
Generate /home/kka151/scratch/holmes/datasets/Tik_Tok_30/aug_valid.npz done.
finished data augmentation script for valid
starting gen taf script for aug_train
  0%|          | 0/230850 [00:00<?, ?it/s]  0%|          | 1/230850 [00:00<11:45:57,  5.45it/s]  0%|          | 1060/230850 [00:00<00:49, 4638.70it/s]  1%|          | 1684/230850 [00:01<03:11, 1199.25it/s]  1%|          | 2052/230850 [00:01<03:51, 988.23it/s]   1%|          | 2302/230850 [00:02<04:15, 895.32it/s]  1%|          | 2486/230850 [00:02<04:47, 793.11it/s]  1%|          | 2625/230850 [00:02<04:55, 773.28it/s]  1%|          | 2741/230850 [00:02<05:10, 735.29it/s]  1%|          | 2839/230850 [00:03<05:09, 736.88it/s]  1%|▏         | 2930/230850 [00:03<05:11, 730.98it/s]  1%|▏         | 3015/230850 [00:03<05:05, 746.26it/s]  1%|▏         | 3099/230850 [00:03<05:29, 691.89it/s]  1%|▏         | 3174/230850 [00:03<05:40, 669.29it/s]  1%|▏         | 3261/230850 [00:03<05:20, 710.83it/s]  1%|▏         | 3337/230850 [00:03<05:17, 715.87it/s]  1%|▏         | 3425/230850 [00:03<05:15, 720.97it/s]  2%|▏         | 3502/230850 [00:04<05:12, 726.88it/s]  2%|▏         | 3577/230850 [00:04<05:15, 721.49it/s]  2%|▏         | 3651/230850 [00:04<06:32, 578.57it/s]  2%|▏         | 3740/230850 [00:04<05:53, 641.86it/s]  2%|▏         | 3809/230850 [00:04<06:08, 616.04it/s]  2%|▏         | 3893/230850 [00:04<05:45, 656.06it/s]  2%|▏         | 3962/230850 [00:04<05:52, 644.33it/s]  2%|▏         | 4029/230850 [00:04<05:56, 636.49it/s]  2%|▏         | 4112/230850 [00:04<05:32, 682.69it/s]  2%|▏         | 4182/230850 [00:05<05:31, 684.70it/s]  2%|▏         | 4252/230850 [00:05<05:33, 679.04it/s]  2%|▏         | 4321/230850 [00:05<05:54, 638.88it/s]  2%|▏         | 4401/230850 [00:05<05:31, 682.51it/s]  2%|▏         | 4471/230850 [00:05<05:52, 642.85it/s]  2%|▏         | 4537/230850 [00:05<05:50, 645.52it/s]  2%|▏         | 4605/230850 [00:05<05:47, 650.44it/s]  2%|▏         | 4671/230850 [00:05<05:53, 639.35it/s]  2%|▏         | 4752/230850 [00:05<05:37, 669.40it/s]  2%|▏         | 4820/230850 [00:06<05:43, 657.88it/s]  2%|▏         | 4886/230850 [00:06<06:16, 599.41it/s]  2%|▏         | 4959/230850 [00:06<05:55, 634.54it/s]  2%|▏         | 5024/230850 [00:06<07:01, 535.43it/s]  2%|▏         | 5123/230850 [00:06<05:53, 638.83it/s]  2%|▏         | 5191/230850 [00:06<05:59, 627.16it/s]  2%|▏         | 5264/230850 [00:06<05:46, 651.94it/s]  2%|▏         | 5332/230850 [00:06<05:51, 642.31it/s]  2%|▏         | 5398/230850 [00:07<06:07, 612.98it/s]  2%|▏         | 5466/230850 [00:07<06:07, 613.74it/s]  2%|▏         | 5542/230850 [00:07<05:53, 638.22it/s]  2%|▏         | 5607/230850 [00:07<06:07, 612.46it/s]  2%|▏         | 5685/230850 [00:07<05:44, 653.80it/s]  2%|▏         | 5752/230850 [00:07<05:55, 632.31it/s]  3%|▎         | 5829/230850 [00:07<05:38, 665.14it/s]  3%|▎         | 5902/230850 [00:07<05:29, 683.39it/s]  3%|▎         | 5984/230850 [00:07<05:14, 714.28it/s]  3%|▎         | 6056/230850 [00:08<05:54, 634.00it/s]  3%|▎         | 6123/230850 [00:08<05:53, 635.94it/s]  3%|▎         | 6203/230850 [00:08<05:30, 680.35it/s]  3%|▎         | 6288/230850 [00:08<05:13, 716.29it/s]  3%|▎         | 6361/230850 [00:08<05:18, 704.04it/s]  3%|▎         | 6433/230850 [00:08<05:31, 676.08it/s]  3%|▎         | 6521/230850 [00:08<05:06, 730.81it/s]  3%|▎         | 6595/230850 [00:08<05:33, 671.72it/s]  3%|▎         | 6677/230850 [00:08<05:18, 704.52it/s]  3%|▎         | 6763/230850 [00:08<05:07, 728.59it/s]  3%|▎         | 6846/230850 [00:09<04:57, 752.22it/s]  3%|▎         | 6922/230850 [00:09<05:03, 736.85it/s]  3%|▎         | 6997/230850 [00:09<05:04, 736.05it/s]  3%|▎         | 7071/230850 [00:09<06:13, 598.55it/s]  3%|▎         | 7136/230850 [00:09<06:09, 605.56it/s]  3%|▎         | 7201/230850 [00:09<06:02, 616.92it/s]  3%|▎         | 7268/230850 [00:09<05:59, 621.64it/s]  3%|▎         | 7332/230850 [00:09<06:31, 571.08it/s]  3%|▎         | 7406/230850 [00:10<06:17, 592.12it/s]  3%|▎         | 7473/230850 [00:10<06:07, 607.49it/s]  3%|▎         | 7550/230850 [00:10<05:47, 642.63it/s]  3%|▎         | 7616/230850 [00:10<06:06, 609.22it/s]  3%|▎         | 7678/230850 [00:10<06:24, 579.73it/s]  3%|▎         | 7754/230850 [00:10<05:57, 623.92it/s]  3%|▎         | 7823/230850 [00:10<05:47, 641.95it/s]  3%|▎         | 7888/230850 [00:10<06:17, 590.27it/s]  3%|▎         | 7984/230850 [00:10<05:23, 688.92it/s]  3%|▎         | 8055/230850 [00:11<06:11, 600.48it/s]  4%|▎         | 8143/230850 [00:11<05:38, 657.29it/s]  4%|▎         | 8224/230850 [00:11<05:23, 688.79it/s]  4%|▎         | 8326/230850 [00:11<04:47, 773.42it/s]  4%|▎         | 8406/230850 [00:11<05:09, 718.67it/s]  4%|▎         | 8481/230850 [00:11<05:21, 692.33it/s]  4%|▎         | 8552/230850 [00:11<05:21, 691.24it/s]  4%|▎         | 8634/230850 [00:11<05:09, 717.81it/s]  4%|▍         | 8714/230850 [00:11<05:01, 736.55it/s]  4%|▍         | 8789/230850 [00:12<05:43, 646.77it/s]  4%|▍         | 8856/230850 [00:12<05:56, 623.57it/s]  4%|▍         | 8944/230850 [00:12<05:26, 679.99it/s]  4%|▍         | 9014/230850 [00:12<05:56, 622.22it/s]  4%|▍         | 9078/230850 [00:12<06:12, 595.96it/s]  4%|▍         | 9154/230850 [00:12<05:54, 625.79it/s]  4%|▍         | 9221/230850 [00:12<05:55, 624.14it/s]  4%|▍         | 9289/230850 [00:12<05:47, 638.26it/s]  4%|▍         | 9354/230850 [00:13<06:10, 597.64it/s]  4%|▍         | 9423/230850 [00:13<06:06, 604.78it/s]  4%|▍         | 9498/230850 [00:13<05:51, 629.36it/s]  4%|▍         | 9585/230850 [00:13<05:27, 675.68it/s]  4%|▍         | 9653/230850 [00:13<05:58, 617.68it/s]  4%|▍         | 9717/230850 [00:13<05:59, 615.39it/s]  4%|▍         | 9810/230850 [00:13<05:18, 694.29it/s]  4%|▍         | 9881/230850 [00:13<05:16, 698.10it/s]  4%|▍         | 9980/230850 [00:13<04:52, 755.62it/s]  4%|▍         | 10056/230850 [00:14<05:05, 723.23it/s]  4%|▍         | 10129/230850 [00:14<05:27, 674.53it/s]  4%|▍         | 10205/230850 [00:14<05:20, 688.30it/s]  4%|▍         | 10278/230850 [00:14<05:22, 683.35it/s]  4%|▍         | 10347/230850 [00:14<06:00, 612.49it/s]  5%|▍         | 10410/230850 [00:14<06:12, 592.50it/s]  5%|▍         | 10475/230850 [00:14<06:12, 591.97it/s]  5%|▍         | 10559/230850 [00:14<05:40, 646.26it/s]  5%|▍         | 10627/230850 [00:14<05:35, 655.48it/s]  5%|▍         | 10694/230850 [00:15<05:46, 635.69it/s]  5%|▍         | 10770/230850 [00:15<05:29, 667.55it/s]  5%|▍         | 10847/230850 [00:15<05:15, 696.52it/s]  5%|▍         | 10918/230850 [00:15<05:24, 678.73it/s]  5%|▍         | 10999/230850 [00:15<05:07, 715.86it/s]  5%|▍         | 11072/230850 [00:15<05:43, 639.04it/s]  5%|▍         | 11163/230850 [00:15<05:08, 711.34it/s]  5%|▍         | 11237/230850 [00:15<05:32, 660.25it/s]  5%|▍         | 11320/230850 [00:15<05:11, 703.69it/s]  5%|▍         | 11393/230850 [00:16<05:25, 674.64it/s]  5%|▍         | 11462/230850 [00:16<05:34, 654.92it/s]  5%|▍         | 11529/230850 [00:16<05:32, 658.68it/s]  5%|▌         | 11609/230850 [00:16<05:23, 676.78it/s]  5%|▌         | 11678/230850 [00:16<06:29, 562.55it/s]  5%|▌         | 11749/230850 [00:16<06:08, 594.20it/s]  5%|▌         | 11812/230850 [00:16<06:31, 560.05it/s]  5%|▌         | 11896/230850 [00:16<05:47, 629.28it/s]  5%|▌         | 11966/230850 [00:17<05:44, 635.91it/s]  5%|▌         | 12033/230850 [00:17<05:42, 638.88it/s]  5%|▌         | 12106/230850 [00:17<05:33, 655.03it/s]  5%|▌         | 12173/230850 [00:17<06:08, 592.72it/s]  5%|▌         | 12237/230850 [00:17<06:01, 605.09it/s]  5%|▌         | 12309/230850 [00:17<05:50, 624.10it/s]  5%|▌         | 12390/230850 [00:17<05:28, 665.20it/s]  5%|▌         | 12458/230850 [00:17<05:33, 655.76it/s]  5%|▌         | 12539/230850 [00:17<05:12, 699.06it/s]  5%|▌         | 12618/230850 [00:18<05:08, 708.20it/s]  5%|▌         | 12690/230850 [00:18<05:23, 674.52it/s]  6%|▌         | 12759/230850 [00:18<06:13, 583.88it/s]  6%|▌         | 12820/230850 [00:18<06:23, 569.11it/s]  6%|▌         | 12909/230850 [00:18<05:35, 650.20it/s]  6%|▌         | 12981/230850 [00:18<05:34, 651.78it/s]  6%|▌         | 13057/230850 [00:18<05:21, 677.89it/s]  6%|▌         | 13127/230850 [00:18<05:36, 646.60it/s]  6%|▌         | 13202/230850 [00:18<05:22, 674.60it/s]  6%|▌         | 13271/230850 [00:19<05:27, 663.56it/s]  6%|▌         | 13339/230850 [00:19<05:27, 664.96it/s]  6%|▌         | 13407/230850 [00:19<07:11, 503.61it/s]  6%|▌         | 13491/230850 [00:19<06:13, 582.62it/s]  6%|▌         | 13556/230850 [00:19<06:38, 544.82it/s]  6%|▌         | 13616/230850 [00:19<06:34, 550.16it/s]  6%|▌         | 13681/230850 [00:19<06:18, 573.88it/s]  6%|▌         | 13759/230850 [00:19<05:48, 623.71it/s]  6%|▌         | 13854/230850 [00:20<05:12, 694.71it/s]  6%|▌         | 13926/230850 [00:20<05:15, 687.47it/s]  6%|▌         | 13996/230850 [00:20<05:39, 639.61it/s]  6%|▌         | 14080/230850 [00:20<05:26, 663.67it/s]  6%|▌         | 14148/230850 [00:20<05:38, 640.64it/s]  6%|▌         | 14213/230850 [00:20<05:43, 630.25it/s]  6%|▌         | 14277/230850 [00:20<06:09, 586.22it/s]  6%|▌         | 14337/230850 [00:20<06:44, 535.79it/s]  6%|▌         | 14404/230850 [00:20<06:24, 563.05it/s]  6%|▋         | 14467/230850 [00:21<06:24, 562.40it/s]  6%|▋         | 14539/230850 [00:21<06:00, 600.07it/s]  6%|▋         | 14600/230850 [00:21<06:09, 585.61it/s]  6%|▋         | 14660/230850 [00:21<06:12, 580.93it/s]  6%|▋         | 14719/230850 [00:21<06:35, 546.08it/s]  6%|▋         | 14800/230850 [00:21<05:54, 608.82it/s]  6%|▋         | 14862/230850 [00:21<05:57, 604.33it/s]  6%|▋         | 14923/230850 [00:21<06:09, 584.59it/s]  6%|▋         | 14992/230850 [00:21<05:59, 600.69it/s]  7%|▋         | 15062/230850 [00:22<05:45, 624.14it/s]  7%|▋         | 15142/230850 [00:22<05:20, 673.17it/s]  7%|▋         | 15210/230850 [00:22<05:40, 633.05it/s]  7%|▋         | 15275/230850 [00:22<06:04, 591.81it/s]  7%|▋         | 15346/230850 [00:22<05:45, 623.37it/s]  7%|▋         | 15425/230850 [00:22<05:22, 668.53it/s]  7%|▋         | 15493/230850 [00:22<05:21, 669.97it/s]  7%|▋         | 15561/230850 [00:22<05:28, 655.53it/s]  7%|▋         | 15632/230850 [00:22<05:27, 656.53it/s]  7%|▋         | 15699/230850 [00:23<05:55, 605.99it/s]  7%|▋         | 15783/230850 [00:23<05:30, 651.25it/s]  7%|▋         | 15852/230850 [00:23<05:27, 656.70it/s]  7%|▋         | 15919/230850 [00:23<05:29, 652.26it/s]  7%|▋         | 15985/230850 [00:23<05:43, 626.18it/s]  7%|▋         | 16060/230850 [00:23<05:45, 621.14it/s]  7%|▋         | 16138/230850 [00:23<05:25, 660.12it/s]  7%|▋         | 16205/230850 [00:23<05:35, 639.81it/s]  7%|▋         | 16291/230850 [00:23<05:13, 684.63it/s]  7%|▋         | 16360/230850 [00:24<05:32, 645.95it/s]  7%|▋         | 16434/230850 [00:24<05:20, 668.87it/s]  7%|▋         | 16502/230850 [00:24<05:25, 658.78it/s]  7%|▋         | 16572/230850 [00:24<05:24, 660.61it/s]  7%|▋         | 16640/230850 [00:24<05:22, 663.60it/s]  7%|▋         | 16708/230850 [00:24<05:23, 661.30it/s]  7%|▋         | 16785/230850 [00:24<05:19, 669.98it/s]  7%|▋         | 16868/230850 [00:24<05:02, 707.65it/s]  7%|▋         | 16941/230850 [00:24<05:06, 698.06it/s]  7%|▋         | 17011/230850 [00:25<05:49, 611.24it/s]  7%|▋         | 17095/230850 [00:25<05:30, 647.37it/s]  7%|▋         | 17162/230850 [00:25<05:37, 632.96it/s]  7%|▋         | 17240/230850 [00:25<05:18, 670.95it/s]  7%|▋         | 17313/230850 [00:25<05:19, 667.89it/s]  8%|▊         | 17381/230850 [00:25<05:27, 651.43it/s]  8%|▊         | 17447/230850 [00:25<05:36, 633.87it/s]  8%|▊         | 17511/230850 [00:25<06:25, 553.67it/s]  8%|▊         | 17608/230850 [00:25<05:32, 642.17it/s]  8%|▊         | 17695/230850 [00:26<05:12, 682.42it/s]  8%|▊         | 17769/230850 [00:26<05:20, 664.18it/s]  8%|▊         | 17868/230850 [00:26<04:44, 749.78it/s]  8%|▊         | 17945/230850 [00:26<04:54, 723.45it/s]  8%|▊         | 18046/230850 [00:26<04:37, 765.91it/s]  8%|▊         | 18127/230850 [00:26<04:36, 768.61it/s]  8%|▊         | 18205/230850 [00:26<05:12, 681.37it/s]  8%|▊         | 18295/230850 [00:26<04:48, 738.02it/s]  8%|▊         | 18375/230850 [00:26<04:42, 750.90it/s]  8%|▊         | 18452/230850 [00:27<04:51, 729.10it/s]  8%|▊         | 18533/230850 [00:27<04:48, 735.11it/s]  8%|▊         | 18609/230850 [00:27<04:53, 722.41it/s]  8%|▊         | 18682/230850 [00:27<05:34, 634.66it/s]  8%|▊         | 18748/230850 [00:27<05:32, 638.44it/s]  8%|▊         | 18836/230850 [00:27<05:01, 702.55it/s]  8%|▊         | 18909/230850 [00:27<05:20, 660.50it/s]  8%|▊         | 18978/230850 [00:27<05:23, 654.19it/s]  8%|▊         | 19045/230850 [00:28<05:29, 642.44it/s]  8%|▊         | 19110/230850 [00:28<05:33, 634.48it/s]  8%|▊         | 19174/230850 [00:28<05:54, 596.30it/s]  8%|▊         | 19253/230850 [00:28<05:30, 640.97it/s]  8%|▊         | 19318/230850 [00:28<05:33, 634.09it/s]  8%|▊         | 19400/230850 [00:28<05:11, 677.78it/s]  8%|▊         | 19469/230850 [00:28<05:20, 659.03it/s]  8%|▊         | 19557/230850 [00:28<04:55, 715.23it/s]  9%|▊         | 19630/230850 [00:28<05:00, 703.11it/s]  9%|▊         | 19701/230850 [00:29<05:09, 681.38it/s]  9%|▊         | 19784/230850 [00:29<05:01, 699.33it/s]  9%|▊         | 19876/230850 [00:29<04:42, 747.87it/s]  9%|▊         | 19952/230850 [00:29<05:12, 674.06it/s]  9%|▊         | 20021/230850 [00:29<05:24, 649.25it/s]  9%|▊         | 20087/230850 [00:29<05:27, 644.52it/s]  9%|▊         | 20154/230850 [00:29<05:32, 634.47it/s]  9%|▉         | 20232/230850 [00:29<05:22, 652.27it/s]  9%|▉         | 20298/230850 [00:29<05:24, 648.34it/s]  9%|▉         | 20364/230850 [00:30<05:25, 647.14it/s]  9%|▉         | 20430/230850 [00:30<05:27, 642.14it/s]  9%|▉         | 20507/230850 [00:30<05:19, 659.23it/s]  9%|▉         | 20575/230850 [00:30<05:26, 643.45it/s]  9%|▉         | 20640/230850 [00:30<05:30, 635.76it/s]  9%|▉         | 20720/230850 [00:30<05:09, 678.72it/s]  9%|▉         | 20791/230850 [00:30<05:13, 670.49it/s]  9%|▉         | 20864/230850 [00:30<05:10, 676.96it/s]  9%|▉         | 20932/230850 [00:30<05:46, 606.00it/s]  9%|▉         | 21006/230850 [00:31<05:30, 635.01it/s]  9%|▉         | 21082/230850 [00:31<05:15, 665.39it/s]  9%|▉         | 21151/230850 [00:31<05:12, 670.78it/s]  9%|▉         | 21219/230850 [00:31<05:17, 659.38it/s]  9%|▉         | 21287/230850 [00:31<05:17, 660.42it/s]  9%|▉         | 21375/230850 [00:31<04:51, 719.26it/s]  9%|▉         | 21468/230850 [00:31<04:34, 763.38it/s]  9%|▉         | 21545/230850 [00:31<04:52, 716.56it/s]  9%|▉         | 21618/230850 [00:31<04:55, 709.08it/s]  9%|▉         | 21705/230850 [00:31<04:37, 754.05it/s]  9%|▉         | 21781/230850 [00:32<04:52, 713.71it/s]  9%|▉         | 21854/230850 [00:32<05:30, 631.48it/s]  9%|▉         | 21920/230850 [00:32<05:55, 587.19it/s] 10%|▉         | 21981/230850 [00:32<06:16, 555.01it/s] 10%|▉         | 22056/230850 [00:32<05:45, 603.87it/s] 10%|▉         | 22119/230850 [00:32<05:45, 604.89it/s] 10%|▉         | 22190/230850 [00:32<05:29, 633.56it/s] 10%|▉         | 22267/230850 [00:32<05:10, 671.80it/s] 10%|▉         | 22356/230850 [00:32<04:46, 728.39it/s] 10%|▉         | 22433/230850 [00:33<04:44, 732.41it/s] 10%|▉         | 22507/230850 [00:33<05:23, 643.95it/s] 10%|▉         | 22591/230850 [00:33<04:59, 694.61it/s] 10%|▉         | 22663/230850 [00:33<05:15, 659.60it/s] 10%|▉         | 22738/230850 [00:33<05:04, 683.53it/s] 10%|▉         | 22808/230850 [00:33<05:07, 676.16it/s] 10%|▉         | 22877/230850 [00:33<05:16, 656.21it/s] 10%|▉         | 22944/230850 [00:33<05:22, 644.60it/s] 10%|▉         | 23009/230850 [00:34<05:39, 612.57it/s] 10%|█         | 23106/230850 [00:34<05:07, 675.83it/s] 10%|█         | 23174/230850 [00:34<06:50, 506.35it/s] 10%|█         | 23237/230850 [00:34<06:28, 533.90it/s] 10%|█         | 23314/230850 [00:34<05:56, 581.43it/s] 10%|█         | 23382/230850 [00:34<05:47, 596.85it/s] 10%|█         | 23445/230850 [00:34<06:11, 557.59it/s] 10%|█         | 23521/230850 [00:34<05:57, 580.07it/s] 10%|█         | 23581/230850 [00:35<05:58, 578.39it/s] 10%|█         | 23645/230850 [00:35<05:52, 588.33it/s] 10%|█         | 23719/230850 [00:35<05:36, 616.40it/s] 10%|█         | 23782/230850 [00:35<06:09, 560.46it/s] 10%|█         | 23876/230850 [00:35<05:24, 638.16it/s] 10%|█         | 23942/230850 [00:35<05:58, 576.94it/s] 10%|█         | 24002/230850 [00:35<05:55, 582.22it/s] 10%|█         | 24080/230850 [00:35<05:42, 603.48it/s] 10%|█         | 24167/230850 [00:35<05:10, 665.42it/s] 10%|█         | 24235/230850 [00:36<06:04, 567.24it/s] 11%|█         | 24304/230850 [00:36<05:52, 586.50it/s] 11%|█         | 24373/230850 [00:36<05:40, 605.53it/s] 11%|█         | 24436/230850 [00:36<05:48, 592.38it/s] 11%|█         | 24520/230850 [00:36<05:36, 612.70it/s] 11%|█         | 24590/230850 [00:36<05:33, 618.15it/s] 11%|█         | 24665/230850 [00:36<05:20, 642.92it/s] 11%|█         | 24742/230850 [00:36<05:07, 670.04it/s] 11%|█         | 24810/230850 [00:37<05:20, 643.67it/s] 11%|█         | 24875/230850 [00:37<05:20, 643.35it/s] 11%|█         | 24945/230850 [00:37<05:20, 642.48it/s] 11%|█         | 25010/230850 [00:37<05:24, 634.28it/s] 11%|█         | 25074/230850 [00:37<05:25, 633.01it/s] 11%|█         | 25138/230850 [00:37<05:44, 596.34it/s] 11%|█         | 25236/230850 [00:37<04:56, 693.16it/s] 11%|█         | 25320/230850 [00:37<04:45, 720.20it/s] 11%|█         | 25393/230850 [00:37<04:52, 701.68it/s] 11%|█         | 25472/230850 [00:37<04:46, 717.94it/s] 11%|█         | 25545/230850 [00:38<05:02, 677.87it/s] 11%|█         | 25614/230850 [00:38<05:34, 613.30it/s] 11%|█         | 25707/230850 [00:38<04:57, 688.41it/s] 11%|█         | 25778/230850 [00:38<05:07, 666.09it/s] 11%|█         | 25846/230850 [00:38<05:16, 647.06it/s] 11%|█         | 25912/230850 [00:38<05:26, 628.00it/s] 11%|█▏        | 25991/230850 [00:38<05:09, 661.20it/s] 11%|█▏        | 26073/230850 [00:38<04:56, 690.32it/s] 11%|█▏        | 26143/230850 [00:39<04:56, 690.40it/s] 11%|█▏        | 26222/230850 [00:39<04:47, 711.33it/s] 11%|█▏        | 26295/230850 [00:39<04:54, 694.81it/s] 11%|█▏        | 26395/230850 [00:39<04:23, 777.15it/s] 11%|█▏        | 26474/230850 [00:39<04:58, 684.81it/s] 12%|█▏        | 26556/230850 [00:39<04:45, 715.06it/s] 12%|█▏        | 26633/230850 [00:39<04:40, 727.42it/s] 12%|█▏        | 26708/230850 [00:39<04:44, 717.71it/s] 12%|█▏        | 26781/230850 [00:39<05:01, 677.10it/s] 12%|█▏        | 26850/230850 [00:40<05:24, 628.88it/s] 12%|█▏        | 26944/230850 [00:40<04:53, 694.83it/s] 12%|█▏        | 27015/230850 [00:40<04:59, 681.10it/s] 12%|█▏        | 27084/230850 [00:40<05:08, 659.80it/s] 12%|█▏        | 27151/230850 [00:40<05:23, 629.73it/s] 12%|█▏        | 27254/230850 [00:40<04:35, 737.79it/s] 12%|█▏        | 27330/230850 [00:40<04:55, 689.48it/s] 12%|█▏        | 27401/230850 [00:40<05:28, 619.05it/s] 12%|█▏        | 27483/230850 [00:40<05:03, 670.22it/s] 12%|█▏        | 27553/230850 [00:41<05:04, 666.95it/s] 12%|█▏        | 27639/230850 [00:41<04:46, 710.01it/s] 12%|█▏        | 27712/230850 [00:41<04:51, 697.28it/s] 12%|█▏        | 27783/230850 [00:41<04:52, 694.68it/s] 12%|█▏        | 27854/230850 [00:41<05:12, 648.85it/s] 12%|█▏        | 27920/230850 [00:41<05:26, 622.11it/s] 12%|█▏        | 27991/230850 [00:41<05:14, 644.98it/s] 12%|█▏        | 28095/230850 [00:41<04:29, 753.39it/s] 12%|█▏        | 28172/230850 [00:41<04:34, 737.89it/s] 12%|█▏        | 28252/230850 [00:42<04:35, 736.48it/s] 12%|█▏        | 28327/230850 [00:42<04:51, 695.02it/s] 12%|█▏        | 28398/230850 [00:42<05:17, 638.03it/s] 12%|█▏        | 28463/230850 [00:42<05:17, 636.64it/s] 12%|█▏        | 28528/230850 [00:42<05:56, 567.05it/s] 12%|█▏        | 28605/230850 [00:42<05:27, 618.33it/s] 12%|█▏        | 28669/230850 [00:42<06:05, 552.50it/s] 12%|█▏        | 28738/230850 [00:42<05:48, 580.04it/s] 12%|█▏        | 28825/230850 [00:43<05:12, 646.84it/s] 13%|█▎        | 28901/230850 [00:43<04:58, 676.21it/s] 13%|█▎        | 28971/230850 [00:43<05:11, 647.14it/s] 13%|█▎        | 29038/230850 [00:43<05:37, 597.18it/s] 13%|█▎        | 29100/230850 [00:43<05:37, 598.26it/s] 13%|█▎        | 29172/230850 [00:43<05:19, 631.23it/s] 13%|█▎        | 29237/230850 [00:43<05:20, 629.96it/s] 13%|█▎        | 29301/230850 [00:43<05:28, 613.59it/s] 13%|█▎        | 29383/230850 [00:43<05:00, 670.18it/s] 13%|█▎        | 29451/230850 [00:44<05:04, 660.72it/s] 13%|█▎        | 29518/230850 [00:44<05:17, 634.02it/s] 13%|█▎        | 29595/230850 [00:44<05:11, 646.30it/s] 13%|█▎        | 29660/230850 [00:44<05:21, 625.50it/s] 13%|█▎        | 29723/230850 [00:44<05:27, 614.93it/s] 13%|█▎        | 29785/230850 [00:44<05:35, 598.51it/s] 13%|█▎        | 29877/230850 [00:44<04:59, 670.75it/s] 13%|█▎        | 29949/230850 [00:44<04:53, 684.28it/s] 13%|█▎        | 30018/230850 [00:44<05:11, 644.52it/s] 13%|█▎        | 30093/230850 [00:45<04:58, 672.55it/s] 13%|█▎        | 30168/230850 [00:45<04:50, 690.03it/s] 13%|█▎        | 30238/230850 [00:45<04:51, 689.06it/s] 13%|█▎        | 30341/230850 [00:45<04:23, 761.22it/s] 13%|█▎        | 30421/230850 [00:45<04:26, 752.79it/s] 13%|█▎        | 30497/230850 [00:45<04:29, 742.72it/s] 13%|█▎        | 30572/230850 [00:45<05:38, 591.10it/s] 13%|█▎        | 30653/230850 [00:45<05:11, 643.64it/s] 13%|█▎        | 30725/230850 [00:45<05:02, 662.60it/s] 13%|█▎        | 30795/230850 [00:46<05:03, 660.17it/s] 13%|█▎        | 30875/230850 [00:46<05:02, 660.43it/s] 13%|█▎        | 30968/230850 [00:46<04:37, 721.42it/s] 13%|█▎        | 31042/230850 [00:46<04:50, 687.30it/s] 13%|█▎        | 31112/230850 [00:46<05:08, 646.46it/s] 14%|█▎        | 31178/230850 [00:46<05:15, 633.73it/s] 14%|█▎        | 31243/230850 [00:46<05:21, 621.52it/s] 14%|█▎        | 31306/230850 [00:46<05:27, 609.70it/s] 14%|█▎        | 31381/230850 [00:46<05:08, 646.14it/s] 14%|█▎        | 31447/230850 [00:47<05:23, 615.72it/s] 14%|█▎        | 31525/230850 [00:47<05:06, 650.72it/s] 14%|█▎        | 31595/230850 [00:47<05:04, 653.86it/s] 14%|█▎        | 31671/230850 [00:47<04:59, 666.10it/s] 14%|█▎        | 31738/230850 [00:47<05:42, 580.82it/s] 14%|█▍        | 31798/230850 [00:47<05:40, 584.46it/s] 14%|█▍        | 31879/230850 [00:47<05:08, 645.07it/s] 14%|█▍        | 31965/230850 [00:47<04:45, 697.11it/s] 14%|█▍        | 32041/230850 [00:47<04:40, 709.18it/s] 14%|█▍        | 32113/230850 [00:48<05:07, 645.60it/s] 14%|█▍        | 32194/230850 [00:48<04:51, 680.72it/s] 14%|█▍        | 32264/230850 [00:48<04:51, 682.05it/s] 14%|█▍        | 32334/230850 [00:48<05:04, 651.70it/s] 14%|█▍        | 32413/230850 [00:48<04:53, 677.13it/s] 14%|█▍        | 32482/230850 [00:48<05:11, 636.30it/s] 14%|█▍        | 32568/230850 [00:48<04:44, 695.94it/s] 14%|█▍        | 32639/230850 [00:48<05:02, 656.02it/s] 14%|█▍        | 32706/230850 [00:48<05:09, 640.11it/s] 14%|█▍        | 32771/230850 [00:49<05:24, 611.30it/s] 14%|█▍        | 32835/230850 [00:49<05:22, 613.95it/s] 14%|█▍        | 32897/230850 [00:49<06:46, 487.49it/s] 14%|█▍        | 32961/230850 [00:49<06:26, 512.62it/s] 14%|█▍        | 33023/230850 [00:49<06:07, 537.61it/s] 14%|█▍        | 33088/230850 [00:49<05:56, 554.97it/s] 14%|█▍        | 33174/230850 [00:49<05:13, 631.47it/s] 14%|█▍        | 33240/230850 [00:49<05:36, 586.66it/s] 14%|█▍        | 33313/230850 [00:50<05:29, 598.61it/s] 14%|█▍        | 33375/230850 [00:50<06:28, 507.66it/s] 14%|█▍        | 33440/230850 [00:50<06:20, 518.74it/s] 15%|█▍        | 33513/230850 [00:50<05:47, 568.30it/s] 15%|█▍        | 33573/230850 [00:50<06:02, 543.98it/s] 15%|█▍        | 33631/230850 [00:50<05:58, 550.58it/s] 15%|█▍        | 33695/230850 [00:50<05:55, 554.62it/s] 15%|█▍        | 33761/230850 [00:50<05:44, 571.55it/s] 15%|█▍        | 33844/230850 [00:50<05:09, 637.45it/s] 15%|█▍        | 33909/230850 [00:51<05:08, 638.20it/s] 15%|█▍        | 33974/230850 [00:51<05:18, 617.29it/s] 15%|█▍        | 34061/230850 [00:51<04:48, 681.73it/s] 15%|█▍        | 34130/230850 [00:51<05:20, 613.88it/s] 15%|█▍        | 34193/230850 [00:51<05:20, 613.41it/s] 15%|█▍        | 34256/230850 [00:51<05:23, 608.04it/s] 15%|█▍        | 34318/230850 [00:51<05:24, 604.85it/s] 15%|█▍        | 34394/230850 [00:51<05:03, 646.70it/s] 15%|█▍        | 34460/230850 [00:51<05:21, 610.84it/s] 15%|█▍        | 34531/230850 [00:52<05:08, 636.31it/s] 15%|█▍        | 34596/230850 [00:52<05:37, 582.32it/s] 15%|█▌        | 34661/230850 [00:52<05:27, 598.71it/s] 15%|█▌        | 34722/230850 [00:52<05:43, 570.16it/s] 15%|█▌        | 34790/230850 [00:52<05:28, 597.35it/s] 15%|█▌        | 34853/230850 [00:52<05:31, 591.25it/s] 15%|█▌        | 34940/230850 [00:52<04:53, 667.93it/s] 15%|█▌        | 35008/230850 [00:52<04:57, 658.12it/s] 15%|█▌        | 35075/230850 [00:52<05:03, 645.38it/s] 15%|█▌        | 35153/230850 [00:53<04:51, 672.39it/s] 15%|█▌        | 35221/230850 [00:53<05:13, 623.85it/s] 15%|█▌        | 35291/230850 [00:53<05:14, 622.00it/s] 15%|█▌        | 35357/230850 [00:53<05:16, 617.19it/s] 15%|█▌        | 35420/230850 [00:53<05:20, 610.00it/s] 15%|█▌        | 35485/230850 [00:53<05:15, 619.44it/s] 15%|█▌        | 35558/230850 [00:53<05:02, 646.20it/s] 15%|█▌        | 35629/230850 [00:53<04:56, 657.41it/s] 15%|█▌        | 35696/230850 [00:53<04:55, 660.18it/s] 15%|█▌        | 35763/230850 [00:54<05:43, 568.08it/s] 16%|█▌        | 35854/230850 [00:54<05:01, 646.57it/s] 16%|█▌        | 35933/230850 [00:54<04:48, 676.15it/s] 16%|█▌        | 36003/230850 [00:54<04:57, 655.71it/s] 16%|█▌        | 36070/230850 [00:54<05:05, 638.55it/s] 16%|█▌        | 36135/230850 [00:54<05:08, 630.37it/s] 16%|█▌        | 36199/230850 [00:54<05:23, 601.08it/s] 16%|█▌        | 36285/230850 [00:54<04:49, 671.89it/s] 16%|█▌        | 36354/230850 [00:54<04:57, 653.22it/s] 16%|█▌        | 36429/230850 [00:55<04:45, 679.96it/s] 16%|█▌        | 36498/230850 [00:55<04:55, 656.68it/s] 16%|█▌        | 36576/230850 [00:55<04:41, 690.26it/s] 16%|█▌        | 36673/230850 [00:55<04:25, 731.96it/s] 16%|█▌        | 36747/230850 [00:55<05:10, 624.38it/s] 16%|█▌        | 36812/230850 [00:55<05:08, 629.83it/s] 16%|█▌        | 36877/230850 [00:55<05:24, 597.55it/s] 16%|█▌        | 36956/230850 [00:55<05:03, 638.20it/s] 16%|█▌        | 37022/230850 [00:56<05:13, 617.29it/s] 16%|█▌        | 37119/230850 [00:56<04:36, 700.02it/s] 16%|█▌        | 37191/230850 [00:56<04:39, 692.92it/s] 16%|█▌        | 37262/230850 [00:56<04:56, 652.02it/s] 16%|█▌        | 37329/230850 [00:56<05:03, 638.22it/s] 16%|█▌        | 37395/230850 [00:56<05:00, 643.34it/s] 16%|█▌        | 37460/230850 [00:56<05:06, 631.58it/s] 16%|█▋        | 37524/230850 [00:56<05:08, 625.80it/s] 16%|█▋        | 37592/230850 [00:56<05:04, 635.69it/s] 16%|█▋        | 37656/230850 [00:57<05:20, 603.09it/s] 16%|█▋        | 37735/230850 [00:57<04:54, 655.34it/s] 16%|█▋        | 37802/230850 [00:57<05:45, 558.20it/s] 16%|█▋        | 37869/230850 [00:57<05:31, 581.75it/s] 16%|█▋        | 37930/230850 [00:57<05:31, 582.27it/s] 16%|█▋        | 38021/230850 [00:57<04:47, 669.93it/s] 16%|█▋        | 38090/230850 [00:57<04:52, 659.93it/s] 17%|█▋        | 38161/230850 [00:57<04:50, 662.26it/s] 17%|█▋        | 38233/230850 [00:57<04:52, 658.29it/s] 17%|█▋        | 38302/230850 [00:58<04:53, 655.59it/s] 17%|█▋        | 38374/230850 [00:58<04:50, 661.74it/s] 17%|█▋        | 38441/230850 [00:58<04:51, 658.99it/s] 17%|█▋        | 38508/230850 [00:58<05:08, 622.94it/s] 17%|█▋        | 38571/230850 [00:58<05:32, 578.14it/s] 17%|█▋        | 38656/230850 [00:58<04:58, 643.55it/s] 17%|█▋        | 38722/230850 [00:58<05:09, 620.84it/s] 17%|█▋        | 38792/230850 [00:58<05:05, 628.88it/s] 17%|█▋        | 38856/230850 [00:58<05:24, 592.55it/s] 17%|█▋        | 38921/230850 [00:59<05:22, 594.32it/s] 17%|█▋        | 39008/230850 [00:59<05:01, 636.09it/s] 17%|█▋        | 39092/230850 [00:59<04:38, 687.44it/s] 17%|█▋        | 39174/230850 [00:59<04:25, 721.26it/s] 17%|█▋        | 39258/230850 [00:59<04:23, 727.32it/s] 17%|█▋        | 39332/230850 [00:59<04:29, 709.36it/s] 17%|█▋        | 39404/230850 [00:59<04:33, 700.24it/s] 17%|█▋        | 39487/230850 [00:59<04:20, 734.05it/s] 17%|█▋        | 39561/230850 [00:59<04:37, 689.48it/s] 17%|█▋        | 39635/230850 [01:00<04:33, 700.27it/s] 17%|█▋        | 39714/230850 [01:00<04:23, 725.36it/s] 17%|█▋        | 39793/230850 [01:00<04:24, 721.55it/s] 17%|█▋        | 39875/230850 [01:00<04:23, 725.10it/s] 17%|█▋        | 39948/230850 [01:00<04:33, 699.25it/s] 17%|█▋        | 40019/230850 [01:00<04:39, 681.56it/s] 17%|█▋        | 40113/230850 [01:00<04:15, 747.89it/s] 17%|█▋        | 40189/230850 [01:00<04:21, 729.21it/s] 17%|█▋        | 40263/230850 [01:00<04:32, 700.37it/s] 17%|█▋        | 40334/230850 [01:01<04:45, 667.07it/s] 18%|█▊        | 40402/230850 [01:01<05:00, 633.46it/s] 18%|█▊        | 40474/230850 [01:01<04:52, 651.72it/s] 18%|█▊        | 40555/230850 [01:01<04:36, 688.41it/s] 18%|█▊        | 40625/230850 [01:01<04:40, 677.94it/s] 18%|█▊        | 40703/230850 [01:01<04:29, 705.24it/s] 18%|█▊        | 40774/230850 [01:01<04:29, 704.81it/s] 18%|█▊        | 40845/230850 [01:01<04:32, 697.96it/s] 18%|█▊        | 40915/230850 [01:01<04:33, 693.75it/s] 18%|█▊        | 40985/230850 [01:01<04:52, 649.74it/s] 18%|█▊        | 41081/230850 [01:02<04:20, 727.33it/s] 18%|█▊        | 41155/230850 [01:02<04:26, 712.37it/s] 18%|█▊        | 41239/230850 [01:02<04:14, 744.15it/s] 18%|█▊        | 41314/230850 [01:02<04:22, 721.61it/s] 18%|█▊        | 41391/230850 [01:02<04:25, 712.79it/s] 18%|█▊        | 41463/230850 [01:02<04:37, 683.64it/s] 18%|█▊        | 41561/230850 [01:02<04:10, 757.06it/s] 18%|█▊        | 41638/230850 [01:02<04:40, 675.48it/s] 18%|█▊        | 41709/230850 [01:02<04:38, 678.29it/s] 18%|█▊        | 41779/230850 [01:03<04:54, 642.46it/s] 18%|█▊        | 41859/230850 [01:03<04:46, 660.56it/s] 18%|█▊        | 41926/230850 [01:03<04:49, 652.58it/s] 18%|█▊        | 41998/230850 [01:03<04:41, 670.93it/s] 18%|█▊        | 42066/230850 [01:03<04:57, 634.96it/s] 18%|█▊        | 42147/230850 [01:03<04:37, 678.81it/s] 18%|█▊        | 42216/230850 [01:03<04:41, 669.51it/s] 18%|█▊        | 42284/230850 [01:03<04:43, 665.21it/s] 18%|█▊        | 42360/230850 [01:03<04:39, 674.54it/s] 18%|█▊        | 42428/230850 [01:04<04:51, 645.69it/s] 18%|█▊        | 42507/230850 [01:04<04:36, 682.31it/s] 18%|█▊        | 42576/230850 [01:04<05:55, 528.88it/s] 18%|█▊        | 42638/230850 [01:04<05:45, 545.47it/s] 18%|█▊        | 42703/230850 [01:04<05:34, 562.81it/s] 19%|█▊        | 42763/230850 [01:04<06:20, 493.87it/s] 19%|█▊        | 42818/230850 [01:04<06:21, 493.01it/s] 19%|█▊        | 42886/230850 [01:04<05:51, 535.05it/s] 19%|█▊        | 42955/230850 [01:05<05:29, 569.74it/s] 19%|█▊        | 43040/230850 [01:05<04:56, 634.20it/s] 19%|█▊        | 43106/230850 [01:05<05:03, 617.66it/s] 19%|█▊        | 43171/230850 [01:05<05:12, 600.02it/s] 19%|█▊        | 43239/230850 [01:05<05:01, 621.87it/s] 19%|█▉        | 43317/230850 [01:05<04:46, 653.93it/s] 19%|█▉        | 43384/230850 [01:05<04:47, 651.49it/s] 19%|█▉        | 43450/230850 [01:05<04:51, 642.80it/s] 19%|█▉        | 43522/230850 [01:05<04:48, 648.91it/s] 19%|█▉        | 43598/230850 [01:06<04:42, 662.10it/s] 19%|█▉        | 43681/230850 [01:06<04:27, 698.96it/s] 19%|█▉        | 43752/230850 [01:06<04:32, 687.13it/s] 19%|█▉        | 43833/230850 [01:06<04:26, 702.66it/s] 19%|█▉        | 43904/230850 [01:06<05:24, 575.91it/s] 19%|█▉        | 43966/230850 [01:06<05:38, 552.10it/s] 19%|█▉        | 44032/230850 [01:06<05:29, 566.20it/s] 19%|█▉        | 44111/230850 [01:06<05:00, 621.73it/s] 19%|█▉        | 44176/230850 [01:07<05:12, 597.17it/s] 19%|█▉        | 44252/230850 [01:07<04:56, 629.97it/s] 19%|█▉        | 44317/230850 [01:07<04:56, 628.49it/s] 19%|█▉        | 44392/230850 [01:07<04:44, 656.13it/s] 19%|█▉        | 44484/230850 [01:07<04:17, 723.07it/s] 19%|█▉        | 44558/230850 [01:07<04:28, 694.07it/s] 19%|█▉        | 44629/230850 [01:07<04:54, 632.65it/s] 19%|█▉        | 44694/230850 [01:07<05:28, 566.10it/s] 19%|█▉        | 44769/230850 [01:07<05:08, 602.54it/s] 19%|█▉        | 44845/230850 [01:08<04:51, 637.35it/s] 19%|█▉        | 44928/230850 [01:08<04:33, 679.86it/s] 19%|█▉        | 45002/230850 [01:08<04:32, 681.96it/s] 20%|█▉        | 45077/230850 [01:08<04:29, 689.58it/s] 20%|█▉        | 45173/230850 [01:08<04:12, 735.85it/s] 20%|█▉        | 45247/230850 [01:08<04:26, 696.30it/s] 20%|█▉        | 45318/230850 [01:08<04:58, 621.29it/s] 20%|█▉        | 45388/230850 [01:08<04:53, 632.37it/s] 20%|█▉        | 45466/230850 [01:08<04:39, 662.24it/s] 20%|█▉        | 45534/230850 [01:09<04:54, 630.18it/s] 20%|█▉        | 45603/230850 [01:09<04:52, 634.13it/s] 20%|█▉        | 45668/230850 [01:09<04:57, 621.53it/s] 20%|█▉        | 45731/230850 [01:09<05:15, 586.47it/s] 20%|█▉        | 45796/230850 [01:09<05:07, 602.71it/s] 20%|█▉        | 45876/230850 [01:09<04:49, 639.03it/s] 20%|█▉        | 45960/230850 [01:09<04:32, 678.60it/s] 20%|█▉        | 46029/230850 [01:09<04:55, 624.93it/s] 20%|█▉        | 46093/230850 [01:09<04:54, 627.22it/s] 20%|██        | 46172/230850 [01:10<04:34, 672.18it/s] 20%|██        | 46241/230850 [01:10<05:11, 592.85it/s] 20%|██        | 46310/230850 [01:10<05:04, 606.32it/s] 20%|██        | 46373/230850 [01:10<05:43, 536.93it/s] 20%|██        | 46446/230850 [01:10<05:24, 567.95it/s] 20%|██        | 46508/230850 [01:10<05:23, 569.60it/s] 20%|██        | 46586/230850 [01:10<05:00, 612.86it/s] 20%|██        | 46649/230850 [01:10<05:15, 584.32it/s] 20%|██        | 46709/230850 [01:11<05:29, 558.87it/s] 20%|██        | 46787/230850 [01:11<05:02, 608.45it/s] 20%|██        | 46868/230850 [01:11<04:38, 661.73it/s] 20%|██        | 46940/230850 [01:11<04:31, 677.50it/s] 20%|██        | 47009/230850 [01:11<04:32, 674.39it/s] 20%|██        | 47085/230850 [01:11<04:25, 693.11it/s] 20%|██        | 47155/230850 [01:11<04:38, 659.10it/s] 20%|██        | 47231/230850 [01:11<04:42, 649.34it/s] 20%|██        | 47301/230850 [01:11<04:45, 643.77it/s] 21%|██        | 47386/230850 [01:12<04:22, 697.94it/s] 21%|██        | 47457/230850 [01:12<04:24, 693.81it/s] 21%|██        | 47527/230850 [01:12<04:27, 684.76it/s] 21%|██        | 47596/230850 [01:12<04:43, 646.08it/s] 21%|██        | 47662/230850 [01:12<04:59, 611.05it/s] 21%|██        | 47731/230850 [01:12<04:54, 621.77it/s] 21%|██        | 47797/230850 [01:12<04:57, 614.63it/s] 21%|██        | 47871/230850 [01:12<04:41, 649.06it/s] 21%|██        | 47958/230850 [01:12<04:21, 700.17it/s] 21%|██        | 48029/230850 [01:13<04:34, 665.43it/s] 21%|██        | 48097/230850 [01:13<04:39, 654.97it/s] 21%|██        | 48169/230850 [01:13<04:33, 667.65it/s] 21%|██        | 48241/230850 [01:13<04:27, 682.49it/s] 21%|██        | 48310/230850 [01:13<04:31, 673.40it/s] 21%|██        | 48396/230850 [01:13<04:16, 712.47it/s] 21%|██        | 48468/230850 [01:13<04:27, 682.78it/s] 21%|██        | 48554/230850 [01:13<04:09, 729.45it/s] 21%|██        | 48628/230850 [01:13<04:43, 641.89it/s] 21%|██        | 48702/230850 [01:14<04:33, 665.49it/s] 21%|██        | 48771/230850 [01:14<05:02, 602.87it/s] 21%|██        | 48834/230850 [01:14<05:01, 603.67it/s] 21%|██        | 48904/230850 [01:14<04:49, 627.76it/s] 21%|██        | 48969/230850 [01:14<05:03, 598.92it/s] 21%|██        | 49031/230850 [01:14<05:01, 603.78it/s] 21%|██▏       | 49118/230850 [01:14<04:33, 665.10it/s] 21%|██▏       | 49199/230850 [01:14<04:18, 703.88it/s] 21%|██▏       | 49271/230850 [01:14<04:31, 668.62it/s] 21%|██▏       | 49370/230850 [01:15<04:00, 753.23it/s] 21%|██▏       | 49447/230850 [01:15<04:22, 691.95it/s] 21%|██▏       | 49520/230850 [01:15<04:18, 701.84it/s] 21%|██▏       | 49592/230850 [01:15<04:28, 675.50it/s] 22%|██▏       | 49661/230850 [01:15<04:30, 669.02it/s] 22%|██▏       | 49746/230850 [01:15<04:12, 716.87it/s] 22%|██▏       | 49829/230850 [01:15<04:10, 722.95it/s] 22%|██▏       | 49917/230850 [01:15<03:57, 762.25it/s] 22%|██▏       | 49994/230850 [01:15<04:13, 712.48it/s] 22%|██▏       | 50067/230850 [01:16<04:46, 630.02it/s] 22%|██▏       | 50141/230850 [01:16<04:35, 657.10it/s] 22%|██▏       | 50216/230850 [01:16<04:41, 641.91it/s] 22%|██▏       | 50282/230850 [01:16<04:48, 625.54it/s] 22%|██▏       | 50363/230850 [01:16<04:30, 666.05it/s] 22%|██▏       | 50437/230850 [01:16<04:23, 685.61it/s] 22%|██▏       | 50507/230850 [01:16<04:27, 674.48it/s] 22%|██▏       | 50576/230850 [01:16<04:26, 677.42it/s] 22%|██▏       | 50648/230850 [01:16<04:22, 687.45it/s] 22%|██▏       | 50719/230850 [01:17<04:26, 677.02it/s] 22%|██▏       | 50787/230850 [01:17<04:41, 640.62it/s] 22%|██▏       | 50874/230850 [01:17<04:17, 698.62it/s] 22%|██▏       | 50945/230850 [01:17<04:26, 675.63it/s] 22%|██▏       | 51014/230850 [01:17<04:39, 642.39it/s] 22%|██▏       | 51079/230850 [01:17<04:41, 639.26it/s] 22%|██▏       | 51170/230850 [01:17<04:12, 711.22it/s] 22%|██▏       | 51242/230850 [01:17<04:49, 621.01it/s] 22%|██▏       | 51307/230850 [01:17<04:48, 621.42it/s] 22%|██▏       | 51371/230850 [01:18<05:01, 595.79it/s] 22%|██▏       | 51440/230850 [01:18<04:56, 605.75it/s] 22%|██▏       | 51515/230850 [01:18<04:43, 631.63it/s] 22%|██▏       | 51585/230850 [01:18<04:56, 604.11it/s] 22%|██▏       | 51655/230850 [01:18<04:45, 627.84it/s] 22%|██▏       | 51725/230850 [01:18<04:43, 632.13it/s] 22%|██▏       | 51789/230850 [01:18<04:43, 630.74it/s] 22%|██▏       | 51862/230850 [01:18<04:31, 658.41it/s] 22%|██▏       | 51941/230850 [01:18<04:20, 687.19it/s] 23%|██▎       | 52014/230850 [01:19<04:17, 693.74it/s] 23%|██▎       | 52084/230850 [01:19<04:18, 692.33it/s] 23%|██▎       | 52154/230850 [01:19<06:03, 491.84it/s] 23%|██▎       | 52230/230850 [01:19<05:42, 521.76it/s] 23%|██▎       | 52311/230850 [01:19<05:04, 586.72it/s] 23%|██▎       | 52380/230850 [01:19<04:52, 610.78it/s] 23%|██▎       | 52458/230850 [01:19<04:37, 641.79it/s] 23%|██▎       | 52539/230850 [01:19<04:20, 684.44it/s] 23%|██▎       | 52611/230850 [01:20<04:31, 657.25it/s] 23%|██▎       | 52687/230850 [01:20<04:20, 685.07it/s] 23%|██▎       | 52758/230850 [01:20<04:28, 663.46it/s] 23%|██▎       | 52826/230850 [01:20<04:28, 663.24it/s] 23%|██▎       | 52898/230850 [01:20<04:22, 678.42it/s] 23%|██▎       | 52972/230850 [01:20<04:18, 686.85it/s] 23%|██▎       | 53042/230850 [01:20<04:24, 672.48it/s] 23%|██▎       | 53110/230850 [01:20<04:27, 663.78it/s] 23%|██▎       | 53185/230850 [01:20<04:20, 683.20it/s] 23%|██▎       | 53254/230850 [01:20<04:31, 654.64it/s] 23%|██▎       | 53320/230850 [01:21<04:43, 627.29it/s] 23%|██▎       | 53412/230850 [01:21<04:16, 692.11it/s] 23%|██▎       | 53482/230850 [01:21<04:53, 605.34it/s] 23%|██▎       | 53545/230850 [01:21<05:07, 576.18it/s] 23%|██▎       | 53604/230850 [01:21<05:08, 575.39it/s] 23%|██▎       | 53683/230850 [01:21<04:45, 621.36it/s] 23%|██▎       | 53756/230850 [01:21<04:32, 649.03it/s] 23%|██▎       | 53822/230850 [01:21<04:39, 633.14it/s] 23%|██▎       | 53893/230850 [01:22<04:33, 645.84it/s] 23%|██▎       | 53964/230850 [01:22<04:26, 663.98it/s] 23%|██▎       | 54031/230850 [01:22<04:31, 651.18it/s] 23%|██▎       | 54097/230850 [01:22<04:37, 637.62it/s] 23%|██▎       | 54193/230850 [01:22<04:04, 721.52it/s] 24%|██▎       | 54266/230850 [01:22<04:12, 700.72it/s] 24%|██▎       | 54337/230850 [01:22<04:10, 703.32it/s] 24%|██▎       | 54429/230850 [01:22<03:58, 740.18it/s] 24%|██▎       | 54504/230850 [01:22<04:01, 731.07it/s] 24%|██▎       | 54578/230850 [01:22<04:12, 697.36it/s] 24%|██▎       | 54657/230850 [01:23<04:10, 702.99it/s] 24%|██▎       | 54735/230850 [01:23<04:07, 712.64it/s] 24%|██▎       | 54807/230850 [01:23<04:25, 663.57it/s] 24%|██▍       | 54874/230850 [01:23<04:25, 663.64it/s] 24%|██▍       | 54941/230850 [01:23<04:52, 602.27it/s] 24%|██▍       | 55012/230850 [01:23<04:47, 612.61it/s] 24%|██▍       | 55075/230850 [01:23<05:07, 572.48it/s] 24%|██▍       | 55141/230850 [01:23<05:00, 585.36it/s] 24%|██▍       | 55228/230850 [01:24<04:27, 656.05it/s] 24%|██▍       | 55295/230850 [01:24<04:42, 621.16it/s] 24%|██▍       | 55359/230850 [01:24<04:46, 611.53it/s] 24%|██▍       | 55443/230850 [01:24<04:21, 671.94it/s] 24%|██▍       | 55512/230850 [01:24<04:20, 673.39it/s] 24%|██▍       | 55581/230850 [01:24<04:24, 662.43it/s] 24%|██▍       | 55655/230850 [01:24<04:21, 669.15it/s] 24%|██▍       | 55723/230850 [01:24<04:26, 656.02it/s] 24%|██▍       | 55805/230850 [01:24<04:13, 689.27it/s] 24%|██▍       | 55880/230850 [01:24<04:12, 694.21it/s] 24%|██▍       | 55959/230850 [01:25<04:07, 705.59it/s] 24%|██▍       | 56035/230850 [01:25<04:14, 685.55it/s] 24%|██▍       | 56108/230850 [01:25<04:10, 697.30it/s] 24%|██▍       | 56188/230850 [01:25<04:07, 705.52it/s] 24%|██▍       | 56259/230850 [01:25<04:15, 683.41it/s] 24%|██▍       | 56334/230850 [01:25<04:09, 700.53it/s] 24%|██▍       | 56405/230850 [01:25<04:34, 635.27it/s] 24%|██▍       | 56470/230850 [01:25<04:34, 636.19it/s] 24%|██▍       | 56535/230850 [01:25<04:47, 606.13it/s] 25%|██▍       | 56610/230850 [01:26<04:38, 626.46it/s] 25%|██▍       | 56676/230850 [01:26<04:35, 632.05it/s] 25%|██▍       | 56740/230850 [01:26<04:45, 608.80it/s] 25%|██▍       | 56820/230850 [01:26<04:24, 658.45it/s] 25%|██▍       | 56887/230850 [01:26<04:27, 650.33it/s] 25%|██▍       | 56953/230850 [01:26<04:26, 652.11it/s] 25%|██▍       | 57019/230850 [01:26<04:26, 651.88it/s] 25%|██▍       | 57095/230850 [01:26<04:15, 680.90it/s] 25%|██▍       | 57176/230850 [01:26<04:02, 716.52it/s] 25%|██▍       | 57248/230850 [01:27<04:16, 676.42it/s] 25%|██▍       | 57317/230850 [01:27<04:24, 655.58it/s] 25%|██▍       | 57386/230850 [01:27<04:25, 653.24it/s] 25%|██▍       | 57460/230850 [01:27<04:24, 655.13it/s] 25%|██▍       | 57526/230850 [01:27<04:24, 655.95it/s] 25%|██▍       | 57592/230850 [01:27<04:33, 632.70it/s] 25%|██▍       | 57661/230850 [01:27<04:28, 645.40it/s] 25%|██▌       | 57726/230850 [01:27<04:45, 607.15it/s] 25%|██▌       | 57793/230850 [01:27<04:38, 621.18it/s] 25%|██▌       | 57856/230850 [01:28<05:24, 533.84it/s] 25%|██▌       | 57934/230850 [01:28<04:55, 584.97it/s] 25%|██▌       | 58008/230850 [01:28<04:40, 617.00it/s] 25%|██▌       | 58082/230850 [01:28<04:26, 648.74it/s] 25%|██▌       | 58158/230850 [01:28<04:14, 677.88it/s] 25%|██▌       | 58233/230850 [01:28<04:07, 696.66it/s] 25%|██▌       | 58314/230850 [01:28<04:03, 708.81it/s] 25%|██▌       | 58386/230850 [01:28<04:20, 662.35it/s] 25%|██▌       | 58454/230850 [01:28<04:24, 651.70it/s] 25%|██▌       | 58520/230850 [01:29<04:29, 638.82it/s] 25%|██▌       | 58593/230850 [01:29<04:20, 662.00it/s] 25%|██▌       | 58660/230850 [01:29<04:30, 636.67it/s] 25%|██▌       | 58725/230850 [01:29<04:28, 640.36it/s] 25%|██▌       | 58790/230850 [01:29<04:59, 573.96it/s] 26%|██▌       | 58872/230850 [01:29<04:33, 628.24it/s] 26%|██▌       | 58947/230850 [01:29<04:32, 630.28it/s] 26%|██▌       | 59026/230850 [01:29<04:18, 664.83it/s] 26%|██▌       | 59094/230850 [01:29<04:30, 636.03it/s] 26%|██▌       | 59191/230850 [01:30<04:00, 712.55it/s] 26%|██▌       | 59277/230850 [01:30<03:53, 733.71it/s] 26%|██▌       | 59352/230850 [01:30<03:55, 727.49it/s] 26%|██▌       | 59426/230850 [01:30<04:05, 697.13it/s] 26%|██▌       | 59500/230850 [01:30<04:07, 691.76it/s] 26%|██▌       | 59570/230850 [01:30<04:42, 605.97it/s] 26%|██▌       | 59652/230850 [01:30<04:22, 651.41it/s] 26%|██▌       | 59746/230850 [01:30<03:57, 720.22it/s] 26%|██▌       | 59823/230850 [01:30<03:55, 726.22it/s] 26%|██▌       | 59909/230850 [01:31<03:45, 757.82it/s] 26%|██▌       | 59986/230850 [01:31<03:57, 718.30it/s] 26%|██▌       | 60059/230850 [01:31<04:03, 699.99it/s] 26%|██▌       | 60132/230850 [01:31<04:08, 686.88it/s] 26%|██▌       | 60202/230850 [01:31<04:17, 662.44it/s] 26%|██▌       | 60269/230850 [01:31<04:18, 660.19it/s] 26%|██▌       | 60336/230850 [01:31<04:42, 602.98it/s] 26%|██▌       | 60408/230850 [01:31<04:34, 621.16it/s] 26%|██▌       | 60482/230850 [01:31<04:30, 628.83it/s] 26%|██▌       | 60546/230850 [01:32<04:30, 629.43it/s] 26%|██▋       | 60612/230850 [01:32<04:32, 625.82it/s] 26%|██▋       | 60696/230850 [01:32<04:08, 685.92it/s] 26%|██▋       | 60766/230850 [01:32<04:28, 632.60it/s] 26%|██▋       | 60875/230850 [01:32<03:45, 754.02it/s] 26%|██▋       | 60953/230850 [01:32<03:54, 723.73it/s] 26%|██▋       | 61027/230850 [01:32<04:16, 661.87it/s] 26%|██▋       | 61098/230850 [01:32<04:17, 659.06it/s] 27%|██▋       | 61188/230850 [01:33<03:58, 711.71it/s] 27%|██▋       | 61261/230850 [01:33<04:08, 683.48it/s] 27%|██▋       | 61331/230850 [01:33<04:11, 674.20it/s] 27%|██▋       | 61400/230850 [01:33<04:10, 676.47it/s] 27%|██▋       | 61469/230850 [01:33<04:25, 637.64it/s] 27%|██▋       | 61541/230850 [01:33<04:24, 639.07it/s] 27%|██▋       | 61606/230850 [01:33<04:26, 634.49it/s] 27%|██▋       | 61675/230850 [01:33<04:24, 640.32it/s] 27%|██▋       | 61740/230850 [01:33<05:02, 559.43it/s] 27%|██▋       | 61817/230850 [01:34<04:42, 598.26it/s] 27%|██▋       | 61888/230850 [01:34<04:33, 616.96it/s] 27%|██▋       | 61952/230850 [01:34<05:45, 489.02it/s] 27%|██▋       | 62006/230850 [01:34<05:39, 497.91it/s] 27%|██▋       | 62085/230850 [01:34<04:55, 570.46it/s] 27%|██▋       | 62165/230850 [01:34<04:32, 620.06it/s] 27%|██▋       | 62231/230850 [01:34<04:35, 612.44it/s] 27%|██▋       | 62295/230850 [01:34<04:34, 612.93it/s] 27%|██▋       | 62358/230850 [01:35<05:02, 556.18it/s] 27%|██▋       | 62430/230850 [01:35<04:41, 598.34it/s] 27%|██▋       | 62497/230850 [01:35<04:38, 605.13it/s] 27%|██▋       | 62561/230850 [01:35<04:34, 614.01it/s] 27%|██▋       | 62650/230850 [01:35<04:04, 687.86it/s] 27%|██▋       | 62720/230850 [01:35<04:34, 613.40it/s] 27%|██▋       | 62784/230850 [01:35<05:00, 559.61it/s] 27%|██▋       | 62858/230850 [01:35<04:37, 605.63it/s] 27%|██▋       | 62922/230850 [01:35<04:36, 606.32it/s] 27%|██▋       | 62985/230850 [01:36<04:34, 611.06it/s] 27%|██▋       | 63048/230850 [01:36<04:55, 567.88it/s] 27%|██▋       | 63107/230850 [01:36<05:02, 555.39it/s] 27%|██▋       | 63173/230850 [01:36<04:52, 572.47it/s] 27%|██▋       | 63242/230850 [01:36<04:37, 604.75it/s] 27%|██▋       | 63316/230850 [01:36<04:23, 635.38it/s] 27%|██▋       | 63381/230850 [01:36<04:45, 586.79it/s] 27%|██▋       | 63445/230850 [01:36<04:41, 595.10it/s] 28%|██▊       | 63507/230850 [01:36<04:42, 592.25it/s] 28%|██▊       | 63570/230850 [01:37<04:47, 581.83it/s] 28%|██▊       | 63645/230850 [01:37<04:28, 622.78it/s] 28%|██▊       | 63733/230850 [01:37<04:02, 690.22it/s] 28%|██▊       | 63810/230850 [01:37<03:59, 698.89it/s] 28%|██▊       | 63881/230850 [01:37<04:16, 650.88it/s] 28%|██▊       | 63958/230850 [01:37<04:04, 682.10it/s] 28%|██▊       | 64028/230850 [01:37<04:42, 591.26it/s] 28%|██▊       | 64100/230850 [01:37<04:44, 585.23it/s] 28%|██▊       | 64161/230850 [01:37<04:46, 582.29it/s] 28%|██▊       | 64228/230850 [01:38<04:35, 604.27it/s] 28%|██▊       | 64290/230850 [01:38<04:35, 603.57it/s] 28%|██▊       | 64352/230850 [01:38<04:41, 591.95it/s] 28%|██▊       | 64434/230850 [01:38<04:17, 645.48it/s] 28%|██▊       | 64525/230850 [01:38<03:51, 719.97it/s] 28%|██▊       | 64598/230850 [01:38<04:04, 679.10it/s] 28%|██▊       | 64671/230850 [01:38<04:06, 673.38it/s] 28%|██▊       | 64740/230850 [01:38<04:06, 673.70it/s] 28%|██▊       | 64808/230850 [01:38<04:08, 667.59it/s] 28%|██▊       | 64876/230850 [01:39<04:51, 569.67it/s] 28%|██▊       | 64959/230850 [01:39<04:21, 633.96it/s] 28%|██▊       | 65026/230850 [01:39<04:31, 610.39it/s] 28%|██▊       | 65123/230850 [01:39<04:05, 674.06it/s] 28%|██▊       | 65194/230850 [01:39<04:04, 678.41it/s] 28%|██▊       | 65263/230850 [01:39<04:03, 680.59it/s] 28%|██▊       | 65348/230850 [01:39<03:50, 718.22it/s] 28%|██▊       | 65421/230850 [01:39<03:52, 711.30it/s] 28%|██▊       | 65493/230850 [01:39<04:15, 648.12it/s] 28%|██▊       | 65574/230850 [01:40<04:02, 680.43it/s] 28%|██▊       | 65644/230850 [01:40<04:10, 660.09it/s] 28%|██▊       | 65728/230850 [01:40<03:53, 706.12it/s] 29%|██▊       | 65802/230850 [01:40<03:51, 714.27it/s] 29%|██▊       | 65875/230850 [01:40<04:22, 628.79it/s] 29%|██▊       | 65951/230850 [01:40<04:08, 663.13it/s] 29%|██▊       | 66022/230850 [01:40<04:04, 674.62it/s] 29%|██▊       | 66095/230850 [01:40<04:02, 680.00it/s] 29%|██▊       | 66168/230850 [01:40<03:59, 687.22it/s] 29%|██▊       | 66239/230850 [01:41<04:01, 681.53it/s] 29%|██▊       | 66315/230850 [01:41<03:53, 703.97it/s] 29%|██▉       | 66402/230850 [01:41<03:42, 738.33it/s] 29%|██▉       | 66494/230850 [01:41<03:31, 776.97it/s] 29%|██▉       | 66572/230850 [01:41<03:40, 744.11it/s] 29%|██▉       | 66647/230850 [01:41<03:51, 708.51it/s] 29%|██▉       | 66746/230850 [01:41<03:31, 774.30it/s] 29%|██▉       | 66824/230850 [01:41<03:45, 728.70it/s] 29%|██▉       | 66898/230850 [01:41<03:50, 712.45it/s] 29%|██▉       | 66985/230850 [01:42<03:45, 727.35it/s] 29%|██▉       | 67064/230850 [01:42<03:39, 744.56it/s] 29%|██▉       | 67139/230850 [01:42<04:29, 606.57it/s] 29%|██▉       | 67223/230850 [01:42<04:06, 662.48it/s] 29%|██▉       | 67294/230850 [01:42<04:47, 568.97it/s] 29%|██▉       | 67371/230850 [01:42<04:27, 611.93it/s] 29%|██▉       | 67463/230850 [01:42<04:01, 675.49it/s] 29%|██▉       | 67535/230850 [01:42<04:13, 645.02it/s] 29%|██▉       | 67621/230850 [01:43<03:56, 691.22it/s] 29%|██▉       | 67693/230850 [01:43<04:27, 609.12it/s] 29%|██▉       | 67757/230850 [01:43<04:29, 606.29it/s] 29%|██▉       | 67820/230850 [01:43<04:43, 575.34it/s] 29%|██▉       | 67899/230850 [01:43<04:27, 608.47it/s] 29%|██▉       | 67974/230850 [01:43<04:15, 637.47it/s] 29%|██▉       | 68039/230850 [01:43<04:16, 635.80it/s] 30%|██▉       | 68104/230850 [01:43<04:14, 639.18it/s] 30%|██▉       | 68169/230850 [01:43<04:20, 624.31it/s] 30%|██▉       | 68232/230850 [01:44<04:20, 623.52it/s] 30%|██▉       | 68308/230850 [01:44<04:10, 647.91it/s] 30%|██▉       | 68373/230850 [01:44<04:13, 640.42it/s] 30%|██▉       | 68438/230850 [01:44<04:12, 642.07it/s] 30%|██▉       | 68503/230850 [01:44<04:13, 639.39it/s] 30%|██▉       | 68567/230850 [01:44<04:15, 635.75it/s] 30%|██▉       | 68643/230850 [01:44<04:03, 665.49it/s] 30%|██▉       | 68735/230850 [01:44<03:40, 734.19it/s] 30%|██▉       | 68811/230850 [01:44<03:39, 737.68it/s] 30%|██▉       | 68901/230850 [01:44<03:26, 784.43it/s] 30%|██▉       | 68980/230850 [01:45<03:41, 731.98it/s] 30%|██▉       | 69054/230850 [01:45<03:47, 710.15it/s] 30%|██▉       | 69129/230850 [01:45<03:46, 713.64it/s] 30%|██▉       | 69201/230850 [01:45<04:21, 617.93it/s] 30%|███       | 69266/230850 [01:45<04:32, 593.83it/s] 30%|███       | 69353/230850 [01:45<04:09, 647.73it/s] 30%|███       | 69425/230850 [01:45<04:04, 661.18it/s] 30%|███       | 69498/230850 [01:45<03:58, 677.15it/s] 30%|███       | 69567/230850 [01:46<04:06, 655.37it/s] 30%|███       | 69634/230850 [01:46<04:12, 638.98it/s] 30%|███       | 69699/230850 [01:46<04:31, 594.63it/s] 30%|███       | 69761/230850 [01:46<04:28, 599.60it/s] 30%|███       | 69833/230850 [01:46<04:14, 632.52it/s] 30%|███       | 69897/230850 [01:46<04:16, 628.33it/s] 30%|███       | 69965/230850 [01:46<04:13, 633.55it/s] 30%|███       | 70052/230850 [01:46<03:59, 671.29it/s] 30%|███       | 70143/230850 [01:46<03:42, 722.81it/s] 30%|███       | 70216/230850 [01:47<03:45, 710.88it/s] 30%|███       | 70293/230850 [01:47<03:45, 712.86it/s] 30%|███       | 70365/230850 [01:47<03:55, 681.25it/s] 31%|███       | 70446/230850 [01:47<03:44, 716.00it/s] 31%|███       | 70525/230850 [01:47<03:41, 723.17it/s] 31%|███       | 70598/230850 [01:47<04:20, 615.77it/s] 31%|███       | 70663/230850 [01:47<04:17, 622.28it/s] 31%|███       | 70728/230850 [01:47<04:29, 593.28it/s] 31%|███       | 70789/230850 [01:47<04:30, 591.25it/s] 31%|███       | 70865/230850 [01:48<04:11, 636.36it/s] 31%|███       | 70939/230850 [01:48<04:03, 656.19it/s] 31%|███       | 71016/230850 [01:48<03:59, 667.40it/s] 31%|███       | 71084/230850 [01:48<04:10, 637.79it/s] 31%|███       | 71149/230850 [01:48<04:10, 637.01it/s] 31%|███       | 71229/230850 [01:48<03:53, 682.86it/s] 31%|███       | 71298/230850 [01:48<03:56, 675.91it/s] 31%|███       | 71380/230850 [01:48<03:42, 717.35it/s] 31%|███       | 71453/230850 [01:48<03:48, 697.92it/s] 31%|███       | 71530/230850 [01:48<03:41, 718.60it/s] 31%|███       | 71603/230850 [01:49<03:46, 703.04it/s] 31%|███       | 71682/230850 [01:49<03:40, 720.26it/s] 31%|███       | 71755/230850 [01:49<05:11, 511.41it/s] 31%|███       | 71815/230850 [01:49<05:01, 527.64it/s] 31%|███       | 71875/230850 [01:49<04:52, 543.77it/s] 31%|███       | 71935/230850 [01:49<04:54, 539.88it/s] 31%|███       | 71993/230850 [01:49<05:01, 526.81it/s] 31%|███       | 72064/230850 [01:49<04:39, 568.65it/s] 31%|███       | 72127/230850 [01:50<04:32, 582.40it/s] 31%|███▏      | 72201/230850 [01:50<04:13, 626.02it/s] 31%|███▏      | 72270/230850 [01:50<04:12, 629.19it/s] 31%|███▏      | 72348/230850 [01:50<04:01, 655.42it/s] 31%|███▏      | 72415/230850 [01:50<04:02, 653.41it/s] 31%|███▏      | 72481/230850 [01:50<04:12, 628.37it/s] 31%|███▏      | 72546/230850 [01:50<04:19, 609.47it/s] 31%|███▏      | 72608/230850 [01:50<04:29, 586.59it/s] 31%|███▏      | 72680/230850 [01:50<04:17, 613.38it/s] 32%|███▏      | 72742/230850 [01:51<04:19, 609.28it/s] 32%|███▏      | 72804/230850 [01:51<04:34, 574.96it/s] 32%|███▏      | 72862/230850 [01:51<04:47, 550.35it/s] 32%|███▏      | 72918/230850 [01:51<04:55, 534.32it/s] 32%|███▏      | 72972/230850 [01:51<05:02, 522.77it/s] 32%|███▏      | 73039/230850 [01:51<04:41, 560.09it/s] 32%|███▏      | 73119/230850 [01:51<04:26, 591.94it/s] 32%|███▏      | 73183/230850 [01:51<04:21, 603.42it/s] 32%|███▏      | 73273/230850 [01:51<03:50, 683.75it/s] 32%|███▏      | 73342/230850 [01:52<04:12, 624.38it/s] 32%|███▏      | 73414/230850 [01:52<04:04, 642.90it/s] 32%|███▏      | 73507/230850 [01:52<03:38, 721.57it/s] 32%|███▏      | 73581/230850 [01:52<03:41, 709.45it/s] 32%|███▏      | 73653/230850 [01:52<03:52, 675.31it/s] 32%|███▏      | 73732/230850 [01:52<03:45, 695.79it/s] 32%|███▏      | 73812/230850 [01:52<03:36, 724.83it/s] 32%|███▏      | 73886/230850 [01:52<03:35, 727.53it/s] 32%|███▏      | 73971/230850 [01:52<03:25, 762.76it/s] 32%|███▏      | 74048/230850 [01:53<03:31, 741.46it/s] 32%|███▏      | 74123/230850 [01:53<03:40, 710.39it/s] 32%|███▏      | 74195/230850 [01:53<03:39, 712.37it/s] 32%|███▏      | 74267/230850 [01:53<03:48, 686.67it/s] 32%|███▏      | 74338/230850 [01:53<03:47, 686.55it/s] 32%|███▏      | 74407/230850 [01:53<03:47, 687.44it/s] 32%|███▏      | 74476/230850 [01:53<04:16, 608.57it/s] 32%|███▏      | 74547/230850 [01:53<04:08, 629.04it/s] 32%|███▏      | 74612/230850 [01:53<04:07, 632.52it/s] 32%|███▏      | 74677/230850 [01:54<04:28, 582.70it/s] 32%|███▏      | 74757/230850 [01:54<04:06, 632.65it/s] 32%|███▏      | 74824/230850 [01:54<04:08, 627.76it/s] 32%|███▏      | 74895/230850 [01:54<04:02, 642.27it/s] 32%|███▏      | 74960/230850 [01:54<04:13, 614.72it/s] 32%|███▏      | 75023/230850 [01:54<04:16, 608.69it/s] 33%|███▎      | 75085/230850 [01:54<04:47, 542.06it/s] 33%|███▎      | 75150/230850 [01:54<04:37, 561.26it/s] 33%|███▎      | 75234/230850 [01:54<04:06, 631.43it/s] 33%|███▎      | 75331/230850 [01:55<03:35, 720.30it/s] 33%|███▎      | 75405/230850 [01:55<03:41, 702.65it/s] 33%|███▎      | 75477/230850 [01:55<04:17, 603.11it/s] 33%|███▎      | 75560/230850 [01:55<03:58, 649.92it/s] 33%|███▎      | 75628/230850 [01:55<04:05, 631.14it/s] 33%|███▎      | 75699/230850 [01:55<03:58, 649.86it/s] 33%|███▎      | 75766/230850 [01:55<04:26, 581.66it/s] 33%|███▎      | 75830/230850 [01:55<04:20, 594.77it/s] 33%|███▎      | 75904/230850 [01:56<04:04, 633.10it/s] 33%|███▎      | 75984/230850 [01:56<03:57, 652.84it/s] 33%|███▎      | 76088/230850 [01:56<03:29, 738.19it/s] 33%|███▎      | 76163/230850 [01:56<03:52, 664.07it/s] 33%|███▎      | 76232/230850 [01:56<04:01, 640.17it/s] 33%|███▎      | 76298/230850 [01:56<04:12, 611.88it/s] 33%|███▎      | 76383/230850 [01:56<03:49, 673.98it/s] 33%|███▎      | 76452/230850 [01:56<03:59, 645.79it/s] 33%|███▎      | 76520/230850 [01:56<04:01, 638.94it/s] 33%|███▎      | 76589/230850 [01:57<03:56, 651.09it/s] 33%|███▎      | 76655/230850 [01:57<04:21, 590.65it/s] 33%|███▎      | 76730/230850 [01:57<04:07, 622.09it/s] 33%|███▎      | 76801/230850 [01:57<03:59, 642.98it/s] 33%|███▎      | 76876/230850 [01:57<03:53, 660.78it/s] 33%|███▎      | 76968/230850 [01:57<03:30, 730.53it/s] 33%|███▎      | 77042/230850 [01:57<03:41, 695.35it/s] 33%|███▎      | 77113/230850 [01:57<03:57, 648.41it/s] 33%|███▎      | 77180/230850 [01:57<03:56, 648.54it/s] 33%|███▎      | 77246/230850 [01:58<04:02, 633.91it/s] 33%|███▎      | 77322/230850 [01:58<03:51, 662.91it/s] 34%|███▎      | 77422/230850 [01:58<03:24, 749.37it/s] 34%|███▎      | 77498/230850 [01:58<03:35, 712.49it/s] 34%|███▎      | 77570/230850 [01:58<03:38, 702.26it/s] 34%|███▎      | 77641/230850 [01:58<03:51, 662.21it/s] 34%|███▎      | 77708/230850 [01:58<03:54, 653.37it/s] 34%|███▎      | 77791/230850 [01:58<03:37, 702.27it/s] 34%|███▎      | 77874/230850 [01:58<03:32, 719.92it/s] 34%|███▍      | 77947/230850 [01:59<03:47, 672.50it/s] 34%|███▍      | 78021/230850 [01:59<03:41, 690.90it/s] 34%|███▍      | 78117/230850 [01:59<03:23, 752.14it/s] 34%|███▍      | 78193/230850 [01:59<03:28, 732.36it/s] 34%|███▍      | 78267/230850 [01:59<03:33, 713.15it/s] 34%|███▍      | 78339/230850 [01:59<03:53, 652.06it/s] 34%|███▍      | 78420/230850 [01:59<03:43, 681.01it/s] 34%|███▍      | 78490/230850 [01:59<03:43, 682.24it/s] 34%|███▍      | 78560/230850 [01:59<03:41, 686.14it/s] 34%|███▍      | 78630/230850 [02:00<03:56, 644.87it/s] 34%|███▍      | 78699/230850 [02:00<03:56, 642.64it/s] 34%|███▍      | 78764/230850 [02:00<03:57, 639.97it/s] 34%|███▍      | 78833/230850 [02:00<03:56, 643.72it/s] 34%|███▍      | 78898/230850 [02:00<04:16, 593.02it/s] 34%|███▍      | 78959/230850 [02:00<04:23, 575.39it/s] 34%|███▍      | 79032/230850 [02:00<04:10, 606.16it/s] 34%|███▍      | 79094/230850 [02:00<04:13, 598.61it/s] 34%|███▍      | 79184/230850 [02:00<03:42, 682.41it/s] 34%|███▍      | 79254/230850 [02:01<03:55, 644.08it/s] 34%|███▍      | 79320/230850 [02:01<04:14, 595.95it/s] 34%|███▍      | 79403/230850 [02:01<03:50, 658.20it/s] 34%|███▍      | 79473/230850 [02:01<03:46, 667.84it/s] 34%|███▍      | 79541/230850 [02:01<03:54, 643.99it/s] 34%|███▍      | 79610/230850 [02:01<03:50, 656.68it/s] 35%|███▍      | 79684/230850 [02:01<03:48, 662.73it/s] 35%|███▍      | 79752/230850 [02:01<03:49, 659.39it/s] 35%|███▍      | 79832/230850 [02:01<03:39, 686.55it/s] 35%|███▍      | 79901/230850 [02:02<03:43, 676.40it/s] 35%|███▍      | 79988/230850 [02:02<03:36, 696.77it/s] 35%|███▍      | 80090/230850 [02:02<03:11, 785.52it/s] 35%|███▍      | 80170/230850 [02:02<03:24, 737.12it/s] 35%|███▍      | 80245/230850 [02:02<03:29, 719.68it/s] 35%|███▍      | 80318/230850 [02:02<03:47, 661.46it/s] 35%|███▍      | 80392/230850 [02:02<03:40, 681.71it/s] 35%|███▍      | 80462/230850 [02:02<03:57, 633.89it/s] 35%|███▍      | 80566/230850 [02:02<03:26, 727.31it/s] 35%|███▍      | 80647/230850 [02:03<03:21, 746.32it/s] 35%|███▍      | 80723/230850 [02:03<03:39, 683.96it/s] 35%|███▍      | 80793/230850 [02:03<03:42, 674.12it/s] 35%|███▌      | 80862/230850 [02:03<03:42, 672.67it/s] 35%|███▌      | 80930/230850 [02:03<03:44, 669.23it/s] 35%|███▌      | 80998/230850 [02:03<04:14, 588.10it/s] 35%|███▌      | 81059/230850 [02:03<04:27, 560.57it/s] 35%|███▌      | 81119/230850 [02:03<04:35, 542.97it/s] 35%|███▌      | 81208/230850 [02:04<03:59, 624.18it/s] 35%|███▌      | 81272/230850 [02:04<03:59, 625.80it/s] 35%|███▌      | 81336/230850 [02:04<04:00, 622.08it/s] 35%|███▌      | 81399/230850 [02:04<05:00, 497.37it/s] 35%|███▌      | 81453/230850 [02:04<04:57, 501.92it/s] 35%|███▌      | 81526/230850 [02:04<04:36, 539.96it/s] 35%|███▌      | 81585/230850 [02:04<04:30, 552.26it/s] 35%|███▌      | 81643/230850 [02:04<04:32, 546.73it/s] 35%|███▌      | 81699/230850 [02:04<04:55, 505.12it/s] 35%|███▌      | 81761/230850 [02:05<04:39, 533.90it/s] 35%|███▌      | 81824/230850 [02:05<04:32, 547.16it/s] 35%|███▌      | 81900/230850 [02:05<04:13, 588.43it/s] 36%|███▌      | 81961/230850 [02:05<04:10, 593.86it/s] 36%|███▌      | 82021/230850 [02:05<04:27, 556.41it/s] 36%|███▌      | 82078/230850 [02:05<04:37, 536.33it/s] 36%|███▌      | 82133/230850 [02:05<05:09, 480.61it/s] 36%|███▌      | 82202/230850 [02:05<04:43, 524.44it/s] 36%|███▌      | 82278/230850 [02:05<04:19, 572.29it/s] 36%|███▌      | 82337/230850 [02:06<04:32, 544.11it/s] 36%|███▌      | 82402/230850 [02:06<04:35, 538.56it/s] 36%|███▌      | 82472/230850 [02:06<04:18, 573.86it/s] 36%|███▌      | 82544/230850 [02:06<04:12, 587.48it/s] 36%|███▌      | 82604/230850 [02:06<04:20, 569.60it/s] 36%|███▌      | 82662/230850 [02:06<04:21, 565.78it/s] 36%|███▌      | 82752/230850 [02:06<03:46, 654.19it/s] 36%|███▌      | 82828/230850 [02:06<03:38, 678.54it/s] 36%|███▌      | 82897/230850 [02:06<03:39, 674.19it/s] 36%|███▌      | 82965/230850 [02:07<03:48, 646.19it/s] 36%|███▌      | 83043/230850 [02:07<03:38, 677.70it/s] 36%|███▌      | 83112/230850 [02:07<03:37, 679.64it/s] 36%|███▌      | 83181/230850 [02:07<03:57, 622.15it/s] 36%|███▌      | 83245/230850 [02:07<04:05, 600.27it/s] 36%|███▌      | 83339/230850 [02:07<03:33, 690.38it/s] 36%|███▌      | 83410/230850 [02:07<03:34, 686.12it/s] 36%|███▌      | 83480/230850 [02:07<03:40, 667.83it/s] 36%|███▌      | 83548/230850 [02:07<03:53, 631.90it/s] 36%|███▌      | 83616/230850 [02:08<03:48, 643.13it/s] 36%|███▋      | 83696/230850 [02:08<03:38, 674.67it/s] 36%|███▋      | 83769/230850 [02:08<03:33, 690.05it/s] 36%|███▋      | 83839/230850 [02:08<04:16, 573.65it/s] 36%|███▋      | 83900/230850 [02:08<04:19, 567.36it/s] 36%|███▋      | 83985/230850 [02:08<03:53, 629.54it/s] 36%|███▋      | 84066/230850 [02:08<03:44, 654.78it/s] 36%|███▋      | 84135/230850 [02:08<03:41, 663.67it/s] 36%|███▋      | 84209/230850 [02:09<03:36, 676.53it/s] 37%|███▋      | 84324/230850 [02:09<03:08, 775.35it/s] 37%|███▋      | 84402/230850 [02:09<03:24, 717.09it/s] 37%|███▋      | 84475/230850 [02:09<03:46, 645.07it/s] 37%|███▋      | 84541/230850 [02:09<04:02, 602.98it/s] 37%|███▋      | 84615/230850 [02:09<03:49, 637.35it/s] 37%|███▋      | 84681/230850 [02:09<03:58, 611.80it/s] 37%|███▋      | 84744/230850 [02:09<03:59, 611.06it/s] 37%|███▋      | 84814/230850 [02:09<03:50, 633.40it/s] 37%|███▋      | 84879/230850 [02:10<03:56, 618.12it/s] 37%|███▋      | 84948/230850 [02:10<03:49, 635.28it/s] 37%|███▋      | 85012/230850 [02:10<03:59, 609.93it/s] 37%|███▋      | 85080/230850 [02:10<03:52, 627.82it/s] 37%|███▋      | 85144/230850 [02:10<04:22, 555.10it/s] 37%|███▋      | 85206/230850 [02:10<04:14, 571.74it/s] 37%|███▋      | 85265/230850 [02:10<04:15, 570.09it/s] 37%|███▋      | 85354/230850 [02:10<03:48, 637.83it/s] 37%|███▋      | 85419/230850 [02:10<03:53, 621.87it/s] 37%|███▋      | 85482/230850 [02:11<04:00, 604.52it/s] 37%|███▋      | 85548/230850 [02:11<03:56, 613.64it/s] 37%|███▋      | 85624/230850 [02:11<03:50, 629.94it/s] 37%|███▋      | 85688/230850 [02:11<04:05, 590.19it/s] 37%|███▋      | 85756/230850 [02:11<04:07, 585.65it/s] 37%|███▋      | 85838/230850 [02:11<03:43, 648.36it/s] 37%|███▋      | 85916/230850 [02:11<03:37, 666.07it/s] 37%|███▋      | 85994/230850 [02:11<03:28, 693.23it/s] 37%|███▋      | 86071/230850 [02:11<03:22, 714.67it/s] 37%|███▋      | 86145/230850 [02:12<03:26, 699.87it/s] 37%|███▋      | 86216/230850 [02:12<03:33, 677.17it/s] 37%|███▋      | 86285/230850 [02:12<03:57, 609.34it/s] 37%|███▋      | 86348/230850 [02:12<04:01, 599.32it/s] 37%|███▋      | 86409/230850 [02:12<04:16, 562.05it/s] 37%|███▋      | 86472/230850 [02:12<04:13, 570.22it/s] 37%|███▋      | 86530/230850 [02:12<04:20, 553.84it/s] 38%|███▊      | 86589/230850 [02:12<04:17, 560.86it/s] 38%|███▊      | 86657/230850 [02:12<04:03, 592.36it/s] 38%|███▊      | 86722/230850 [02:13<03:57, 605.75it/s] 38%|███▊      | 86783/230850 [02:13<03:58, 602.80it/s] 38%|███▊      | 86844/230850 [02:13<04:10, 575.29it/s] 38%|███▊      | 86913/230850 [02:13<03:59, 601.66it/s] 38%|███▊      | 86980/230850 [02:13<03:54, 612.90it/s] 38%|███▊      | 87048/230850 [02:13<03:47, 632.07it/s] 38%|███▊      | 87130/230850 [02:13<03:40, 652.27it/s] 38%|███▊      | 87196/230850 [02:13<03:54, 612.48it/s] 38%|███▊      | 87258/230850 [02:13<03:56, 606.65it/s] 38%|███▊      | 87331/230850 [02:14<03:47, 630.12it/s] 38%|███▊      | 87428/230850 [02:14<03:23, 703.63it/s] 38%|███▊      | 87499/230850 [02:14<03:45, 635.08it/s] 38%|███▊      | 87594/230850 [02:14<03:19, 718.53it/s] 38%|███▊      | 87668/230850 [02:14<03:39, 653.34it/s] 38%|███▊      | 87736/230850 [02:14<03:40, 649.18it/s] 38%|███▊      | 87803/230850 [02:14<03:52, 616.29it/s] 38%|███▊      | 87866/230850 [02:14<04:01, 591.43it/s] 38%|███▊      | 87926/230850 [02:15<04:16, 556.40it/s] 38%|███▊      | 88014/230850 [02:15<03:43, 640.42it/s] 38%|███▊      | 88087/230850 [02:15<03:36, 659.57it/s] 38%|███▊      | 88155/230850 [02:15<03:42, 640.87it/s] 38%|███▊      | 88248/230850 [02:15<03:17, 720.22it/s] 38%|███▊      | 88322/230850 [02:15<03:23, 698.96it/s] 38%|███▊      | 88398/230850 [02:15<03:21, 708.37it/s] 38%|███▊      | 88470/230850 [02:15<03:35, 661.76it/s] 38%|███▊      | 88540/230850 [02:15<03:40, 645.76it/s] 38%|███▊      | 88606/230850 [02:15<03:39, 648.78it/s] 38%|███▊      | 88672/230850 [02:16<03:49, 619.30it/s] 38%|███▊      | 88735/230850 [02:16<03:54, 606.02it/s] 38%|███▊      | 88800/230850 [02:16<03:49, 617.84it/s] 38%|███▊      | 88863/230850 [02:16<04:08, 570.38it/s] 39%|███▊      | 88921/230850 [02:16<04:10, 567.27it/s] 39%|███▊      | 88995/230850 [02:16<03:52, 610.26it/s] 39%|███▊      | 89057/230850 [02:16<03:52, 609.89it/s] 39%|███▊      | 89125/230850 [02:16<03:46, 626.70it/s] 39%|███▊      | 89201/230850 [02:16<03:33, 664.84it/s] 39%|███▊      | 89268/230850 [02:17<03:51, 611.95it/s] 39%|███▊      | 89339/230850 [02:17<03:50, 614.15it/s] 39%|███▊      | 89402/230850 [02:17<03:51, 611.05it/s] 39%|███▉      | 89500/230850 [02:17<03:17, 714.18it/s] 39%|███▉      | 89573/230850 [02:17<03:45, 627.53it/s] 39%|███▉      | 89661/230850 [02:17<03:23, 693.52it/s] 39%|███▉      | 89734/230850 [02:17<03:21, 700.09it/s] 39%|███▉      | 89806/230850 [02:17<03:26, 683.89it/s] 39%|███▉      | 89876/230850 [02:17<03:40, 639.27it/s] 39%|███▉      | 89961/230850 [02:18<03:24, 687.48it/s] 39%|███▉      | 90032/230850 [02:18<03:39, 642.60it/s] 39%|███▉      | 90098/230850 [02:18<03:54, 601.23it/s] 39%|███▉      | 90170/230850 [02:18<03:47, 619.70it/s] 39%|███▉      | 90250/230850 [02:18<03:31, 665.61it/s] 39%|███▉      | 90318/230850 [02:18<03:41, 634.51it/s] 39%|███▉      | 90383/230850 [02:18<03:54, 597.93it/s] 39%|███▉      | 90464/230850 [02:18<03:40, 635.97it/s] 39%|███▉      | 90539/230850 [02:19<03:31, 663.72it/s] 39%|███▉      | 90607/230850 [02:19<04:01, 581.25it/s] 39%|███▉      | 90668/230850 [02:19<05:02, 462.65it/s] 39%|███▉      | 90733/230850 [02:19<04:38, 502.92it/s] 39%|███▉      | 90788/230850 [02:19<04:48, 485.60it/s] 39%|███▉      | 90840/230850 [02:19<05:12, 447.69it/s] 39%|███▉      | 90901/230850 [02:19<04:52, 478.57it/s] 39%|███▉      | 90953/230850 [02:19<04:50, 481.66it/s] 39%|███▉      | 91003/230850 [02:20<04:53, 476.43it/s] 39%|███▉      | 91064/230850 [02:20<04:40, 499.04it/s] 39%|███▉      | 91129/230850 [02:20<04:18, 539.65it/s] 40%|███▉      | 91186/230850 [02:20<04:19, 538.86it/s] 40%|███▉      | 91244/230850 [02:20<04:16, 544.65it/s] 40%|███▉      | 91307/230850 [02:20<04:09, 559.34it/s] 40%|███▉      | 91364/230850 [02:20<04:20, 534.56it/s] 40%|███▉      | 91439/230850 [02:20<04:01, 577.60it/s] 40%|███▉      | 91498/230850 [02:20<04:19, 537.60it/s] 40%|███▉      | 91564/230850 [02:21<04:04, 570.24it/s] 40%|███▉      | 91622/230850 [02:21<04:39, 497.36it/s] 40%|███▉      | 91708/230850 [02:21<03:56, 588.15it/s] 40%|███▉      | 91770/230850 [02:21<04:11, 552.54it/s] 40%|███▉      | 91828/230850 [02:21<04:13, 548.83it/s] 40%|███▉      | 91907/230850 [02:21<03:48, 607.84it/s] 40%|███▉      | 91970/230850 [02:21<04:05, 566.19it/s] 40%|███▉      | 92030/230850 [02:21<04:02, 571.49it/s] 40%|███▉      | 92110/230850 [02:21<03:41, 627.54it/s] 40%|███▉      | 92174/230850 [02:22<03:43, 621.53it/s] 40%|███▉      | 92237/230850 [02:22<03:55, 588.41it/s] 40%|███▉      | 92306/230850 [02:22<03:44, 616.20it/s] 40%|████      | 92369/230850 [02:22<03:44, 616.25it/s] 40%|████      | 92432/230850 [02:22<03:44, 617.34it/s] 40%|████      | 92495/230850 [02:22<03:45, 614.06it/s] 40%|████      | 92581/230850 [02:22<03:25, 671.75it/s] 40%|████      | 92649/230850 [02:22<03:29, 658.80it/s] 40%|████      | 92715/230850 [02:22<03:30, 657.09it/s] 40%|████      | 92781/230850 [02:23<03:33, 646.87it/s] 40%|████      | 92846/230850 [02:23<03:40, 626.56it/s] 40%|████      | 92909/230850 [02:23<04:03, 566.38it/s] 40%|████      | 92991/230850 [02:23<03:37, 634.06it/s] 40%|████      | 93056/230850 [02:23<03:59, 574.17it/s] 40%|████      | 93116/230850 [02:23<04:03, 564.60it/s] 40%|████      | 93190/230850 [02:23<03:45, 611.33it/s] 40%|████      | 93261/230850 [02:23<03:35, 638.53it/s] 40%|████      | 93337/230850 [02:23<03:29, 657.28it/s] 40%|████      | 93404/230850 [02:24<03:33, 644.49it/s] 40%|████      | 93470/230850 [02:24<03:59, 573.82it/s] 41%|████      | 93548/230850 [02:24<03:41, 619.63it/s] 41%|████      | 93618/230850 [02:24<03:34, 641.22it/s] 41%|████      | 93684/230850 [02:24<04:06, 556.32it/s] 41%|████      | 93760/230850 [02:24<03:49, 597.91it/s] 41%|████      | 93835/230850 [02:24<03:40, 621.50it/s] 41%|████      | 93921/230850 [02:24<03:21, 680.65it/s] 41%|████      | 93991/230850 [02:25<03:31, 646.01it/s] 41%|████      | 94058/230850 [02:25<03:32, 644.69it/s] 41%|████      | 94135/230850 [02:25<03:25, 666.12it/s] 41%|████      | 94203/230850 [02:25<03:36, 631.38it/s] 41%|████      | 94267/230850 [02:25<03:43, 611.21it/s] 41%|████      | 94345/230850 [02:25<03:28, 654.84it/s] 41%|████      | 94412/230850 [02:25<03:30, 648.45it/s] 41%|████      | 94478/230850 [02:25<03:46, 603.16it/s] 41%|████      | 94545/230850 [02:25<03:41, 615.08it/s] 41%|████      | 94608/230850 [02:26<03:59, 568.63it/s] 41%|████      | 94678/230850 [02:26<03:46, 600.68it/s] 41%|████      | 94740/230850 [02:26<03:45, 603.64it/s] 41%|████      | 94802/230850 [02:26<03:56, 574.94it/s] 41%|████      | 94861/230850 [02:26<04:16, 531.16it/s] 41%|████      | 94924/230850 [02:26<04:04, 556.34it/s] 41%|████      | 94982/230850 [02:26<04:04, 555.09it/s] 41%|████      | 95070/230850 [02:26<03:35, 629.99it/s] 41%|████      | 95139/230850 [02:26<03:33, 636.82it/s] 41%|████      | 95218/230850 [02:27<03:19, 679.05it/s] 41%|████▏     | 95287/230850 [02:27<03:35, 630.38it/s] 41%|████▏     | 95369/230850 [02:27<03:18, 682.36it/s] 41%|████▏     | 95448/230850 [02:27<03:10, 709.74it/s] 41%|████▏     | 95520/230850 [02:27<03:18, 681.19it/s] 41%|████▏     | 95589/230850 [02:27<03:23, 664.99it/s] 41%|████▏     | 95657/230850 [02:27<03:57, 568.38it/s] 41%|████▏     | 95717/230850 [02:27<03:59, 564.05it/s] 41%|████▏     | 95781/230850 [02:27<03:51, 582.95it/s] 42%|████▏     | 95848/230850 [02:28<03:42, 606.59it/s] 42%|████▏     | 95951/230850 [02:28<03:06, 724.71it/s] 42%|████▏     | 96026/230850 [02:28<03:05, 728.56it/s] 42%|████▏     | 96101/230850 [02:28<03:20, 671.74it/s] 42%|████▏     | 96170/230850 [02:28<03:32, 633.46it/s] 42%|████▏     | 96235/230850 [02:28<03:53, 576.98it/s] 42%|████▏     | 96322/230850 [02:28<03:31, 635.46it/s] 42%|████▏     | 96388/230850 [02:28<03:30, 638.32it/s] 42%|████▏     | 96454/230850 [02:28<03:43, 600.53it/s] 42%|████▏     | 96535/230850 [02:29<03:27, 647.40it/s] 42%|████▏     | 96601/230850 [02:29<03:53, 573.80it/s] 42%|████▏     | 96664/230850 [02:29<03:52, 578.10it/s] 42%|████▏     | 96731/230850 [02:29<03:47, 588.96it/s] 42%|████▏     | 96822/230850 [02:29<03:20, 669.47it/s] 42%|████▏     | 96891/230850 [02:29<03:49, 583.99it/s] 42%|████▏     | 96956/230850 [02:29<03:44, 596.41it/s] 42%|████▏     | 97018/230850 [02:29<04:23, 508.40it/s] 42%|████▏     | 97076/230850 [02:30<04:19, 515.33it/s] 42%|████▏     | 97130/230850 [02:30<04:19, 515.96it/s] 42%|████▏     | 97196/230850 [02:30<04:05, 543.39it/s] 42%|████▏     | 97252/230850 [02:30<04:07, 539.63it/s] 42%|████▏     | 97321/230850 [02:30<03:49, 580.78it/s] 42%|████▏     | 97381/230850 [02:30<04:05, 543.14it/s] 42%|████▏     | 97440/230850 [02:30<04:10, 531.96it/s] 42%|████▏     | 97529/230850 [02:30<03:33, 623.19it/s] 42%|████▏     | 97601/230850 [02:30<03:26, 644.92it/s] 42%|████▏     | 97670/230850 [02:31<03:23, 655.88it/s] 42%|████▏     | 97737/230850 [02:31<03:35, 619.02it/s] 42%|████▏     | 97800/230850 [02:31<03:43, 594.52it/s] 42%|████▏     | 97861/230850 [02:31<03:50, 577.77it/s] 42%|████▏     | 97920/230850 [02:31<03:58, 557.07it/s] 42%|████▏     | 97998/230850 [02:31<03:35, 617.77it/s] 42%|████▏     | 98075/230850 [02:31<03:22, 656.97it/s] 43%|████▎     | 98142/230850 [02:31<03:34, 619.35it/s] 43%|████▎     | 98234/230850 [02:31<03:14, 680.81it/s] 43%|████▎     | 98311/230850 [02:32<03:08, 704.42it/s] 43%|████▎     | 98383/230850 [02:32<03:31, 627.33it/s] 43%|████▎     | 98451/230850 [02:32<03:26, 639.83it/s] 43%|████▎     | 98517/230850 [02:32<03:32, 622.62it/s] 43%|████▎     | 98581/230850 [02:32<04:01, 547.95it/s] 43%|████▎     | 98640/230850 [02:32<03:58, 554.30it/s] 43%|████▎     | 98698/230850 [02:32<03:56, 559.50it/s] 43%|████▎     | 98756/230850 [02:32<04:01, 546.44it/s] 43%|████▎     | 98812/230850 [02:33<04:10, 526.77it/s] 43%|████▎     | 98886/230850 [02:33<03:48, 578.30it/s] 43%|████▎     | 98945/230850 [02:33<04:06, 534.53it/s] 43%|████▎     | 99031/230850 [02:33<03:32, 620.79it/s] 43%|████▎     | 99095/230850 [02:33<03:39, 598.92it/s] 43%|████▎     | 99165/230850 [02:33<03:35, 610.23it/s] 43%|████▎     | 99232/230850 [02:33<03:34, 612.34it/s] 43%|████▎     | 99325/230850 [02:33<03:12, 684.77it/s] 43%|████▎     | 99395/230850 [02:33<03:32, 617.54it/s] 43%|████▎     | 99459/230850 [02:34<03:58, 549.91it/s] 43%|████▎     | 99556/230850 [02:34<03:24, 643.52it/s] 43%|████▎     | 99624/230850 [02:34<04:30, 485.61it/s] 43%|████▎     | 99680/230850 [02:34<04:24, 495.03it/s] 43%|████▎     | 99735/230850 [02:34<04:25, 494.63it/s] 43%|████▎     | 99808/230850 [02:34<03:58, 549.26it/s] 43%|████▎     | 99867/230850 [02:34<04:06, 531.45it/s] 43%|████▎     | 99930/230850 [02:34<03:56, 553.11it/s] 43%|████▎     | 99988/230850 [02:35<04:15, 512.76it/s] 43%|████▎     | 100071/230850 [02:35<03:45, 579.77it/s] 43%|████▎     | 100131/230850 [02:35<04:58, 438.37it/s] 43%|████▎     | 100188/230850 [02:35<04:42, 463.28it/s] 43%|████▎     | 100249/230850 [02:35<04:34, 475.57it/s] 43%|████▎     | 100301/230850 [02:35<04:42, 462.25it/s] 43%|████▎     | 100350/230850 [02:35<04:40, 466.02it/s] 44%|████▎     | 100424/230850 [02:36<04:18, 504.97it/s] 44%|████▎     | 100486/230850 [02:36<04:10, 520.76it/s] 44%|████▎     | 100557/230850 [02:36<03:53, 558.79it/s] 44%|████▎     | 100614/230850 [02:36<03:59, 544.30it/s] 44%|████▎     | 100670/230850 [02:36<04:03, 534.04it/s] 44%|████▎     | 100729/230850 [02:36<04:02, 535.75it/s] 44%|████▎     | 100797/230850 [02:36<03:48, 569.13it/s] 44%|████▎     | 100871/230850 [02:36<03:31, 615.46it/s] 44%|████▎     | 100938/230850 [02:36<03:27, 625.84it/s] 44%|████▍     | 101001/230850 [02:36<03:29, 620.16it/s] 44%|████▍     | 101081/230850 [02:37<03:13, 671.73it/s] 44%|████▍     | 101149/230850 [02:37<03:45, 576.01it/s] 44%|████▍     | 101210/230850 [02:37<03:49, 565.82it/s] 44%|████▍     | 101284/230850 [02:37<03:37, 596.23it/s] 44%|████▍     | 101346/230850 [02:37<03:52, 555.84it/s] 44%|████▍     | 101420/230850 [02:37<03:34, 603.67it/s] 44%|████▍     | 101492/230850 [02:37<03:28, 620.77it/s] 44%|████▍     | 101556/230850 [02:37<03:31, 612.02it/s] 44%|████▍     | 101636/230850 [02:38<03:14, 664.01it/s] 44%|████▍     | 101704/230850 [02:38<03:19, 646.47it/s] 44%|████▍     | 101773/230850 [02:38<03:22, 637.81it/s] 44%|████▍     | 101838/230850 [02:38<03:27, 620.95it/s] 44%|████▍     | 101907/230850 [02:38<03:23, 635.13it/s] 44%|████▍     | 101982/230850 [02:38<03:17, 652.86it/s] 44%|████▍     | 102054/230850 [02:38<03:16, 656.62it/s] 44%|████▍     | 102125/230850 [02:38<03:11, 671.05it/s] 44%|████▍     | 102193/230850 [02:38<03:40, 584.03it/s] 44%|████▍     | 102254/230850 [02:39<03:51, 556.25it/s] 44%|████▍     | 102320/230850 [02:39<03:43, 573.91it/s] 44%|████▍     | 102380/230850 [02:39<03:42, 578.07it/s] 44%|████▍     | 102451/230850 [02:39<03:31, 608.43it/s] 44%|████▍     | 102525/230850 [02:39<03:22, 635.17it/s] 44%|████▍     | 102590/230850 [02:39<03:28, 614.73it/s] 44%|████▍     | 102653/230850 [02:39<03:34, 598.99it/s] 44%|████▍     | 102727/230850 [02:39<03:28, 615.20it/s] 45%|████▍     | 102789/230850 [02:39<03:27, 616.34it/s] 45%|████▍     | 102851/230850 [02:39<03:28, 613.41it/s] 45%|████▍     | 102916/230850 [02:40<03:28, 614.47it/s] 45%|████▍     | 102984/230850 [02:40<03:25, 623.15it/s] 45%|████▍     | 103047/230850 [02:40<03:38, 585.62it/s] 45%|████▍     | 103135/230850 [02:40<03:11, 667.42it/s] 45%|████▍     | 103209/230850 [02:40<03:07, 680.20it/s] 45%|████▍     | 103278/230850 [02:40<03:13, 660.98it/s] 45%|████▍     | 103345/230850 [02:40<03:36, 588.07it/s] 45%|████▍     | 103406/230850 [02:40<03:41, 575.23it/s] 45%|████▍     | 103468/230850 [02:41<03:37, 585.74it/s] 45%|████▍     | 103530/230850 [02:41<03:36, 588.36it/s] 45%|████▍     | 103597/230850 [02:41<03:36, 588.37it/s] 45%|████▍     | 103657/230850 [02:41<03:49, 553.67it/s] 45%|████▍     | 103717/230850 [02:41<03:46, 562.10it/s] 45%|████▍     | 103799/230850 [02:41<03:21, 631.00it/s] 45%|████▍     | 103863/230850 [02:41<03:50, 550.51it/s] 45%|████▌     | 103960/230850 [02:41<03:12, 657.84it/s] 45%|████▌     | 104029/230850 [02:41<03:16, 644.49it/s] 45%|████▌     | 104096/230850 [02:42<03:18, 639.99it/s] 45%|████▌     | 104162/230850 [02:42<03:33, 594.71it/s] 45%|████▌     | 104223/230850 [02:42<03:32, 595.01it/s] 45%|████▌     | 104284/230850 [02:42<03:41, 571.26it/s] 45%|████▌     | 104342/230850 [02:42<04:03, 519.70it/s] 45%|████▌     | 104399/230850 [02:42<04:00, 526.10it/s] 45%|████▌     | 104453/230850 [02:42<04:06, 513.69it/s] 45%|████▌     | 104514/230850 [02:42<03:58, 529.83it/s] 45%|████▌     | 104578/230850 [02:42<03:48, 553.30it/s] 45%|████▌     | 104644/230850 [02:43<03:38, 577.27it/s] 45%|████▌     | 104703/230850 [02:43<03:42, 568.13it/s] 45%|████▌     | 104770/230850 [02:43<03:31, 595.24it/s] 45%|████▌     | 104830/230850 [02:43<03:53, 539.84it/s] 45%|████▌     | 104904/230850 [02:43<03:39, 572.48it/s] 45%|████▌     | 104963/230850 [02:43<03:45, 558.34it/s] 45%|████▌     | 105034/230850 [02:43<03:33, 588.32it/s] 46%|████▌     | 105094/230850 [02:43<03:40, 569.16it/s] 46%|████▌     | 105152/230850 [02:43<03:42, 564.16it/s] 46%|████▌     | 105234/230850 [02:44<03:19, 630.28it/s] 46%|████▌     | 105298/230850 [02:44<03:35, 583.45it/s] 46%|████▌     | 105358/230850 [02:44<03:34, 585.97it/s] 46%|████▌     | 105434/230850 [02:44<03:18, 631.33it/s] 46%|████▌     | 105498/230850 [02:44<03:24, 612.36it/s] 46%|████▌     | 105572/230850 [02:44<03:20, 623.99it/s] 46%|████▌     | 105638/230850 [02:44<03:18, 631.69it/s] 46%|████▌     | 105716/230850 [02:44<03:08, 663.28it/s] 46%|████▌     | 105798/230850 [02:44<03:02, 684.15it/s] 46%|████▌     | 105867/230850 [02:45<03:03, 681.78it/s] 46%|████▌     | 105942/230850 [02:45<03:03, 681.49it/s] 46%|████▌     | 106011/230850 [02:45<03:14, 640.34it/s] 46%|████▌     | 106078/230850 [02:45<03:12, 648.36it/s] 46%|████▌     | 106144/230850 [02:45<03:18, 628.71it/s] 46%|████▌     | 106208/230850 [02:45<03:37, 573.26it/s] 46%|████▌     | 106293/230850 [02:45<03:16, 634.63it/s] 46%|████▌     | 106364/230850 [02:45<03:11, 651.10it/s] 46%|████▌     | 106431/230850 [02:45<03:23, 611.68it/s] 46%|████▌     | 106509/230850 [02:46<03:13, 643.22it/s] 46%|████▌     | 106581/230850 [02:46<03:09, 657.01it/s] 46%|████▌     | 106663/230850 [02:46<02:57, 698.91it/s] 46%|████▌     | 106734/230850 [02:46<02:57, 700.10it/s] 46%|████▋     | 106805/230850 [02:46<02:57, 700.19it/s] 46%|████▋     | 106876/230850 [02:46<03:10, 652.34it/s] 46%|████▋     | 106951/230850 [02:46<03:04, 670.03it/s] 46%|████▋     | 107019/230850 [02:46<03:06, 662.40it/s] 46%|████▋     | 107093/230850 [02:46<03:01, 683.53it/s] 46%|████▋     | 107166/230850 [02:47<02:59, 687.94it/s] 46%|████▋     | 107240/230850 [02:47<02:57, 696.95it/s] 46%|████▋     | 107310/230850 [02:47<03:03, 671.80it/s] 47%|████▋     | 107378/230850 [02:47<03:22, 608.34it/s] 47%|████▋     | 107441/230850 [02:47<03:32, 580.82it/s] 47%|████▋     | 107516/230850 [02:47<03:17, 625.67it/s] 47%|████▋     | 107580/230850 [02:47<03:24, 604.19it/s] 47%|████▋     | 107653/230850 [02:47<03:20, 615.09it/s] 47%|████▋     | 107716/230850 [02:47<03:54, 524.70it/s] 47%|████▋     | 107778/230850 [02:48<03:47, 541.13it/s] 47%|████▋     | 107850/230850 [02:48<03:40, 557.41it/s] 47%|████▋     | 107908/230850 [02:48<03:52, 529.68it/s] 47%|████▋     | 108002/230850 [02:48<03:14, 632.80it/s] 47%|████▋     | 108068/230850 [02:48<03:15, 628.82it/s] 47%|████▋     | 108136/230850 [02:48<03:12, 638.89it/s] 47%|████▋     | 108237/230850 [02:48<02:45, 740.40it/s] 47%|████▋     | 108313/230850 [02:48<02:59, 683.11it/s] 47%|████▋     | 108386/230850 [02:48<03:00, 678.35it/s] 47%|████▋     | 108456/230850 [02:49<03:08, 649.01it/s] 47%|████▋     | 108522/230850 [02:49<03:08, 649.69it/s] 47%|████▋     | 108588/230850 [02:49<04:41, 434.18it/s] 47%|████▋     | 108646/230850 [02:49<04:23, 463.75it/s] 47%|████▋     | 108710/230850 [02:49<04:02, 503.10it/s] 47%|████▋     | 108768/230850 [02:49<04:09, 489.42it/s] 47%|████▋     | 108833/230850 [02:49<03:52, 524.87it/s] 47%|████▋     | 108916/230850 [02:50<03:27, 589.04it/s] 47%|████▋     | 108987/230850 [02:50<03:22, 601.77it/s] 47%|████▋     | 109081/230850 [02:50<02:57, 686.62it/s] 47%|████▋     | 109153/230850 [02:50<03:05, 654.60it/s] 47%|████▋     | 109221/230850 [02:50<03:20, 606.09it/s] 47%|████▋     | 109284/230850 [02:50<03:48, 533.05it/s] 47%|████▋     | 109358/230850 [02:50<03:28, 581.45it/s] 47%|████▋     | 109436/230850 [02:50<03:14, 622.78it/s] 47%|████▋     | 109505/230850 [02:50<03:09, 640.40it/s] 47%|████▋     | 109571/230850 [02:51<03:30, 575.66it/s] 47%|████▋     | 109641/230850 [02:51<03:21, 602.53it/s] 48%|████▊     | 109704/230850 [02:51<03:30, 576.60it/s] 48%|████▊     | 109768/230850 [02:51<03:27, 584.10it/s] 48%|████▊     | 109843/230850 [02:51<03:14, 621.36it/s] 48%|████▊     | 109907/230850 [02:51<03:16, 615.25it/s] 48%|████▊     | 109978/230850 [02:51<03:09, 637.38it/s] 48%|████▊     | 110046/230850 [02:51<03:06, 647.87it/s] 48%|████▊     | 110112/230850 [02:51<03:11, 630.78it/s] 48%|████▊     | 110180/230850 [02:52<03:07, 644.73it/s] 48%|████▊     | 110245/230850 [02:52<03:15, 616.70it/s] 48%|████▊     | 110308/230850 [02:52<03:27, 579.76it/s] 48%|████▊     | 110384/230850 [02:52<03:12, 625.19it/s] 48%|████▊     | 110448/230850 [02:52<03:16, 613.49it/s] 48%|████▊     | 110510/230850 [02:52<03:25, 586.56it/s] 48%|████▊     | 110570/230850 [02:52<03:28, 578.08it/s] 48%|████▊     | 110640/230850 [02:52<03:16, 612.07it/s] 48%|████▊     | 110703/230850 [02:52<03:18, 606.24it/s] 48%|████▊     | 110785/230850 [02:53<03:04, 651.32it/s] 48%|████▊     | 110851/230850 [02:53<03:05, 646.27it/s] 48%|████▊     | 110917/230850 [02:53<03:04, 650.19it/s] 48%|████▊     | 110997/230850 [02:53<02:54, 688.32it/s] 48%|████▊     | 111066/230850 [02:53<02:59, 668.90it/s] 48%|████▊     | 111139/230850 [02:53<02:54, 686.31it/s] 48%|████▊     | 111208/230850 [02:53<03:16, 609.80it/s] 48%|████▊     | 111272/230850 [02:53<03:16, 609.61it/s] 48%|████▊     | 111354/230850 [02:53<02:59, 666.17it/s] 48%|████▊     | 111422/230850 [02:54<03:01, 659.22it/s] 48%|████▊     | 111491/230850 [02:54<03:01, 656.47it/s] 48%|████▊     | 111558/230850 [02:54<03:27, 574.99it/s] 48%|████▊     | 111618/230850 [02:54<03:25, 580.15it/s] 48%|████▊     | 111696/230850 [02:54<03:09, 629.08it/s] 48%|████▊     | 111761/230850 [02:54<03:14, 612.08it/s] 48%|████▊     | 111849/230850 [02:54<02:57, 668.70it/s] 48%|████▊     | 111917/230850 [02:54<03:21, 589.44it/s] 49%|████▊     | 111997/230850 [02:54<03:04, 643.75it/s] 49%|████▊     | 112064/230850 [02:55<03:04, 644.82it/s] 49%|████▊     | 112131/230850 [02:55<03:04, 642.68it/s] 49%|████▊     | 112197/230850 [02:55<03:04, 644.18it/s] 49%|████▊     | 112300/230850 [02:55<02:37, 753.82it/s] 49%|████▊     | 112377/230850 [02:55<02:44, 718.70it/s] 49%|████▊     | 112450/230850 [02:55<02:52, 687.90it/s] 49%|████▊     | 112520/230850 [02:55<02:57, 665.12it/s] 49%|████▉     | 112588/230850 [02:55<03:10, 620.20it/s] 49%|████▉     | 112685/230850 [02:55<02:49, 696.36it/s] 49%|████▉     | 112756/230850 [02:56<02:54, 675.74it/s] 49%|████▉     | 112825/230850 [02:56<03:03, 644.13it/s] 49%|████▉     | 112899/230850 [02:56<02:58, 659.50it/s] 49%|████▉     | 112972/230850 [02:56<02:56, 666.74it/s] 49%|████▉     | 113041/230850 [02:56<02:56, 666.67it/s] 49%|████▉     | 113110/230850 [02:56<02:55, 671.88it/s] 49%|████▉     | 113210/230850 [02:56<02:34, 763.61it/s] 49%|████▉     | 113287/230850 [02:56<02:53, 678.75it/s] 49%|████▉     | 113357/230850 [02:56<03:02, 643.86it/s] 49%|████▉     | 113423/230850 [02:57<03:16, 599.09it/s] 49%|████▉     | 113485/230850 [02:57<03:16, 598.12it/s] 49%|████▉     | 113563/230850 [02:57<03:04, 634.29it/s] 49%|████▉     | 113628/230850 [02:57<03:04, 636.82it/s] 49%|████▉     | 113693/230850 [02:57<03:44, 521.83it/s] 49%|████▉     | 113770/230850 [02:57<03:22, 578.37it/s] 49%|████▉     | 113836/230850 [02:57<03:15, 598.70it/s] 49%|████▉     | 113930/230850 [02:57<02:51, 682.83it/s] 49%|████▉     | 114001/230850 [02:58<03:10, 612.54it/s] 49%|████▉     | 114081/230850 [02:58<02:58, 652.88it/s] 49%|████▉     | 114164/230850 [02:58<02:46, 700.23it/s] 49%|████▉     | 114243/230850 [02:58<02:42, 719.70it/s] 50%|████▉     | 114317/230850 [02:58<02:49, 686.72it/s] 50%|████▉     | 114390/230850 [02:58<02:49, 686.53it/s] 50%|████▉     | 114460/230850 [02:58<03:00, 643.94it/s] 50%|████▉     | 114526/230850 [02:58<03:08, 617.69it/s] 50%|████▉     | 114597/230850 [02:58<03:02, 635.46it/s] 50%|████▉     | 114662/230850 [02:59<03:07, 620.84it/s] 50%|████▉     | 114725/230850 [02:59<03:11, 605.80it/s] 50%|████▉     | 114786/230850 [02:59<03:11, 606.77it/s] 50%|████▉     | 114847/230850 [02:59<03:14, 597.59it/s] 50%|████▉     | 114914/230850 [02:59<03:09, 613.13it/s] 50%|████▉     | 114992/230850 [02:59<02:56, 657.64it/s] 50%|████▉     | 115059/230850 [02:59<03:10, 607.77it/s] 50%|████▉     | 115132/230850 [02:59<03:02, 634.28it/s] 50%|████▉     | 115197/230850 [02:59<03:16, 589.91it/s] 50%|████▉     | 115263/230850 [03:00<03:10, 608.08it/s] 50%|████▉     | 115339/230850 [03:00<02:57, 649.66it/s] 50%|████▉     | 115405/230850 [03:00<03:16, 586.19it/s] 50%|█████     | 115483/230850 [03:00<03:00, 637.84it/s] 50%|█████     | 115560/230850 [03:00<02:52, 667.34it/s] 50%|█████     | 115631/230850 [03:00<02:51, 672.96it/s] 50%|█████     | 115705/230850 [03:00<02:52, 667.93it/s] 50%|█████     | 115777/230850 [03:00<02:51, 671.14it/s] 50%|█████     | 115847/230850 [03:00<02:52, 666.56it/s] 50%|█████     | 115931/230850 [03:01<02:41, 712.93it/s] 50%|█████     | 116003/230850 [03:01<02:47, 685.32it/s] 50%|█████     | 116073/230850 [03:01<02:48, 679.55it/s] 50%|█████     | 116142/230850 [03:01<02:50, 674.50it/s] 50%|█████     | 116225/230850 [03:01<02:40, 714.80it/s] 50%|█████     | 116297/230850 [03:01<02:42, 703.26it/s] 50%|█████     | 116369/230850 [03:01<02:44, 697.98it/s] 50%|█████     | 116439/230850 [03:01<02:52, 662.56it/s] 50%|█████     | 116538/230850 [03:01<02:32, 750.66it/s] 51%|█████     | 116614/230850 [03:02<02:46, 687.59it/s] 51%|█████     | 116685/230850 [03:02<02:52, 662.55it/s] 51%|█████     | 116753/230850 [03:02<03:02, 624.93it/s] 51%|█████     | 116753/230850 [03:02<02:58, 640.17it/s]
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/concurrent/futures/process.py", line 256, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 169, in process_TAF
    st_time = abs_packets[0]
              ~~~~~~~~~~~^^^
IndexError: index 0 is out of bounds for axis 0 with size 0
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 52, in <module>
    X_taf = data_processor.extract_TAF(X_taf)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 208, in extract_TAF
    index, result = future.result()
                    ^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
IndexError: index 0 is out of bounds for axis 0 with size 0
starting gen taf script for aug_valid
  0%|          | 0/25650 [00:00<?, ?it/s]  1%|          | 136/25650 [00:00<00:22, 1154.62it/s]  1%|          | 252/25650 [00:00<00:52, 484.55it/s]   1%|          | 319/25650 [00:00<01:04, 393.68it/s]  1%|▏         | 369/25650 [00:00<01:09, 361.78it/s]  2%|▏         | 411/25650 [00:01<01:12, 349.58it/s]  2%|▏         | 449/25650 [00:01<01:12, 349.94it/s]  2%|▏         | 486/25650 [00:01<01:16, 330.14it/s]  2%|▏         | 520/25650 [00:01<01:18, 321.83it/s]  2%|▏         | 556/25650 [00:01<01:15, 330.46it/s]  2%|▏         | 590/25650 [00:01<01:17, 322.96it/s]  2%|▏         | 623/25650 [00:01<01:22, 301.91it/s]  3%|▎         | 654/25650 [00:01<01:25, 291.28it/s]  3%|▎         | 685/25650 [00:01<01:25, 293.01it/s]  3%|▎         | 715/25650 [00:02<01:28, 281.42it/s]  3%|▎         | 752/25650 [00:02<01:23, 297.50it/s]  3%|▎         | 796/25650 [00:02<01:14, 335.68it/s]  3%|▎         | 831/25650 [00:02<01:13, 337.84it/s]  3%|▎         | 866/25650 [00:02<01:22, 300.07it/s]  3%|▎         | 897/25650 [00:02<01:22, 301.37it/s]  4%|▎         | 930/25650 [00:02<01:20, 308.44it/s]  4%|▍         | 962/25650 [00:02<01:21, 304.09it/s]  4%|▍         | 993/25650 [00:02<01:26, 284.49it/s]  4%|▍         | 1022/25650 [00:03<01:27, 281.78it/s]  4%|▍         | 1051/25650 [00:03<01:29, 275.01it/s]  4%|▍         | 1082/25650 [00:03<01:26, 283.61it/s]  4%|▍         | 1111/25650 [00:03<01:28, 278.74it/s]  4%|▍         | 1148/25650 [00:03<01:23, 292.77it/s]  5%|▍         | 1178/25650 [00:03<01:25, 284.88it/s]  5%|▍         | 1218/25650 [00:03<01:17, 314.67it/s]  5%|▍         | 1256/25650 [00:03<01:16, 320.29it/s]  5%|▌         | 1310/25650 [00:03<01:04, 377.40it/s]  5%|▌         | 1372/25650 [00:04<00:54, 446.04it/s]  6%|▌         | 1418/25650 [00:04<00:57, 421.06it/s]  6%|▌         | 1462/25650 [00:04<00:57, 423.65it/s]  6%|▌         | 1534/25650 [00:04<00:48, 492.68it/s]  6%|▌         | 1584/25650 [00:04<00:49, 484.95it/s]  6%|▋         | 1633/25650 [00:04<00:52, 460.10it/s]  7%|▋         | 1701/25650 [00:04<00:46, 509.83it/s]  7%|▋         | 1760/25650 [00:04<00:45, 525.00it/s]  7%|▋         | 1813/25650 [00:04<00:51, 461.74it/s]  7%|▋         | 1885/25650 [00:05<00:45, 521.75it/s]  8%|▊         | 1941/25650 [00:05<00:45, 520.02it/s]  8%|▊         | 2019/25650 [00:05<00:40, 587.92it/s]  8%|▊         | 2094/25650 [00:05<00:38, 616.81it/s]  8%|▊         | 2157/25650 [00:05<00:38, 604.69it/s]  9%|▉         | 2253/25650 [00:05<00:33, 694.21it/s]  9%|▉         | 2324/25650 [00:05<00:36, 641.61it/s]  9%|▉         | 2390/25650 [00:05<00:36, 634.17it/s] 10%|▉         | 2484/25650 [00:05<00:33, 687.55it/s] 10%|▉         | 2561/25650 [00:06<00:33, 698.11it/s] 10%|█         | 2651/25650 [00:06<00:31, 739.82it/s] 11%|█         | 2726/25650 [00:06<00:33, 694.26it/s] 11%|█         | 2797/25650 [00:06<00:33, 690.88it/s] 11%|█         | 2885/25650 [00:06<00:30, 743.12it/s] 12%|█▏        | 2967/25650 [00:06<00:30, 756.10it/s] 12%|█▏        | 3073/25650 [00:06<00:26, 838.63it/s] 12%|█▏        | 3158/25650 [00:06<00:28, 796.31it/s] 13%|█▎        | 3239/25650 [00:06<00:33, 661.96it/s] 13%|█▎        | 3319/25650 [00:07<00:33, 671.59it/s] 13%|█▎        | 3407/25650 [00:07<00:30, 724.67it/s] 14%|█▎        | 3483/25650 [00:07<00:31, 706.69it/s] 14%|█▍        | 3556/25650 [00:07<00:32, 689.01it/s] 14%|█▍        | 3627/25650 [00:07<00:33, 657.88it/s] 14%|█▍        | 3694/25650 [00:07<00:34, 640.40it/s] 15%|█▍        | 3765/25650 [00:07<00:33, 647.30it/s] 15%|█▍        | 3839/25650 [00:07<00:33, 660.31it/s] 15%|█▌        | 3915/25650 [00:07<00:31, 686.60it/s] 16%|█▌        | 3985/25650 [00:08<00:34, 620.73it/s] 16%|█▌        | 4049/25650 [00:08<00:34, 618.04it/s] 16%|█▌        | 4113/25650 [00:08<00:34, 623.44it/s] 16%|█▋        | 4192/25650 [00:08<00:32, 669.95it/s] 17%|█▋        | 4275/25650 [00:08<00:29, 713.29it/s] 17%|█▋        | 4362/25650 [00:08<00:28, 742.43it/s] 17%|█▋        | 4449/25650 [00:08<00:28, 753.22it/s] 18%|█▊        | 4525/25650 [00:08<00:28, 745.93it/s] 18%|█▊        | 4600/25650 [00:08<00:30, 701.05it/s] 18%|█▊        | 4671/25650 [00:09<00:32, 648.35it/s] 18%|█▊        | 4737/25650 [00:09<00:32, 650.57it/s] 19%|█▊        | 4803/25650 [00:09<00:33, 614.26it/s] 19%|█▉        | 4866/25650 [00:09<00:34, 603.00it/s] 19%|█▉        | 4946/25650 [00:09<00:31, 655.42it/s] 20%|█▉        | 5013/25650 [00:09<00:31, 654.00it/s] 20%|█▉        | 5079/25650 [00:09<00:31, 648.54it/s] 20%|██        | 5145/25650 [00:09<00:33, 618.63it/s] 20%|██        | 5208/25650 [00:09<00:36, 563.67it/s] 21%|██        | 5284/25650 [00:10<00:33, 601.75it/s] 21%|██        | 5352/25650 [00:10<00:32, 621.21it/s] 21%|██        | 5443/25650 [00:10<00:29, 681.19it/s] 22%|██▏       | 5520/25650 [00:10<00:28, 699.32it/s] 22%|██▏       | 5599/25650 [00:10<00:28, 707.69it/s] 22%|██▏       | 5671/25650 [00:10<00:33, 602.08it/s] 22%|██▏       | 5743/25650 [00:10<00:31, 625.46it/s] 23%|██▎       | 5808/25650 [00:10<00:32, 614.16it/s] 23%|██▎       | 5871/25650 [00:11<00:33, 596.25it/s] 23%|██▎       | 5942/25650 [00:11<00:32, 608.24it/s] 24%|██▎       | 6029/25650 [00:11<00:28, 679.53it/s] 24%|██▍       | 6099/25650 [00:11<00:30, 635.75it/s] 24%|██▍       | 6172/25650 [00:11<00:29, 660.55it/s] 24%|██▍       | 6240/25650 [00:11<00:30, 632.10it/s] 25%|██▍       | 6307/25650 [00:11<00:30, 642.50it/s] 25%|██▍       | 6395/25650 [00:11<00:27, 691.31it/s] 25%|██▌       | 6482/25650 [00:11<00:25, 739.08it/s] 26%|██▌       | 6557/25650 [00:12<00:28, 664.69it/s] 26%|██▌       | 6626/25650 [00:12<00:31, 610.73it/s] 26%|██▌       | 6689/25650 [00:12<00:32, 585.98it/s] 26%|██▋       | 6759/25650 [00:12<00:31, 600.86it/s] 27%|██▋       | 6826/25650 [00:12<00:30, 611.56it/s] 27%|██▋       | 6892/25650 [00:12<00:30, 624.57it/s] 27%|██▋       | 6956/25650 [00:12<00:32, 578.56it/s] 27%|██▋       | 7015/25650 [00:12<00:32, 577.13it/s] 28%|██▊       | 7080/25650 [00:12<00:31, 581.67it/s] 28%|██▊       | 7139/25650 [00:13<00:32, 569.56it/s] 28%|██▊       | 7209/25650 [00:13<00:30, 598.71it/s] 28%|██▊       | 7274/25650 [00:13<00:30, 612.24it/s] 29%|██▊       | 7369/25650 [00:13<00:26, 681.52it/s] 29%|██▉       | 7438/25650 [00:13<00:26, 675.94it/s] 29%|██▉       | 7506/25650 [00:13<00:27, 652.34it/s] 30%|██▉       | 7572/25650 [00:13<00:27, 653.38it/s] 30%|██▉       | 7638/25650 [00:13<00:28, 633.89it/s] 30%|███       | 7703/25650 [00:13<00:28, 638.33it/s] 30%|███       | 7767/25650 [00:14<00:30, 589.16it/s] 31%|███       | 7849/25650 [00:14<00:27, 643.70it/s] 31%|███       | 7920/25650 [00:14<00:27, 648.43it/s] 31%|███       | 7989/25650 [00:14<00:26, 654.27it/s] 31%|███▏      | 8068/25650 [00:14<00:25, 682.18it/s] 32%|███▏      | 8145/25650 [00:14<00:24, 704.16it/s] 32%|███▏      | 8216/25650 [00:14<00:25, 678.54it/s] 32%|███▏      | 8285/25650 [00:14<00:25, 669.49it/s] 33%|███▎      | 8354/25650 [00:14<00:26, 661.11it/s] 33%|███▎      | 8421/25650 [00:14<00:26, 642.96it/s] 33%|███▎      | 8486/25650 [00:15<00:27, 615.98it/s] 33%|███▎      | 8581/25650 [00:15<00:25, 665.55it/s] 34%|███▍      | 8663/25650 [00:15<00:24, 705.22it/s] 34%|███▍      | 8734/25650 [00:15<00:24, 680.40it/s] 34%|███▍      | 8820/25650 [00:15<00:23, 711.07it/s] 35%|███▍      | 8892/25650 [00:15<00:25, 667.24it/s] 35%|███▍      | 8971/25650 [00:15<00:23, 699.76it/s] 35%|███▌      | 9042/25650 [00:15<00:24, 664.78it/s] 36%|███▌      | 9124/25650 [00:16<00:23, 699.44it/s] 36%|███▌      | 9195/25650 [00:16<00:26, 630.94it/s] 36%|███▌      | 9260/25650 [00:16<00:25, 634.80it/s] 36%|███▋      | 9336/25650 [00:16<00:24, 659.48it/s] 37%|███▋      | 9426/25650 [00:16<00:22, 713.79it/s] 37%|███▋      | 9499/25650 [00:16<00:23, 685.22it/s] 37%|███▋      | 9569/25650 [00:16<00:24, 665.46it/s] 38%|███▊      | 9662/25650 [00:16<00:22, 712.63it/s] 38%|███▊      | 9734/25650 [00:16<00:24, 649.99it/s] 38%|███▊      | 9800/25650 [00:17<00:24, 644.24it/s] 38%|███▊      | 9866/25650 [00:17<00:25, 627.86it/s] 39%|███▉      | 9956/25650 [00:17<00:22, 697.31it/s] 39%|███▉      | 10057/25650 [00:17<00:20, 773.76it/s] 40%|███▉      | 10136/25650 [00:17<00:23, 657.36it/s] 40%|███▉      | 10206/25650 [00:17<00:24, 639.27it/s] 40%|████      | 10284/25650 [00:17<00:22, 674.80it/s] 40%|████      | 10355/25650 [00:17<00:22, 679.70it/s] 41%|████      | 10425/25650 [00:17<00:23, 659.70it/s] 41%|████      | 10508/25650 [00:18<00:22, 687.64it/s] 41%|████▏     | 10595/25650 [00:18<00:20, 725.55it/s] 42%|████▏     | 10675/25650 [00:18<00:20, 743.73it/s] 42%|████▏     | 10750/25650 [00:18<00:22, 671.42it/s] 42%|████▏     | 10819/25650 [00:18<00:23, 640.48it/s] 42%|████▏     | 10897/25650 [00:18<00:22, 667.34it/s] 43%|████▎     | 10965/25650 [00:18<00:23, 635.69it/s] 43%|████▎     | 11043/25650 [00:18<00:22, 657.43it/s] 43%|████▎     | 11126/25650 [00:18<00:20, 703.94it/s] 44%|████▍     | 11232/25650 [00:19<00:18, 799.74it/s] 44%|████▍     | 11314/25650 [00:19<00:18, 763.78it/s] 44%|████▍     | 11392/25650 [00:19<00:18, 765.84it/s] 45%|████▍     | 11470/25650 [00:19<00:20, 702.28it/s] 45%|████▍     | 11542/25650 [00:19<00:20, 684.88it/s] 45%|████▌     | 11612/25650 [00:19<00:20, 688.04it/s] 46%|████▌     | 11699/25650 [00:19<00:19, 730.28it/s] 46%|████▌     | 11773/25650 [00:19<00:20, 682.63it/s] 46%|████▌     | 11843/25650 [00:19<00:20, 686.53it/s] 46%|████▋     | 11925/25650 [00:20<00:19, 721.87it/s] 47%|████▋     | 11998/25650 [00:20<00:20, 679.00it/s] 47%|████▋     | 12075/25650 [00:20<00:20, 677.52it/s] 47%|████▋     | 12175/25650 [00:20<00:17, 762.09it/s] 48%|████▊     | 12263/25650 [00:20<00:16, 788.46it/s] 48%|████▊     | 12343/25650 [00:20<00:18, 729.10it/s] 48%|████▊     | 12418/25650 [00:20<00:20, 658.18it/s] 49%|████▊     | 12494/25650 [00:20<00:19, 682.49it/s] 49%|████▉     | 12564/25650 [00:21<00:19, 683.45it/s] 49%|████▉     | 12639/25650 [00:21<00:18, 692.73it/s] 50%|████▉     | 12710/25650 [00:21<00:18, 686.61it/s] 50%|████▉     | 12783/25650 [00:21<00:18, 695.18it/s] 50%|█████     | 12853/25650 [00:21<00:20, 628.55it/s] 50%|█████     | 12918/25650 [00:21<00:21, 598.30it/s] 51%|█████     | 12988/25650 [00:21<00:20, 617.32it/s] 51%|█████     | 13051/25650 [00:21<00:21, 598.73it/s] 51%|█████     | 13112/25650 [00:21<00:22, 560.96it/s] 51%|█████▏    | 13178/25650 [00:22<00:21, 582.94it/s] 52%|█████▏    | 13261/25650 [00:22<00:19, 630.95it/s] 52%|█████▏    | 13343/25650 [00:22<00:18, 680.50it/s] 52%|█████▏    | 13412/25650 [00:22<00:18, 670.48it/s] 53%|█████▎    | 13480/25650 [00:22<00:19, 624.52it/s] 53%|█████▎    | 13544/25650 [00:22<00:19, 611.61it/s] 53%|█████▎    | 13616/25650 [00:22<00:18, 640.18it/s] 53%|█████▎    | 13681/25650 [00:22<00:22, 542.53it/s] 54%|█████▎    | 13754/25650 [00:22<00:20, 589.20it/s] 54%|█████▍    | 13826/25650 [00:23<00:19, 620.74it/s] 54%|█████▍    | 13897/25650 [00:23<00:18, 634.61it/s] 54%|█████▍    | 13963/25650 [00:23<00:18, 625.32it/s] 55%|█████▍    | 14036/25650 [00:23<00:17, 650.87it/s] 55%|█████▍    | 14103/25650 [00:23<00:17, 648.13it/s] 55%|█████▌    | 14169/25650 [00:23<00:18, 619.91it/s] 56%|█████▌    | 14252/25650 [00:23<00:17, 663.22it/s] 56%|█████▌    | 14319/25650 [00:23<00:18, 598.58it/s] 56%|█████▌    | 14384/25650 [00:23<00:18, 601.44it/s] 56%|█████▋    | 14464/25650 [00:24<00:17, 635.73it/s] 57%|█████▋    | 14529/25650 [00:24<00:19, 580.79it/s] 57%|█████▋    | 14589/25650 [00:24<00:19, 564.90it/s] 57%|█████▋    | 14664/25650 [00:24<00:18, 587.78it/s] 57%|█████▋    | 14738/25650 [00:24<00:17, 625.87it/s] 58%|█████▊    | 14802/25650 [00:24<00:17, 613.82it/s] 58%|█████▊    | 14868/25650 [00:24<00:17, 607.47it/s] 58%|█████▊    | 14938/25650 [00:24<00:17, 599.90it/s] 59%|█████▊    | 15018/25650 [00:24<00:16, 654.43it/s] 59%|█████▉    | 15088/25650 [00:25<00:15, 662.76it/s] 59%|█████▉    | 15155/25650 [00:25<00:16, 646.95it/s] 59%|█████▉    | 15233/25650 [00:25<00:15, 667.67it/s] 60%|█████▉    | 15313/25650 [00:25<00:14, 689.49it/s] 60%|█████▉    | 15383/25650 [00:25<00:16, 640.96it/s] 60%|██████    | 15452/25650 [00:25<00:15, 641.71it/s] 60%|██████    | 15518/25650 [00:25<00:15, 646.61it/s] 61%|██████    | 15584/25650 [00:25<00:15, 648.73it/s] 61%|██████    | 15650/25650 [00:25<00:15, 648.89it/s] 61%|██████▏   | 15722/25650 [00:26<00:15, 660.38it/s] 62%|██████▏   | 15789/25650 [00:26<00:14, 659.50it/s] 62%|██████▏   | 15856/25650 [00:26<00:16, 597.38it/s] 62%|██████▏   | 15934/25650 [00:26<00:15, 636.02it/s] 62%|██████▏   | 16024/25650 [00:26<00:13, 707.87it/s] 63%|██████▎   | 16097/25650 [00:26<00:13, 686.30it/s] 63%|██████▎   | 16174/25650 [00:26<00:13, 691.49it/s] 63%|██████▎   | 16245/25650 [00:26<00:13, 689.71it/s] 64%|██████▎   | 16315/25650 [00:26<00:14, 645.05it/s] 64%|██████▍   | 16383/25650 [00:27<00:14, 648.86it/s] 64%|██████▍   | 16478/25650 [00:27<00:12, 730.07it/s] 65%|██████▍   | 16552/25650 [00:27<00:13, 658.36it/s] 65%|██████▍   | 16620/25650 [00:27<00:14, 632.86it/s] 65%|██████▌   | 16685/25650 [00:27<00:14, 623.00it/s] 65%|██████▌   | 16760/25650 [00:27<00:13, 645.75it/s] 66%|██████▌   | 16826/25650 [00:27<00:13, 634.44it/s] 66%|██████▌   | 16894/25650 [00:27<00:13, 626.93it/s] 66%|██████▌   | 16958/25650 [00:27<00:15, 571.27it/s] 66%|██████▋   | 17050/25650 [00:28<00:13, 652.63it/s] 67%|██████▋   | 17117/25650 [00:28<00:13, 634.98it/s] 67%|██████▋   | 17182/25650 [00:28<00:13, 620.54it/s] 67%|██████▋   | 17248/25650 [00:28<00:13, 625.36it/s] 68%|██████▊   | 17315/25650 [00:28<00:13, 633.15it/s] 68%|██████▊   | 17396/25650 [00:28<00:12, 683.32it/s] 68%|██████▊   | 17465/25650 [00:28<00:12, 661.39it/s] 68%|██████▊   | 17532/25650 [00:28<00:14, 565.67it/s] 69%|██████▊   | 17603/25650 [00:28<00:13, 600.14it/s] 69%|██████▉   | 17689/25650 [00:29<00:12, 649.87it/s] 69%|██████▉   | 17756/25650 [00:29<00:13, 582.54it/s] 69%|██████▉   | 17824/25650 [00:29<00:12, 606.06it/s] 70%|██████▉   | 17895/25650 [00:29<00:12, 628.67it/s] 70%|███████   | 17976/25650 [00:29<00:11, 660.86it/s] 70%|███████   | 18044/25650 [00:29<00:11, 653.94it/s] 71%|███████   | 18114/25650 [00:29<00:11, 650.09it/s] 71%|███████   | 18195/25650 [00:29<00:10, 685.12it/s] 71%|███████   | 18267/25650 [00:29<00:10, 694.52it/s] 71%|███████▏  | 18337/25650 [00:30<00:10, 691.38it/s] 72%|███████▏  | 18407/25650 [00:30<00:10, 686.75it/s] 72%|███████▏  | 18476/25650 [00:30<00:10, 678.82it/s] 72%|███████▏  | 18565/25650 [00:30<00:09, 732.25it/s] 73%|███████▎  | 18639/25650 [00:30<00:09, 723.69it/s] 73%|███████▎  | 18712/25650 [00:30<00:10, 662.15it/s] 73%|███████▎  | 18801/25650 [00:30<00:09, 724.04it/s] 74%|███████▎  | 18875/25650 [00:30<00:10, 654.95it/s] 74%|███████▍  | 18946/25650 [00:30<00:10, 667.28it/s] 74%|███████▍  | 19015/25650 [00:31<00:09, 667.93it/s] 74%|███████▍  | 19083/25650 [00:31<00:11, 578.54it/s] 75%|███████▍  | 19144/25650 [00:31<00:11, 561.10it/s] 75%|███████▍  | 19217/25650 [00:31<00:10, 598.01it/s] 75%|███████▌  | 19286/25650 [00:31<00:11, 577.07it/s] 75%|███████▌  | 19355/25650 [00:31<00:10, 606.29it/s] 76%|███████▌  | 19440/25650 [00:31<00:09, 668.74it/s] 76%|███████▌  | 19509/25650 [00:31<00:10, 612.40it/s] 76%|███████▋  | 19573/25650 [00:32<00:10, 567.09it/s] 77%|███████▋  | 19632/25650 [00:32<00:10, 558.75it/s] 77%|███████▋  | 19697/25650 [00:32<00:10, 581.23it/s] 77%|███████▋  | 19780/25650 [00:32<00:09, 616.26it/s] 77%|███████▋  | 19843/25650 [00:32<00:09, 597.47it/s] 78%|███████▊  | 19932/25650 [00:32<00:08, 667.63it/s] 78%|███████▊  | 20000/25650 [00:32<00:08, 645.47it/s] 78%|███████▊  | 20071/25650 [00:32<00:08, 650.71it/s] 79%|███████▊  | 20137/25650 [00:32<00:09, 589.75it/s] 79%|███████▉  | 20221/25650 [00:33<00:08, 647.37it/s] 79%|███████▉  | 20288/25650 [00:33<00:08, 633.07it/s] 79%|███████▉  | 20353/25650 [00:33<00:08, 612.26it/s] 80%|███████▉  | 20429/25650 [00:33<00:08, 643.98it/s] 80%|███████▉  | 20505/25650 [00:33<00:07, 658.08it/s] 80%|████████  | 20587/25650 [00:33<00:07, 691.02it/s] 81%|████████  | 20657/25650 [00:33<00:07, 681.45it/s] 81%|████████  | 20726/25650 [00:33<00:07, 621.90it/s] 81%|████████  | 20811/25650 [00:33<00:07, 677.47it/s] 81%|████████▏ | 20880/25650 [00:34<00:07, 675.10it/s] 82%|████████▏ | 20959/25650 [00:34<00:06, 707.28it/s] 82%|████████▏ | 21038/25650 [00:34<00:06, 717.85it/s] 82%|████████▏ | 21111/25650 [00:34<00:06, 656.48it/s] 83%|████████▎ | 21178/25650 [00:34<00:06, 654.15it/s] 83%|████████▎ | 21251/25650 [00:34<00:06, 673.00it/s] 83%|████████▎ | 21320/25650 [00:34<00:06, 670.84it/s] 83%|████████▎ | 21388/25650 [00:34<00:06, 669.45it/s] 84%|████████▎ | 21456/25650 [00:34<00:06, 670.42it/s] 84%|████████▍ | 21524/25650 [00:35<00:06, 617.72it/s] 84%|████████▍ | 21614/25650 [00:35<00:06, 670.91it/s] 85%|████████▍ | 21698/25650 [00:35<00:05, 696.88it/s] 85%|████████▍ | 21769/25650 [00:35<00:05, 696.42it/s] 85%|████████▌ | 21839/25650 [00:35<00:05, 635.35it/s] 85%|████████▌ | 21915/25650 [00:35<00:05, 638.05it/s] 86%|████████▌ | 21980/25650 [00:35<00:05, 623.07it/s] 86%|████████▌ | 22049/25650 [00:35<00:05, 630.96it/s] 86%|████████▌ | 22113/25650 [00:35<00:06, 583.53it/s] 86%|████████▋ | 22179/25650 [00:36<00:05, 603.55it/s] 87%|████████▋ | 22260/25650 [00:36<00:05, 659.86it/s] 87%|████████▋ | 22336/25650 [00:36<00:04, 686.86it/s] 87%|████████▋ | 22406/25650 [00:36<00:04, 680.12it/s] 88%|████████▊ | 22475/25650 [00:36<00:04, 661.62it/s] 88%|████████▊ | 22542/25650 [00:36<00:05, 613.05it/s] 88%|████████▊ | 22605/25650 [00:36<00:04, 612.35it/s] 88%|████████▊ | 22691/25650 [00:36<00:04, 669.96it/s] 89%|████████▊ | 22759/25650 [00:36<00:04, 614.43it/s] 89%|████████▉ | 22822/25650 [00:37<00:04, 578.55it/s] 89%|████████▉ | 22881/25650 [00:37<00:05, 545.52it/s] 89%|████████▉ | 22953/25650 [00:37<00:04, 586.44it/s] 90%|████████▉ | 23020/25650 [00:37<00:04, 597.58it/s] 90%|█████████ | 23101/25650 [00:37<00:03, 640.04it/s] 90%|█████████ | 23176/25650 [00:37<00:03, 670.27it/s] 91%|█████████ | 23244/25650 [00:37<00:03, 649.30it/s] 91%|█████████ | 23310/25650 [00:37<00:03, 600.75it/s] 91%|█████████ | 23375/25650 [00:37<00:03, 611.80it/s] 91%|█████████▏| 23437/25650 [00:38<00:03, 569.54it/s] 92%|█████████▏| 23495/25650 [00:38<00:04, 532.93it/s] 92%|█████████▏| 23550/25650 [00:38<00:04, 468.90it/s] 92%|█████████▏| 23629/25650 [00:38<00:03, 547.47it/s] 92%|█████████▏| 23687/25650 [00:38<00:03, 499.88it/s] 93%|█████████▎| 23751/25650 [00:38<00:03, 527.87it/s] 93%|█████████▎| 23806/25650 [00:38<00:03, 525.37it/s] 93%|█████████▎| 23861/25650 [00:38<00:03, 487.48it/s] 93%|█████████▎| 23938/25650 [00:39<00:03, 554.20it/s] 94%|█████████▎| 23996/25650 [00:39<00:03, 522.00it/s] 94%|█████████▍| 24050/25650 [00:39<00:03, 518.31it/s] 94%|█████████▍| 24111/25650 [00:39<00:02, 542.22it/s] 94%|█████████▍| 24185/25650 [00:39<00:02, 578.86it/s] 95%|█████████▍| 24278/25650 [00:39<00:02, 650.57it/s] 95%|█████████▍| 24344/25650 [00:39<00:02, 634.26it/s] 95%|█████████▌| 24409/25650 [00:39<00:01, 637.05it/s] 95%|█████████▌| 24477/25650 [00:39<00:01, 648.03it/s] 96%|█████████▌| 24542/25650 [00:40<00:01, 603.29it/s] 96%|█████████▌| 24612/25650 [00:40<00:01, 626.54it/s] 96%|█████████▌| 24676/25650 [00:40<00:01, 602.11it/s] 96%|█████████▋| 24746/25650 [00:40<00:01, 595.55it/s] 97%|█████████▋| 24814/25650 [00:40<00:01, 609.88it/s] 97%|█████████▋| 24887/25650 [00:40<00:01, 635.90it/s] 97%|█████████▋| 24961/25650 [00:40<00:01, 664.36it/s] 98%|█████████▊| 25028/25650 [00:40<00:00, 654.69it/s] 98%|█████████▊| 25094/25650 [00:40<00:00, 636.53it/s] 98%|█████████▊| 25160/25650 [00:41<00:00, 638.81it/s] 98%|█████████▊| 25225/25650 [00:41<00:00, 635.79it/s] 99%|█████████▊| 25293/25650 [00:41<00:00, 636.17it/s] 99%|█████████▉| 25357/25650 [00:41<00:00, 588.34it/s] 99%|█████████▉| 25421/25650 [00:41<00:00, 600.72it/s] 99%|█████████▉| 25493/25650 [00:41<00:00, 631.51it/s]100%|█████████▉| 25559/25650 [00:41<00:00, 632.10it/s]100%|█████████▉| 25630/25650 [00:41<00:00, 639.00it/s]100%|██████████| 25650/25650 [00:41<00:00, 612.63it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 71, in <module>
    predicted_size = predict_npz_file_size(data_dict, verbose=True)
                                           ^^^^^^^^^
NameError: name 'data_dict' is not defined
aug_valid process done: X = (25650, 3, 2, 2000), y = (25650,)
starting gen taf script for test
  0%|          | 0/9500 [00:00<?, ?it/s]  1%|          | 49/9500 [00:00<00:24, 381.69it/s]  1%|          | 88/9500 [00:00<00:33, 276.89it/s]  1%|          | 117/9500 [00:00<00:40, 230.46it/s]  1%|▏         | 141/9500 [00:00<00:42, 220.58it/s]  2%|▏         | 172/9500 [00:00<00:38, 245.15it/s]  2%|▏         | 198/9500 [00:00<00:41, 224.80it/s]  2%|▏         | 230/9500 [00:00<00:37, 250.41it/s]  3%|▎         | 258/9500 [00:01<00:36, 255.28it/s]  3%|▎         | 285/9500 [00:01<00:36, 252.10it/s]  3%|▎         | 315/9500 [00:01<00:34, 264.50it/s]  4%|▎         | 344/9500 [00:01<00:33, 270.77it/s]  4%|▍         | 372/9500 [00:01<00:36, 251.25it/s]  4%|▍         | 398/9500 [00:01<00:38, 236.06it/s]  4%|▍         | 423/9500 [00:01<00:40, 223.53it/s]  5%|▍         | 446/9500 [00:01<00:44, 202.92it/s]  5%|▍         | 474/9500 [00:01<00:40, 221.34it/s]  5%|▌         | 497/9500 [00:02<00:43, 207.10it/s]  5%|▌         | 519/9500 [00:02<00:42, 209.59it/s]  6%|▌         | 546/9500 [00:02<00:40, 222.42it/s]  6%|▌         | 569/9500 [00:02<00:40, 222.82it/s]  6%|▋         | 595/9500 [00:02<00:38, 232.06it/s]  7%|▋         | 619/9500 [00:02<00:40, 219.36it/s]  7%|▋         | 649/9500 [00:02<00:37, 239.10it/s]  7%|▋         | 674/9500 [00:02<00:39, 225.78it/s]  7%|▋         | 697/9500 [00:02<00:39, 221.72it/s]  8%|▊         | 720/9500 [00:03<00:40, 216.87it/s]  8%|▊         | 742/9500 [00:03<00:42, 203.75it/s]  8%|▊         | 770/9500 [00:03<00:39, 221.40it/s]  8%|▊         | 794/9500 [00:03<00:39, 217.77it/s]  9%|▊         | 822/9500 [00:03<00:38, 226.13it/s]  9%|▉         | 854/9500 [00:03<00:35, 243.86it/s]  9%|▉         | 879/9500 [00:03<00:36, 237.68it/s] 10%|▉         | 911/9500 [00:03<00:33, 255.20it/s] 10%|▉         | 937/9500 [00:03<00:34, 249.36it/s] 10%|█         | 964/9500 [00:04<00:35, 243.24it/s] 10%|█         | 989/9500 [00:04<00:37, 224.92it/s] 11%|█         | 1012/9500 [00:04<00:37, 223.89it/s] 11%|█         | 1063/9500 [00:04<00:28, 293.54it/s] 12%|█▏        | 1104/9500 [00:04<00:26, 319.82it/s] 12%|█▏        | 1151/9500 [00:04<00:24, 347.74it/s] 12%|█▏        | 1186/9500 [00:04<00:27, 306.70it/s] 13%|█▎        | 1255/9500 [00:04<00:20, 393.53it/s] 14%|█▎        | 1300/9500 [00:05<00:20, 402.89it/s] 14%|█▍        | 1342/9500 [00:05<00:24, 327.28it/s] 15%|█▍        | 1414/9500 [00:05<00:19, 413.33it/s] 15%|█▌        | 1462/9500 [00:05<00:18, 423.60it/s] 16%|█▌        | 1508/9500 [00:05<00:20, 388.73it/s] 16%|█▋        | 1556/9500 [00:05<00:19, 402.54it/s] 17%|█▋        | 1626/9500 [00:05<00:16, 469.41it/s] 18%|█▊        | 1675/9500 [00:05<00:17, 441.24it/s] 18%|█▊        | 1724/9500 [00:06<00:17, 449.13it/s] 19%|█▊        | 1771/9500 [00:06<00:17, 443.58it/s] 19%|█▉        | 1828/9500 [00:06<00:16, 477.32it/s] 20%|█▉        | 1885/9500 [00:06<00:15, 499.32it/s] 20%|██        | 1936/9500 [00:06<00:15, 478.92it/s] 21%|██        | 1985/9500 [00:06<00:15, 469.98it/s] 21%|██▏       | 2033/9500 [00:06<00:16, 451.05it/s] 22%|██▏       | 2079/9500 [00:06<00:17, 433.71it/s] 22%|██▏       | 2123/9500 [00:06<00:17, 410.49it/s] 23%|██▎       | 2165/9500 [00:07<00:17, 408.70it/s] 23%|██▎       | 2231/9500 [00:07<00:15, 476.24it/s] 24%|██▍       | 2280/9500 [00:07<00:15, 456.05it/s] 24%|██▍       | 2327/9500 [00:07<00:16, 436.46it/s] 25%|██▍       | 2372/9500 [00:07<00:16, 431.73it/s] 26%|██▌       | 2426/9500 [00:07<00:15, 451.13it/s] 26%|██▌       | 2486/9500 [00:07<00:14, 480.69it/s] 27%|██▋       | 2555/9500 [00:07<00:13, 530.41it/s] 27%|██▋       | 2609/9500 [00:07<00:13, 498.87it/s] 28%|██▊       | 2660/9500 [00:08<00:13, 495.97it/s] 29%|██▊       | 2710/9500 [00:08<00:13, 485.89it/s] 29%|██▉       | 2763/9500 [00:08<00:13, 491.95it/s] 30%|██▉       | 2813/9500 [00:08<00:14, 460.51it/s] 30%|███       | 2865/9500 [00:08<00:14, 465.25it/s] 31%|███       | 2912/9500 [00:08<00:15, 426.09it/s] 31%|███       | 2956/9500 [00:08<00:15, 420.60it/s] 32%|███▏      | 3005/9500 [00:08<00:16, 389.73it/s] 32%|███▏      | 3074/9500 [00:08<00:13, 466.21it/s] 33%|███▎      | 3130/9500 [00:09<00:13, 485.16it/s] 34%|███▎      | 3188/9500 [00:09<00:13, 483.63it/s] 34%|███▍      | 3246/9500 [00:09<00:12, 507.42it/s] 35%|███▍      | 3298/9500 [00:09<00:12, 477.23it/s] 35%|███▌      | 3356/9500 [00:09<00:12, 497.03it/s] 36%|███▌      | 3408/9500 [00:09<00:12, 503.23it/s] 36%|███▋      | 3464/9500 [00:09<00:12, 485.14it/s] 37%|███▋      | 3514/9500 [00:09<00:12, 479.71it/s] 38%|███▊      | 3567/9500 [00:09<00:12, 491.68it/s] 38%|███▊      | 3617/9500 [00:10<00:12, 485.45it/s] 39%|███▊      | 3666/9500 [00:10<00:12, 479.83it/s] 39%|███▉      | 3715/9500 [00:10<00:12, 449.58it/s] 40%|███▉      | 3761/9500 [00:10<00:12, 443.64it/s] 40%|████      | 3806/9500 [00:10<00:14, 387.35it/s] 41%|████      | 3894/9500 [00:10<00:11, 507.95it/s] 42%|████▏     | 3948/9500 [00:10<00:12, 448.98it/s] 42%|████▏     | 4006/9500 [00:10<00:11, 474.34it/s] 43%|████▎     | 4056/9500 [00:11<00:11, 480.37it/s] 43%|████▎     | 4106/9500 [00:11<00:12, 432.41it/s] 44%|████▎     | 4152/9500 [00:11<00:12, 415.80it/s] 44%|████▍     | 4220/9500 [00:11<00:11, 473.24it/s] 45%|████▍     | 4269/9500 [00:11<00:11, 448.43it/s] 45%|████▌     | 4317/9500 [00:11<00:11, 456.24it/s] 46%|████▌     | 4364/9500 [00:11<00:11, 429.51it/s] 46%|████▋     | 4417/9500 [00:11<00:11, 448.33it/s] 47%|████▋     | 4463/9500 [00:12<00:12, 406.26it/s] 47%|████▋     | 4506/9500 [00:12<00:12, 407.38it/s] 48%|████▊     | 4548/9500 [00:12<00:12, 404.57it/s] 48%|████▊     | 4603/9500 [00:12<00:11, 438.78it/s] 49%|████▉     | 4652/9500 [00:12<00:10, 441.59it/s] 49%|████▉     | 4697/9500 [00:12<00:11, 427.47it/s] 50%|█████     | 4750/9500 [00:12<00:10, 453.51it/s] 50%|█████     | 4796/9500 [00:12<00:11, 426.58it/s] 51%|█████     | 4848/9500 [00:12<00:10, 443.41it/s] 52%|█████▏    | 4901/9500 [00:12<00:10, 454.77it/s] 52%|█████▏    | 4949/9500 [00:13<00:10, 439.32it/s] 53%|█████▎    | 4994/9500 [00:13<00:10, 421.51it/s] 53%|█████▎    | 5037/9500 [00:13<00:10, 420.62it/s] 54%|█████▎    | 5091/9500 [00:13<00:09, 448.37it/s] 54%|█████▍    | 5155/9500 [00:13<00:08, 498.95it/s] 55%|█████▍    | 5218/9500 [00:13<00:08, 521.52it/s] 55%|█████▌    | 5271/9500 [00:13<00:08, 500.96it/s] 56%|█████▌    | 5322/9500 [00:13<00:08, 503.13it/s] 57%|█████▋    | 5373/9500 [00:13<00:09, 455.38it/s] 57%|█████▋    | 5437/9500 [00:14<00:08, 502.68it/s] 58%|█████▊    | 5489/9500 [00:14<00:08, 451.50it/s] 58%|█████▊    | 5536/9500 [00:14<00:09, 434.98it/s] 59%|█████▉    | 5593/9500 [00:14<00:08, 462.88it/s] 59%|█████▉    | 5641/9500 [00:14<00:08, 451.57it/s] 60%|█████▉    | 5687/9500 [00:14<00:08, 431.33it/s] 61%|██████    | 5756/9500 [00:14<00:07, 499.91it/s] 61%|██████    | 5808/9500 [00:14<00:07, 480.29it/s] 62%|██████▏   | 5863/9500 [00:15<00:07, 494.81it/s] 62%|██████▏   | 5914/9500 [00:15<00:07, 497.05it/s] 63%|██████▎   | 5965/9500 [00:15<00:08, 435.49it/s] 63%|██████▎   | 6017/9500 [00:15<00:07, 456.52it/s] 64%|██████▍   | 6083/9500 [00:15<00:06, 495.00it/s] 65%|██████▍   | 6144/9500 [00:15<00:06, 508.35it/s] 65%|██████▌   | 6196/9500 [00:15<00:06, 497.54it/s] 66%|██████▌   | 6247/9500 [00:15<00:06, 467.44it/s] 66%|██████▋   | 6310/9500 [00:15<00:06, 510.58it/s] 67%|██████▋   | 6362/9500 [00:16<00:06, 452.60it/s] 67%|██████▋   | 6409/9500 [00:16<00:07, 440.52it/s] 68%|██████▊   | 6478/9500 [00:16<00:06, 498.94it/s] 69%|██████▊   | 6530/9500 [00:16<00:06, 457.88it/s] 69%|██████▉   | 6590/9500 [00:16<00:06, 479.03it/s] 70%|███████   | 6650/9500 [00:16<00:05, 510.67it/s] 71%|███████   | 6703/9500 [00:16<00:05, 483.85it/s] 71%|███████   | 6761/9500 [00:16<00:05, 507.08it/s] 72%|███████▏  | 6813/9500 [00:16<00:05, 502.84it/s] 72%|███████▏  | 6864/9500 [00:17<00:05, 486.09it/s] 73%|███████▎  | 6918/9500 [00:17<00:05, 499.36it/s] 73%|███████▎  | 6969/9500 [00:17<00:05, 455.95it/s] 74%|███████▍  | 7016/9500 [00:17<00:05, 459.67it/s] 74%|███████▍  | 7065/9500 [00:17<00:05, 463.18it/s] 75%|███████▍  | 7112/9500 [00:17<00:05, 457.01it/s] 75%|███████▌  | 7159/9500 [00:17<00:05, 442.25it/s] 76%|███████▌  | 7222/9500 [00:17<00:04, 487.85it/s] 77%|███████▋  | 7272/9500 [00:17<00:04, 463.64it/s] 77%|███████▋  | 7319/9500 [00:18<00:05, 416.90it/s] 78%|███████▊  | 7377/9500 [00:18<00:04, 457.53it/s] 78%|███████▊  | 7424/9500 [00:18<00:04, 415.85it/s] 79%|███████▊  | 7467/9500 [00:18<00:05, 403.97it/s] 79%|███████▉  | 7526/9500 [00:18<00:04, 436.84it/s] 80%|███████▉  | 7589/9500 [00:18<00:03, 482.78it/s] 80%|████████  | 7639/9500 [00:18<00:04, 461.20it/s] 81%|████████  | 7692/9500 [00:18<00:03, 463.73it/s] 81%|████████▏ | 7742/9500 [00:19<00:03, 469.38it/s] 82%|████████▏ | 7791/9500 [00:19<00:03, 458.73it/s] 83%|████████▎ | 7838/9500 [00:19<00:03, 425.83it/s] 83%|████████▎ | 7885/9500 [00:19<00:03, 436.61it/s] 84%|████████▎ | 7944/9500 [00:19<00:03, 477.01it/s] 84%|████████▍ | 7993/9500 [00:19<00:03, 476.50it/s] 85%|████████▍ | 8044/9500 [00:19<00:03, 469.78it/s] 85%|████████▌ | 8092/9500 [00:19<00:03, 443.53it/s] 86%|████████▌ | 8141/9500 [00:19<00:03, 445.56it/s] 86%|████████▌ | 8186/9500 [00:20<00:03, 428.37it/s] 87%|████████▋ | 8250/9500 [00:20<00:02, 452.62it/s] 87%|████████▋ | 8305/9500 [00:20<00:02, 465.92it/s] 88%|████████▊ | 8370/9500 [00:20<00:02, 511.98it/s] 89%|████████▊ | 8422/9500 [00:20<00:02, 477.81it/s] 89%|████████▉ | 8478/9500 [00:20<00:02, 487.03it/s] 90%|████████▉ | 8533/9500 [00:20<00:01, 490.93it/s] 90%|█████████ | 8583/9500 [00:20<00:01, 482.52it/s] 91%|█████████ | 8632/9500 [00:20<00:01, 478.16it/s] 91%|█████████▏| 8683/9500 [00:21<00:01, 455.08it/s] 92%|█████████▏| 8729/9500 [00:21<00:01, 449.32it/s] 92%|█████████▏| 8784/9500 [00:21<00:01, 457.25it/s] 93%|█████████▎| 8830/9500 [00:21<00:01, 427.28it/s] 94%|█████████▎| 8888/9500 [00:21<00:01, 465.79it/s] 94%|█████████▍| 8936/9500 [00:21<00:01, 466.24it/s] 95%|█████████▍| 8984/9500 [00:21<00:01, 430.92it/s] 95%|█████████▌| 9037/9500 [00:21<00:01, 456.52it/s] 96%|█████████▌| 9084/9500 [00:22<00:01, 405.86it/s] 96%|█████████▌| 9132/9500 [00:22<00:00, 420.58it/s] 97%|█████████▋| 9176/9500 [00:22<00:00, 417.35it/s] 97%|█████████▋| 9226/9500 [00:22<00:00, 424.74it/s] 98%|█████████▊| 9279/9500 [00:22<00:00, 448.33it/s] 98%|█████████▊| 9332/9500 [00:22<00:00, 438.35it/s] 99%|█████████▉| 9384/9500 [00:22<00:00, 459.89it/s] 99%|█████████▉| 9431/9500 [00:22<00:00, 451.11it/s]100%|█████████▉| 9497/9500 [00:22<00:00, 505.47it/s]100%|██████████| 9500/9500 [00:22<00:00, 414.18it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 71, in <module>
    predicted_size = predict_npz_file_size(data_dict, verbose=True)
                                           ^^^^^^^^^
NameError: name 'data_dict' is not defined
test process done: X = (9500, 3, 2, 2000), y = (9500,)
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/train.py", line 77, in <module>
    train_X, train_y = data_processor.load_data(os.path.join(in_path, f"{args.train_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_train.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/data_analysis/spatial_analysis.py", line 65, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p20.npz'
starting gen taf script for test_p20
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p20.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p30.npz'
starting gen taf script for test_p30
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p30.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p40.npz'
starting gen taf script for test_p40
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p40.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p50.npz'
starting gen taf script for test_p50
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p50.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p60.npz'
starting gen taf script for test_p60
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p60.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p70.npz'
starting gen taf script for test_p70
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p70.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p80.npz'
starting gen taf script for test_p80
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p80.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p90.npz'
starting gen taf script for test_p90
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p90.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 46, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/test_p100.npz'
starting gen taf script for test_p100
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 75, in <module>
    valid_X, valid_y = data_processor.load_data(os.path.join(in_path, f"{args.valid_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_aug_valid.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/Tik_Tok_30/taf_test_p100.npz
