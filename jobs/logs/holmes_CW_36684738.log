  0%|          | 0/85641 [00:00<?, ?it/s]  0%|          | 90/85641 [00:00<01:37, 874.03it/s]  0%|          | 202/85641 [00:00<01:24, 1008.36it/s]  0%|          | 318/85641 [00:00<01:19, 1074.96it/s]  1%|          | 432/85641 [00:00<01:17, 1097.47it/s]  1%|          | 551/85641 [00:00<01:15, 1124.10it/s]  1%|          | 669/85641 [00:00<01:14, 1136.80it/s]  1%|          | 783/85641 [00:00<01:20, 1050.83it/s]  1%|          | 907/85641 [00:00<01:16, 1103.71it/s]  1%|          | 1019/85641 [00:00<01:20, 1053.42it/s]  1%|▏         | 1126/85641 [00:01<01:22, 1019.69it/s]  1%|▏         | 1229/85641 [00:01<01:22, 1017.36it/s]  2%|▏         | 1332/85641 [00:01<01:23, 1015.43it/s]  2%|▏         | 1434/85641 [00:01<01:24, 1002.00it/s]  2%|▏         | 1535/85641 [00:01<01:24, 996.29it/s]   2%|▏         | 1649/85641 [00:01<01:20, 1037.21it/s]  2%|▏         | 1753/85641 [00:01<01:21, 1029.06it/s]  2%|▏         | 1867/85641 [00:01<01:19, 1059.10it/s]  2%|▏         | 1974/85641 [00:01<01:21, 1025.62it/s]  2%|▏         | 2082/85641 [00:01<01:20, 1039.43it/s]  3%|▎         | 2200/85641 [00:02<01:17, 1080.21it/s]  3%|▎         | 2309/85641 [00:02<01:20, 1033.58it/s]  3%|▎         | 2418/85641 [00:02<01:19, 1049.03it/s]  3%|▎         | 2534/85641 [00:02<01:17, 1073.97it/s]  3%|▎         | 2648/85641 [00:02<01:16, 1090.83it/s]  3%|▎         | 2758/85641 [00:02<01:17, 1070.72it/s]  3%|▎         | 2866/85641 [00:02<01:20, 1028.48it/s]  3%|▎         | 2988/85641 [00:02<01:16, 1082.00it/s]  4%|▎         | 3097/85641 [00:02<01:17, 1066.84it/s]  4%|▎         | 3205/85641 [00:03<01:18, 1046.01it/s]  4%|▍         | 3329/85641 [00:03<01:14, 1099.67it/s]  4%|▍         | 3440/85641 [00:03<01:17, 1057.88it/s]  4%|▍         | 3563/85641 [00:03<01:14, 1102.57it/s]  4%|▍         | 3674/85641 [00:03<01:15, 1079.25it/s]  4%|▍         | 3783/85641 [00:03<01:18, 1045.99it/s]  5%|▍         | 3889/85641 [00:03<01:20, 1013.64it/s]  5%|▍         | 3991/85641 [00:03<01:20, 1008.24it/s]  5%|▍         | 4095/85641 [00:03<01:20, 1016.78it/s]  5%|▍         | 4216/85641 [00:04<01:16, 1069.50it/s]  5%|▌         | 4325/85641 [00:04<01:15, 1075.29it/s]  5%|▌         | 4433/85641 [00:04<01:17, 1042.63it/s]  5%|▌         | 4538/85641 [00:04<01:21, 999.72it/s]   5%|▌         | 4656/85641 [00:04<01:17, 1047.22it/s]  6%|▌         | 4771/85641 [00:04<01:15, 1074.73it/s]  6%|▌         | 4879/85641 [00:04<01:16, 1061.51it/s]  6%|▌         | 4986/85641 [00:04<01:18, 1032.42it/s]  6%|▌         | 5090/85641 [00:04<01:18, 1023.74it/s]  6%|▌         | 5211/85641 [00:04<01:14, 1076.41it/s]  6%|▌         | 5320/85641 [00:05<01:16, 1043.75it/s]  6%|▋         | 5428/85641 [00:05<01:16, 1052.78it/s]  6%|▋         | 5536/85641 [00:05<01:15, 1060.01it/s]  7%|▋         | 5665/85641 [00:05<01:11, 1126.00it/s]  7%|▋         | 5778/85641 [00:05<01:11, 1113.99it/s]  7%|▋         | 5902/85641 [00:05<01:09, 1146.95it/s]  7%|▋         | 6017/85641 [00:05<01:10, 1123.13it/s]  7%|▋         | 6130/85641 [00:05<01:11, 1107.85it/s]  7%|▋         | 6242/85641 [00:05<01:11, 1109.99it/s]  7%|▋         | 6354/85641 [00:05<01:13, 1084.65it/s]  8%|▊         | 6463/85641 [00:06<01:17, 1016.58it/s]  8%|▊         | 6566/85641 [00:06<01:18, 1002.20it/s]  8%|▊         | 6667/85641 [00:06<01:21, 969.58it/s]   8%|▊         | 6773/85641 [00:06<01:19, 994.64it/s]  8%|▊         | 6888/85641 [00:06<01:15, 1038.59it/s]  8%|▊         | 6993/85641 [00:06<01:18, 999.69it/s]   8%|▊         | 7109/85641 [00:06<01:15, 1044.52it/s]  8%|▊         | 7221/85641 [00:06<01:13, 1064.88it/s]  9%|▊         | 7329/85641 [00:06<01:17, 1016.57it/s]  9%|▊         | 7436/85641 [00:07<01:15, 1029.45it/s]  9%|▉         | 7543/85641 [00:07<01:15, 1039.90it/s]  9%|▉         | 7656/85641 [00:07<01:13, 1061.00it/s]  9%|▉         | 7785/85641 [00:07<01:09, 1126.30it/s]  9%|▉         | 7901/85641 [00:07<01:08, 1132.95it/s]  9%|▉         | 8015/85641 [00:07<01:08, 1131.25it/s]  9%|▉         | 8129/85641 [00:07<01:13, 1061.69it/s] 10%|▉         | 8237/85641 [00:07<01:14, 1033.19it/s] 10%|▉         | 8361/85641 [00:07<01:10, 1091.30it/s] 10%|▉         | 8472/85641 [00:08<01:11, 1081.36it/s] 10%|█         | 8581/85641 [00:08<01:11, 1078.96it/s] 10%|█         | 8696/85641 [00:08<01:10, 1097.05it/s] 10%|█         | 8817/85641 [00:08<01:08, 1125.02it/s] 10%|█         | 8930/85641 [00:08<01:11, 1070.03it/s] 11%|█         | 9045/85641 [00:08<01:10, 1087.90it/s] 11%|█         | 9155/85641 [00:08<01:12, 1050.71it/s] 11%|█         | 9261/85641 [00:08<01:17, 984.51it/s]  11%|█         | 9361/85641 [00:08<01:17, 980.30it/s] 11%|█         | 9466/85641 [00:08<01:16, 998.79it/s] 11%|█         | 9584/85641 [00:09<01:12, 1042.93it/s] 11%|█▏        | 9694/85641 [00:09<01:12, 1051.10it/s] 11%|█▏        | 9802/85641 [00:09<01:11, 1057.68it/s] 12%|█▏        | 9909/85641 [00:09<01:14, 1020.29it/s] 12%|█▏        | 10024/85641 [00:09<01:11, 1052.51it/s] 12%|█▏        | 10144/85641 [00:09<01:08, 1095.07it/s] 12%|█▏        | 10254/85641 [00:09<01:09, 1078.83it/s] 12%|█▏        | 10378/85641 [00:09<01:06, 1125.54it/s] 12%|█▏        | 10491/85641 [00:09<01:10, 1069.34it/s] 12%|█▏        | 10599/85641 [00:10<01:10, 1059.80it/s] 13%|█▎        | 10706/85641 [00:10<01:10, 1060.87it/s] 13%|█▎        | 10816/85641 [00:10<01:09, 1070.02it/s] 13%|█▎        | 10935/85641 [00:10<01:07, 1104.69it/s] 13%|█▎        | 11047/85641 [00:10<01:07, 1099.46it/s] 13%|█▎        | 11158/85641 [00:10<01:07, 1100.52it/s] 13%|█▎        | 11269/85641 [00:10<01:09, 1069.53it/s] 13%|█▎        | 11377/85641 [00:10<01:11, 1036.26it/s] 13%|█▎        | 11481/85641 [00:10<01:11, 1030.33it/s] 14%|█▎        | 11606/85641 [00:10<01:07, 1093.66it/s] 14%|█▎        | 11718/85641 [00:11<01:07, 1099.28it/s] 14%|█▍        | 11829/85641 [00:11<01:09, 1067.28it/s] 14%|█▍        | 11937/85641 [00:11<01:10, 1052.46it/s] 14%|█▍        | 12053/85641 [00:11<01:08, 1079.69it/s] 14%|█▍        | 12162/85641 [00:11<01:09, 1056.96it/s] 14%|█▍        | 12268/85641 [00:11<01:09, 1052.95it/s] 14%|█▍        | 12377/85641 [00:11<01:08, 1062.80it/s] 15%|█▍        | 12484/85641 [00:11<01:09, 1055.87it/s] 15%|█▍        | 12590/85641 [00:11<01:09, 1052.01it/s] 15%|█▍        | 12702/85641 [00:11<01:08, 1071.99it/s] 15%|█▍        | 12821/85641 [00:12<01:06, 1102.73it/s] 15%|█▌        | 12932/85641 [00:12<01:11, 1021.82it/s] 15%|█▌        | 13043/85641 [00:12<01:09, 1045.37it/s] 15%|█▌        | 13160/85641 [00:12<01:07, 1075.67it/s] 15%|█▌        | 13269/85641 [00:12<01:09, 1038.70it/s] 16%|█▌        | 13395/85641 [00:12<01:06, 1090.12it/s] 16%|█▌        | 13505/85641 [00:12<01:07, 1074.50it/s] 16%|█▌        | 13622/85641 [00:12<01:05, 1100.38it/s] 16%|█▌        | 13735/85641 [00:12<01:05, 1100.80it/s] 16%|█▌        | 13846/85641 [00:13<01:06, 1076.24it/s] 16%|█▋        | 13954/85641 [00:13<01:07, 1061.24it/s] 16%|█▋        | 14070/85641 [00:13<01:06, 1078.68it/s] 17%|█▋        | 14192/85641 [00:13<01:04, 1111.37it/s] 17%|█▋        | 14304/85641 [00:13<01:04, 1103.00it/s] 17%|█▋        | 14419/85641 [00:13<01:03, 1114.02it/s] 17%|█▋        | 14531/85641 [00:13<01:07, 1051.86it/s] 17%|█▋        | 14637/85641 [00:13<01:11, 989.52it/s]  17%|█▋        | 14754/85641 [00:13<01:08, 1032.54it/s] 17%|█▋        | 14859/85641 [00:14<01:08, 1035.79it/s] 17%|█▋        | 14966/85641 [00:14<01:07, 1042.26it/s] 18%|█▊        | 15071/85641 [00:14<01:09, 1014.40it/s] 18%|█▊        | 15173/85641 [00:14<01:11, 990.87it/s]  18%|█▊        | 15290/85641 [00:14<01:07, 1034.84it/s] 18%|█▊        | 15395/85641 [00:14<01:07, 1038.84it/s] 18%|█▊        | 15500/85641 [00:14<01:07, 1039.92it/s] 18%|█▊        | 15605/85641 [00:14<01:08, 1024.28it/s] 18%|█▊        | 15709/85641 [00:14<01:08, 1028.36it/s] 18%|█▊        | 15813/85641 [00:14<01:07, 1031.46it/s] 19%|█▊        | 15938/85641 [00:15<01:03, 1095.56it/s] 19%|█▉        | 16062/85641 [00:15<01:01, 1132.40it/s] 19%|█▉        | 16176/85641 [00:15<01:02, 1107.91it/s] 19%|█▉        | 16292/85641 [00:15<01:01, 1120.24it/s] 19%|█▉        | 16405/85641 [00:15<01:03, 1085.85it/s] 19%|█▉        | 16534/85641 [00:15<01:00, 1144.27it/s] 19%|█▉        | 16649/85641 [00:15<01:00, 1140.20it/s] 20%|█▉        | 16769/85641 [00:15<00:59, 1154.27it/s] 20%|█▉        | 16885/85641 [00:15<01:04, 1067.49it/s] 20%|█▉        | 16994/85641 [00:16<01:06, 1035.83it/s] 20%|█▉        | 17099/85641 [00:16<01:05, 1039.67it/s] 20%|██        | 17208/85641 [00:16<01:05, 1045.16it/s] 20%|██        | 17314/85641 [00:16<01:05, 1045.90it/s] 20%|██        | 17419/85641 [00:16<01:07, 1015.87it/s] 20%|██        | 17521/85641 [00:16<01:07, 1010.03it/s] 21%|██        | 17629/85641 [00:16<01:06, 1030.27it/s] 21%|██        | 17735/85641 [00:16<01:05, 1038.81it/s] 21%|██        | 17842/85641 [00:16<01:04, 1046.91it/s] 21%|██        | 17952/85641 [00:16<01:03, 1058.91it/s] 21%|██        | 18070/85641 [00:17<01:01, 1092.88it/s] 21%|██        | 18180/85641 [00:17<01:02, 1080.69it/s] 21%|██▏       | 18289/85641 [00:17<01:05, 1033.11it/s] 21%|██▏       | 18403/85641 [00:17<01:03, 1062.44it/s] 22%|██▏       | 18510/85641 [00:17<01:03, 1048.96it/s] 22%|██▏       | 18625/85641 [00:17<01:02, 1069.34it/s] 22%|██▏       | 18733/85641 [00:17<01:03, 1053.03it/s] 22%|██▏       | 18839/85641 [00:17<01:03, 1046.58it/s] 22%|██▏       | 18944/85641 [00:17<01:04, 1036.20it/s] 22%|██▏       | 19049/85641 [00:17<01:04, 1037.90it/s] 22%|██▏       | 19153/85641 [00:18<01:06, 997.44it/s]  22%|██▏       | 19261/85641 [00:18<01:05, 1019.37it/s] 23%|██▎       | 19364/85641 [00:18<01:06, 992.45it/s]  23%|██▎       | 19464/85641 [00:18<01:06, 989.99it/s] 23%|██▎       | 19564/85641 [00:18<01:08, 970.83it/s] 23%|██▎       | 19662/85641 [00:18<01:08, 970.00it/s] 23%|██▎       | 19774/85641 [00:18<01:05, 1013.01it/s] 23%|██▎       | 19883/85641 [00:18<01:03, 1034.40it/s] 23%|██▎       | 19991/85641 [00:18<01:02, 1044.42it/s] 23%|██▎       | 20096/85641 [00:19<01:04, 1023.78it/s] 24%|██▎       | 20208/85641 [00:19<01:02, 1051.75it/s] 24%|██▎       | 20325/85641 [00:19<01:00, 1085.67it/s] 24%|██▍       | 20434/85641 [00:19<01:03, 1031.30it/s] 24%|██▍       | 20541/85641 [00:19<01:02, 1040.21it/s] 24%|██▍       | 20654/85641 [00:19<01:01, 1057.49it/s] 24%|██▍       | 20763/85641 [00:19<01:00, 1066.54it/s] 24%|██▍       | 20875/85641 [00:19<00:59, 1082.15it/s] 25%|██▍       | 20984/85641 [00:19<01:02, 1042.71it/s] 25%|██▍       | 21100/85641 [00:19<01:00, 1075.39it/s] 25%|██▍       | 21208/85641 [00:20<01:01, 1050.44it/s] 25%|██▍       | 21329/85641 [00:20<00:58, 1095.76it/s] 25%|██▌       | 21448/85641 [00:20<00:57, 1121.50it/s] 25%|██▌       | 21568/85641 [00:20<00:56, 1134.53it/s] 25%|██▌       | 21682/85641 [00:20<00:59, 1082.11it/s] 25%|██▌       | 21791/85641 [00:20<01:00, 1061.98it/s] 26%|██▌       | 21898/85641 [00:20<01:01, 1039.79it/s] 26%|██▌       | 22007/85641 [00:20<01:00, 1053.42it/s] 26%|██▌       | 22118/85641 [00:20<00:59, 1062.63it/s] 26%|██▌       | 22225/85641 [00:21<00:59, 1058.06it/s] 26%|██▌       | 22347/85641 [00:21<00:57, 1102.40it/s] 26%|██▌       | 22458/85641 [00:21<00:57, 1089.75it/s] 26%|██▋       | 22568/85641 [00:21<00:59, 1065.21it/s] 26%|██▋       | 22675/85641 [00:21<00:59, 1050.93it/s] 27%|██▋       | 22805/85641 [00:21<00:56, 1114.08it/s] 27%|██▋       | 22921/85641 [00:21<00:55, 1124.79it/s] 27%|██▋       | 23049/85641 [00:21<00:53, 1168.27it/s] 27%|██▋       | 23167/85641 [00:21<00:55, 1122.09it/s] 27%|██▋       | 23280/85641 [00:21<00:57, 1081.30it/s] 27%|██▋       | 23391/85641 [00:22<00:57, 1088.62it/s] 27%|██▋       | 23501/85641 [00:22<00:58, 1066.73it/s] 28%|██▊       | 23611/85641 [00:22<00:57, 1074.20it/s] 28%|██▊       | 23719/85641 [00:22<01:00, 1031.32it/s] 28%|██▊       | 23823/85641 [00:22<01:00, 1029.43it/s] 28%|██▊       | 23941/85641 [00:22<00:57, 1070.57it/s] 28%|██▊       | 24083/85641 [00:22<00:52, 1170.29it/s] 28%|██▊       | 24201/85641 [00:22<00:55, 1117.06it/s] 28%|██▊       | 24317/85641 [00:22<00:54, 1128.45it/s] 29%|██▊       | 24441/85641 [00:23<00:52, 1160.03it/s] 29%|██▊       | 24558/85641 [00:23<00:55, 1108.95it/s] 29%|██▉       | 24670/85641 [00:23<00:55, 1099.57it/s] 29%|██▉       | 24794/85641 [00:23<00:53, 1139.46it/s] 29%|██▉       | 24909/85641 [00:23<00:55, 1089.99it/s] 29%|██▉       | 25019/85641 [00:23<00:58, 1039.29it/s] 29%|██▉       | 25124/85641 [00:23<00:59, 1015.00it/s] 29%|██▉       | 25235/85641 [00:23<00:58, 1041.43it/s] 30%|██▉       | 25349/85641 [00:23<00:56, 1066.77it/s] 30%|██▉       | 25457/85641 [00:23<00:56, 1058.33it/s] 30%|██▉       | 25564/85641 [00:24<00:56, 1057.88it/s] 30%|██▉       | 25681/85641 [00:24<00:55, 1089.80it/s] 30%|███       | 25791/85641 [00:24<00:57, 1043.92it/s] 30%|███       | 25914/85641 [00:24<00:54, 1095.47it/s] 30%|███       | 26026/85641 [00:24<00:54, 1102.23it/s] 31%|███       | 26142/85641 [00:24<00:53, 1114.93it/s] 31%|███       | 26255/85641 [00:24<00:53, 1117.12it/s] 31%|███       | 26367/85641 [00:24<00:53, 1111.09it/s] 31%|███       | 26479/85641 [00:24<00:55, 1066.37it/s] 31%|███       | 26587/85641 [00:25<00:55, 1055.83it/s] 31%|███       | 26699/85641 [00:25<00:55, 1069.75it/s] 31%|███▏      | 26807/85641 [00:25<00:56, 1039.03it/s] 31%|███▏      | 26923/85641 [00:25<00:54, 1068.53it/s] 32%|███▏      | 27031/85641 [00:25<00:55, 1060.87it/s] 32%|███▏      | 27138/85641 [00:25<00:55, 1049.02it/s] 32%|███▏      | 27244/85641 [00:25<00:56, 1032.23it/s] 32%|███▏      | 27366/85641 [00:25<00:53, 1085.24it/s] 32%|███▏      | 27475/85641 [00:25<00:55, 1041.01it/s] 32%|███▏      | 27580/85641 [00:25<00:56, 1028.50it/s] 32%|███▏      | 27709/85641 [00:26<00:52, 1102.99it/s] 32%|███▏      | 27820/85641 [00:26<00:54, 1062.15it/s] 33%|███▎      | 27927/85641 [00:26<00:55, 1043.42it/s] 33%|███▎      | 28040/85641 [00:26<00:54, 1057.34it/s] 33%|███▎      | 28147/85641 [00:26<00:54, 1054.66it/s] 33%|███▎      | 28253/85641 [00:26<00:55, 1036.57it/s] 33%|███▎      | 28357/85641 [00:26<00:57, 992.24it/s]  33%|███▎      | 28462/85641 [00:26<00:56, 1007.51it/s] 33%|███▎      | 28571/85641 [00:26<00:55, 1028.84it/s] 33%|███▎      | 28675/85641 [00:27<00:58, 980.70it/s]  34%|███▎      | 28774/85641 [00:27<00:57, 980.96it/s] 34%|███▎      | 28879/85641 [00:27<00:56, 999.10it/s] 34%|███▍      | 28995/85641 [00:27<00:54, 1045.34it/s] 34%|███▍      | 29100/85641 [00:27<00:54, 1029.05it/s] 34%|███▍      | 29207/85641 [00:27<00:54, 1032.45it/s] 34%|███▍      | 29328/85641 [00:27<00:52, 1082.67it/s] 34%|███▍      | 29438/85641 [00:27<00:51, 1086.67it/s] 35%|███▍      | 29552/85641 [00:27<00:50, 1101.80it/s] 35%|███▍      | 29663/85641 [00:27<00:51, 1086.93it/s] 35%|███▍      | 29783/85641 [00:28<00:50, 1112.51it/s] 35%|███▍      | 29895/85641 [00:28<00:50, 1105.52it/s] 35%|███▌      | 30006/85641 [00:28<00:52, 1059.94it/s] 35%|███▌      | 30113/85641 [00:28<00:54, 1010.09it/s] 35%|███▌      | 30228/85641 [00:28<00:52, 1049.25it/s] 35%|███▌      | 30336/85641 [00:28<00:52, 1052.40it/s] 36%|███▌      | 30454/85641 [00:28<00:50, 1087.58it/s] 36%|███▌      | 30564/85641 [00:28<00:53, 1034.85it/s] 36%|███▌      | 30679/85641 [00:28<00:52, 1056.75it/s] 36%|███▌      | 30786/85641 [00:29<00:51, 1057.24it/s] 36%|███▌      | 30898/85641 [00:29<00:51, 1069.85it/s] 36%|███▌      | 31019/85641 [00:29<00:49, 1110.43it/s] 36%|███▋      | 31141/85641 [00:29<00:47, 1141.96it/s] 36%|███▋      | 31256/85641 [00:29<00:51, 1061.47it/s] 37%|███▋      | 31369/85641 [00:29<00:50, 1076.79it/s] 37%|███▋      | 31480/85641 [00:29<00:50, 1082.47it/s] 37%|███▋      | 31589/85641 [00:29<00:50, 1065.48it/s] 37%|███▋      | 31716/85641 [00:29<00:47, 1124.40it/s] 37%|███▋      | 31830/85641 [00:29<00:47, 1122.27it/s] 37%|███▋      | 31943/85641 [00:30<00:51, 1050.45it/s] 37%|███▋      | 32064/85641 [00:30<00:48, 1095.11it/s] 38%|███▊      | 32193/85641 [00:30<00:46, 1148.93it/s] 38%|███▊      | 32309/85641 [00:30<00:48, 1105.64it/s] 38%|███▊      | 32421/85641 [00:30<00:48, 1105.60it/s] 38%|███▊      | 32533/85641 [00:30<00:48, 1086.55it/s] 38%|███▊      | 32643/85641 [00:30<00:49, 1076.99it/s] 38%|███▊      | 32752/85641 [00:30<00:48, 1079.50it/s] 38%|███▊      | 32861/85641 [00:30<00:49, 1072.47it/s] 38%|███▊      | 32969/85641 [00:31<00:49, 1059.36it/s] 39%|███▊      | 33076/85641 [00:31<00:49, 1051.79it/s] 39%|███▊      | 33182/85641 [00:31<00:52, 991.54it/s]  39%|███▉      | 33283/85641 [00:31<00:52, 996.24it/s] 39%|███▉      | 33385/85641 [00:31<00:52, 999.14it/s] 39%|███▉      | 33486/85641 [00:31<00:54, 952.06it/s] 39%|███▉      | 33582/85641 [00:31<00:55, 939.81it/s] 39%|███▉      | 33691/85641 [00:31<00:52, 981.79it/s] 39%|███▉      | 33806/85641 [00:31<00:50, 1029.67it/s] 40%|███▉      | 33910/85641 [00:31<00:50, 1016.72it/s] 40%|███▉      | 34045/85641 [00:32<00:46, 1104.52it/s] 40%|███▉      | 34156/85641 [00:32<00:48, 1070.98it/s] 40%|████      | 34269/85641 [00:32<00:47, 1087.81it/s] 40%|████      | 34379/85641 [00:32<00:47, 1079.01it/s] 40%|████      | 34497/85641 [00:32<00:46, 1107.96it/s] 40%|████      | 34624/85641 [00:32<00:44, 1155.41it/s] 41%|████      | 34740/85641 [00:32<00:45, 1123.72it/s] 41%|████      | 34853/85641 [00:32<00:45, 1119.67it/s] 41%|████      | 34967/85641 [00:32<00:45, 1119.69it/s] 41%|████      | 35088/85641 [00:33<00:44, 1145.19it/s] 41%|████      | 35203/85641 [00:33<00:45, 1097.62it/s] 41%|████      | 35314/85641 [00:33<00:47, 1057.27it/s] 41%|████▏     | 35426/85641 [00:33<00:46, 1073.67it/s] 41%|████▏     | 35534/85641 [00:33<00:48, 1040.51it/s] 42%|████▏     | 35649/85641 [00:33<00:46, 1071.11it/s] 42%|████▏     | 35757/85641 [00:33<00:48, 1026.48it/s] 42%|████▏     | 35865/85641 [00:33<00:47, 1041.56it/s] 42%|████▏     | 35993/85641 [00:33<00:44, 1109.55it/s] 42%|████▏     | 36105/85641 [00:33<00:44, 1109.11it/s] 42%|████▏     | 36225/85641 [00:34<00:43, 1131.59it/s] 42%|████▏     | 36339/85641 [00:34<00:44, 1112.19it/s] 43%|████▎     | 36451/85641 [00:34<00:45, 1088.80it/s] 43%|████▎     | 36561/85641 [00:34<00:45, 1068.92it/s] 43%|████▎     | 36674/85641 [00:34<00:45, 1078.25it/s] 43%|████▎     | 36797/85641 [00:34<00:43, 1121.19it/s] 43%|████▎     | 36911/85641 [00:34<00:43, 1125.20it/s] 43%|████▎     | 37024/85641 [00:34<00:44, 1086.08it/s] 43%|████▎     | 37133/85641 [00:34<00:45, 1075.61it/s] 43%|████▎     | 37241/85641 [00:35<00:46, 1038.28it/s] 44%|████▎     | 37351/85641 [00:35<00:45, 1055.60it/s] 44%|████▎     | 37460/85641 [00:35<00:45, 1064.94it/s] 44%|████▍     | 37586/85641 [00:35<00:42, 1121.57it/s] 44%|████▍     | 37699/85641 [00:35<00:43, 1105.48it/s] 44%|████▍     | 37810/85641 [00:35<00:44, 1069.17it/s] 44%|████▍     | 37918/85641 [00:35<00:45, 1046.07it/s] 44%|████▍     | 38023/85641 [00:35<00:46, 1026.05it/s] 45%|████▍     | 38141/85641 [00:35<00:44, 1063.72it/s] 45%|████▍     | 38264/85641 [00:35<00:42, 1109.88it/s] 45%|████▍     | 38376/85641 [00:36<00:43, 1096.88it/s] 45%|████▍     | 38486/85641 [00:36<00:43, 1083.10it/s] 45%|████▌     | 38607/85641 [00:36<00:42, 1108.02it/s] 45%|████▌     | 38726/85641 [00:36<00:41, 1127.03it/s] 45%|████▌     | 38839/85641 [00:36<00:42, 1095.75it/s] 45%|████▌     | 38949/85641 [00:36<00:43, 1067.16it/s] 46%|████▌     | 39056/85641 [00:36<00:43, 1062.93it/s] 46%|████▌     | 39163/85641 [00:36<00:44, 1054.60it/s] 46%|████▌     | 39269/85641 [00:36<00:44, 1031.89it/s] 46%|████▌     | 39374/85641 [00:37<00:44, 1036.56it/s] 46%|████▌     | 39478/85641 [00:37<00:44, 1030.06it/s] 46%|████▌     | 39582/85641 [00:37<00:46, 991.82it/s]  46%|████▋     | 39682/85641 [00:37<00:46, 981.48it/s] 46%|████▋     | 39793/85641 [00:37<00:45, 1006.09it/s] 47%|████▋     | 39894/85641 [00:37<00:45, 1001.14it/s] 47%|████▋     | 40010/85641 [00:37<00:43, 1046.59it/s] 47%|████▋     | 40115/85641 [00:37<00:43, 1038.34it/s] 47%|████▋     | 40219/85641 [00:37<00:43, 1036.12it/s] 47%|████▋     | 40332/85641 [00:37<00:42, 1057.18it/s] 47%|████▋     | 40438/85641 [00:38<00:43, 1044.73it/s] 47%|████▋     | 40546/85641 [00:38<00:42, 1051.19it/s] 47%|████▋     | 40652/85641 [00:38<00:43, 1037.92it/s] 48%|████▊     | 40756/85641 [00:38<00:43, 1030.85it/s] 48%|████▊     | 40869/85641 [00:38<00:42, 1059.80it/s] 48%|████▊     | 40980/85641 [00:38<00:41, 1074.01it/s] 48%|████▊     | 41091/85641 [00:38<00:41, 1083.19it/s] 48%|████▊     | 41200/85641 [00:38<00:41, 1068.35it/s] 48%|████▊     | 41307/85641 [00:38<00:42, 1050.16it/s] 48%|████▊     | 41413/85641 [00:38<00:43, 1019.29it/s] 48%|████▊     | 41516/85641 [00:39<00:43, 1020.03it/s] 49%|████▊     | 41619/85641 [00:39<00:45, 967.17it/s]  49%|████▊     | 41724/85641 [00:39<00:44, 988.68it/s] 49%|████▉     | 41839/85641 [00:39<00:42, 1033.58it/s] 49%|████▉     | 41953/85641 [00:39<00:41, 1062.05it/s] 49%|████▉     | 42060/85641 [00:39<00:42, 1032.54it/s] 49%|████▉     | 42164/85641 [00:39<00:42, 1012.88it/s] 49%|████▉     | 42280/85641 [00:39<00:41, 1046.50it/s] 50%|████▉     | 42405/85641 [00:39<00:39, 1102.05it/s] 50%|████▉     | 42516/85641 [00:40<00:41, 1034.74it/s] 50%|████▉     | 42635/85641 [00:40<00:39, 1078.13it/s] 50%|████▉     | 42757/85641 [00:40<00:38, 1114.03it/s] 50%|█████     | 42870/85641 [00:40<00:38, 1101.47it/s] 50%|█████     | 42981/85641 [00:40<00:39, 1078.03it/s] 50%|█████     | 43090/85641 [00:40<00:39, 1078.29it/s] 50%|█████     | 43212/85641 [00:40<00:37, 1117.97it/s] 51%|█████     | 43325/85641 [00:40<00:40, 1051.18it/s] 51%|█████     | 43432/85641 [00:40<00:40, 1050.94it/s] 51%|█████     | 43538/85641 [00:40<00:40, 1031.92it/s] 51%|█████     | 43650/85641 [00:41<00:39, 1054.30it/s] 51%|█████     | 43760/85641 [00:41<00:39, 1067.10it/s] 51%|█████     | 43868/85641 [00:41<00:39, 1063.28it/s] 51%|█████▏    | 43975/85641 [00:41<00:39, 1050.29it/s] 51%|█████▏    | 44081/85641 [00:41<00:40, 1032.63it/s] 52%|█████▏    | 44190/85641 [00:41<00:39, 1038.44it/s] 52%|█████▏    | 44310/85641 [00:41<00:38, 1084.37it/s] 52%|█████▏    | 44422/85641 [00:41<00:37, 1089.84it/s] 52%|█████▏    | 44532/85641 [00:41<00:38, 1074.26it/s] 52%|█████▏    | 44640/85641 [00:42<00:39, 1033.67it/s] 52%|█████▏    | 44758/85641 [00:42<00:38, 1074.96it/s] 52%|█████▏    | 44866/85641 [00:42<00:38, 1070.71it/s] 53%|█████▎    | 44982/85641 [00:42<00:37, 1093.49it/s] 53%|█████▎    | 45103/85641 [00:42<00:36, 1125.01it/s] 53%|█████▎    | 45216/85641 [00:42<00:36, 1097.24it/s] 53%|█████▎    | 45341/85641 [00:42<00:35, 1139.05it/s] 53%|█████▎    | 45456/85641 [00:42<00:35, 1122.77it/s] 53%|█████▎    | 45569/85641 [00:42<00:35, 1122.75it/s] 53%|█████▎    | 45682/85641 [00:42<00:35, 1110.56it/s] 53%|█████▎    | 45800/85641 [00:43<00:35, 1130.78it/s] 54%|█████▎    | 45914/85641 [00:43<00:36, 1096.24it/s] 54%|█████▎    | 46028/85641 [00:43<00:35, 1107.07it/s] 54%|█████▍    | 46139/85641 [00:43<00:37, 1043.96it/s] 54%|█████▍    | 46245/85641 [00:43<00:38, 1029.28it/s] 54%|█████▍    | 46349/85641 [00:43<00:38, 1028.05it/s] 54%|█████▍    | 46453/85641 [00:43<00:39, 993.27it/s]  54%|█████▍    | 46561/85641 [00:43<00:38, 1016.00it/s] 55%|█████▍    | 46694/85641 [00:43<00:35, 1106.01it/s] 55%|█████▍    | 46812/85641 [00:44<00:34, 1127.45it/s] 55%|█████▍    | 46926/85641 [00:44<00:34, 1115.99it/s] 55%|█████▍    | 47038/85641 [00:44<00:34, 1114.47it/s] 55%|█████▌    | 47150/85641 [00:44<00:35, 1087.23it/s] 55%|█████▌    | 47260/85641 [00:44<00:36, 1065.05it/s] 55%|█████▌    | 47367/85641 [00:44<00:36, 1056.49it/s] 55%|█████▌    | 47473/85641 [00:44<00:36, 1051.19it/s] 56%|█████▌    | 47579/85641 [00:44<00:36, 1046.35it/s] 56%|█████▌    | 47684/85641 [00:44<00:36, 1043.70it/s] 56%|█████▌    | 47792/85641 [00:44<00:35, 1053.84it/s] 56%|█████▌    | 47898/85641 [00:45<00:36, 1024.41it/s] 56%|█████▌    | 48014/85641 [00:45<00:35, 1056.42it/s] 56%|█████▌    | 48120/85641 [00:45<00:36, 1037.07it/s] 56%|█████▋    | 48224/85641 [00:45<00:36, 1015.20it/s] 56%|█████▋    | 48358/85641 [00:45<00:33, 1107.81it/s] 57%|█████▋    | 48470/85641 [00:45<00:34, 1079.02it/s] 57%|█████▋    | 48579/85641 [00:45<00:34, 1081.59it/s] 57%|█████▋    | 48688/85641 [00:45<00:35, 1055.59it/s] 57%|█████▋    | 48814/85641 [00:45<00:33, 1102.41it/s] 57%|█████▋    | 48938/85641 [00:45<00:32, 1139.52it/s] 57%|█████▋    | 49057/85641 [00:46<00:31, 1153.32it/s] 57%|█████▋    | 49173/85641 [00:46<00:31, 1148.01it/s] 58%|█████▊    | 49295/85641 [00:46<00:31, 1165.49it/s] 58%|█████▊    | 49412/85641 [00:46<00:32, 1114.94it/s] 58%|█████▊    | 49525/85641 [00:46<00:32, 1113.21it/s] 58%|█████▊    | 49655/85641 [00:46<00:30, 1164.30it/s] 58%|█████▊    | 49772/85641 [00:46<00:31, 1154.73it/s] 58%|█████▊    | 49888/85641 [00:46<00:32, 1095.93it/s] 58%|█████▊    | 49999/85641 [00:46<00:32, 1083.45it/s] 59%|█████▊    | 50122/85641 [00:47<00:31, 1125.00it/s] 59%|█████▊    | 50236/85641 [00:47<00:33, 1066.60it/s] 59%|█████▉    | 50344/85641 [00:47<00:33, 1047.66it/s] 59%|█████▉    | 50450/85641 [00:47<00:33, 1040.19it/s] 59%|█████▉    | 50555/85641 [00:47<00:33, 1034.79it/s] 59%|█████▉    | 50659/85641 [00:47<00:33, 1031.42it/s] 59%|█████▉    | 50763/85641 [00:47<00:35, 984.63it/s]  59%|█████▉    | 50868/85641 [00:47<00:34, 1001.80it/s] 60%|█████▉    | 50969/85641 [00:47<00:34, 999.42it/s]  60%|█████▉    | 51070/85641 [00:48<00:35, 983.58it/s] 60%|█████▉    | 51171/85641 [00:48<00:35, 982.37it/s] 60%|█████▉    | 51270/85641 [00:48<00:35, 979.62it/s] 60%|█████▉    | 51370/85641 [00:48<00:34, 985.28it/s] 60%|██████    | 51480/85641 [00:48<00:33, 1018.61it/s] 60%|██████    | 51594/85641 [00:48<00:32, 1050.63it/s] 60%|██████    | 51700/85641 [00:48<00:33, 1004.53it/s] 61%|██████    | 51816/85641 [00:48<00:32, 1046.38it/s] 61%|██████    | 51936/85641 [00:48<00:30, 1090.60it/s] 61%|██████    | 52046/85641 [00:48<00:30, 1086.92it/s] 61%|██████    | 52156/85641 [00:49<00:31, 1067.42it/s] 61%|██████    | 52264/85641 [00:49<00:31, 1069.83it/s] 61%|██████    | 52372/85641 [00:49<00:31, 1064.18it/s] 61%|██████▏   | 52493/85641 [00:49<00:30, 1100.45it/s] 61%|██████▏   | 52604/85641 [00:49<00:30, 1091.60it/s] 62%|██████▏   | 52721/85641 [00:49<00:29, 1113.67it/s] 62%|██████▏   | 52833/85641 [00:49<00:30, 1082.90it/s] 62%|██████▏   | 52945/85641 [00:49<00:30, 1089.74it/s] 62%|██████▏   | 53055/85641 [00:49<00:30, 1061.03it/s] 62%|██████▏   | 53162/85641 [00:49<00:31, 1044.85it/s] 62%|██████▏   | 53274/85641 [00:50<00:30, 1065.95it/s] 62%|██████▏   | 53381/85641 [00:50<00:31, 1038.22it/s] 62%|██████▏   | 53486/85641 [00:50<00:31, 1007.43it/s] 63%|██████▎   | 53588/85641 [00:50<00:32, 978.77it/s]  63%|██████▎   | 53687/85641 [00:50<00:32, 979.44it/s] 63%|██████▎   | 53802/85641 [00:50<00:30, 1027.59it/s] 63%|██████▎   | 53906/85641 [00:50<00:30, 1028.63it/s] 63%|██████▎   | 54010/85641 [00:50<00:32, 988.35it/s]  63%|██████▎   | 54110/85641 [00:50<00:32, 963.98it/s] 63%|██████▎   | 54213/85641 [00:51<00:32, 980.77it/s] 63%|██████▎   | 54312/85641 [00:51<00:31, 980.98it/s] 64%|██████▎   | 54432/85641 [00:51<00:29, 1042.98it/s] 64%|██████▎   | 54545/85641 [00:51<00:29, 1066.27it/s] 64%|██████▍   | 54652/85641 [00:51<00:29, 1036.19it/s] 64%|██████▍   | 54756/85641 [00:51<00:30, 1023.71it/s] 64%|██████▍   | 54868/85641 [00:51<00:29, 1040.73it/s] 64%|██████▍   | 54975/85641 [00:51<00:29, 1047.32it/s] 64%|██████▍   | 55086/85641 [00:51<00:28, 1064.23it/s] 64%|██████▍   | 55194/85641 [00:51<00:28, 1067.18it/s] 65%|██████▍   | 55303/85641 [00:52<00:28, 1070.57it/s] 65%|██████▍   | 55411/85641 [00:52<00:29, 1014.63it/s] 65%|██████▍   | 55514/85641 [00:52<00:30, 1000.32it/s] 65%|██████▍   | 55622/85641 [00:52<00:29, 1022.63it/s] 65%|██████▌   | 55725/85641 [00:52<00:29, 1017.94it/s] 65%|██████▌   | 55830/85641 [00:52<00:29, 1027.14it/s] 65%|██████▌   | 55935/85641 [00:52<00:28, 1032.53it/s] 65%|██████▌   | 56039/85641 [00:52<00:29, 991.93it/s]  66%|██████▌   | 56141/85641 [00:52<00:29, 999.79it/s] 66%|██████▌   | 56252/85641 [00:53<00:28, 1024.58it/s] 66%|██████▌   | 56362/85641 [00:53<00:28, 1036.52it/s] 66%|██████▌   | 56480/85641 [00:53<00:27, 1072.18it/s] 66%|██████▌   | 56594/85641 [00:53<00:26, 1088.67it/s] 66%|██████▌   | 56704/85641 [00:53<00:26, 1071.92it/s] 66%|██████▋   | 56812/85641 [00:53<00:27, 1053.15it/s] 66%|██████▋   | 56918/85641 [00:53<00:27, 1037.59it/s] 67%|██████▋   | 57032/85641 [00:53<00:27, 1059.36it/s] 67%|██████▋   | 57139/85641 [00:53<00:27, 1055.05it/s] 67%|██████▋   | 57254/85641 [00:53<00:26, 1079.59it/s] 67%|██████▋   | 57363/85641 [00:54<00:26, 1055.48it/s] 67%|██████▋   | 57469/85641 [00:54<00:26, 1047.97it/s] 67%|██████▋   | 57574/85641 [00:54<00:26, 1041.01it/s] 67%|██████▋   | 57679/85641 [00:54<00:27, 1035.12it/s] 67%|██████▋   | 57783/85641 [00:54<00:27, 1000.86it/s] 68%|██████▊   | 57896/85641 [00:54<00:26, 1036.19it/s] 68%|██████▊   | 58000/85641 [00:54<00:27, 1013.24it/s] 68%|██████▊   | 58102/85641 [00:54<00:28, 966.33it/s]  68%|██████▊   | 58207/85641 [00:54<00:27, 985.04it/s] 68%|██████▊   | 58316/85641 [00:55<00:26, 1012.62it/s] 68%|██████▊   | 58430/85641 [00:55<00:25, 1048.33it/s] 68%|██████▊   | 58539/85641 [00:55<00:25, 1060.32it/s] 68%|██████▊   | 58646/85641 [00:55<00:26, 1018.77it/s] 69%|██████▊   | 58771/85641 [00:55<00:24, 1082.40it/s] 69%|██████▉   | 58880/85641 [00:55<00:25, 1058.19it/s] 69%|██████▉   | 58993/85641 [00:55<00:24, 1074.40it/s] 69%|██████▉   | 59101/85641 [00:55<00:25, 1059.95it/s] 69%|██████▉   | 59208/85641 [00:55<00:24, 1060.55it/s] 69%|██████▉   | 59315/85641 [00:55<00:25, 1052.35it/s] 69%|██████▉   | 59422/85641 [00:56<00:24, 1055.68it/s] 70%|██████▉   | 59533/85641 [00:56<00:24, 1065.43it/s] 70%|██████▉   | 59640/85641 [00:56<00:25, 1026.86it/s] 70%|██████▉   | 59758/85641 [00:56<00:24, 1070.32it/s] 70%|██████▉   | 59866/85641 [00:56<00:24, 1070.97it/s] 70%|███████   | 59984/85641 [00:56<00:23, 1101.76it/s] 70%|███████   | 60108/85641 [00:56<00:22, 1129.04it/s] 70%|███████   | 60222/85641 [00:56<00:22, 1105.96it/s] 70%|███████   | 60333/85641 [00:56<00:23, 1096.48it/s] 71%|███████   | 60449/85641 [00:56<00:22, 1114.97it/s] 71%|███████   | 60578/85641 [00:57<00:21, 1157.29it/s] 71%|███████   | 60694/85641 [00:57<00:21, 1144.28it/s] 71%|███████   | 60810/85641 [00:57<00:21, 1147.30it/s] 71%|███████   | 60925/85641 [00:57<00:22, 1119.66it/s] 71%|███████▏  | 61038/85641 [00:57<00:22, 1117.16it/s] 71%|███████▏  | 61150/85641 [00:57<00:22, 1086.59it/s] 72%|███████▏  | 61259/85641 [00:57<00:23, 1043.34it/s] 72%|███████▏  | 61383/85641 [00:57<00:22, 1097.69it/s] 72%|███████▏  | 61494/85641 [00:57<00:22, 1061.34it/s] 72%|███████▏  | 61607/85641 [00:58<00:22, 1080.44it/s] 72%|███████▏  | 61716/85641 [00:58<00:22, 1074.21it/s] 72%|███████▏  | 61824/85641 [00:58<00:22, 1042.62it/s] 72%|███████▏  | 61937/85641 [00:58<00:22, 1064.47it/s] 72%|███████▏  | 62044/85641 [00:58<00:22, 1053.02it/s] 73%|███████▎  | 62161/85641 [00:58<00:21, 1083.55it/s] 73%|███████▎  | 62270/85641 [00:58<00:22, 1053.29it/s] 73%|███████▎  | 62376/85641 [00:58<00:22, 1017.43it/s] 73%|███████▎  | 62479/85641 [00:58<00:23, 997.08it/s]  73%|███████▎  | 62604/85641 [00:58<00:21, 1068.12it/s] 73%|███████▎  | 62723/85641 [00:59<00:20, 1100.72it/s] 73%|███████▎  | 62836/85641 [00:59<00:20, 1106.81it/s] 74%|███████▎  | 62957/85641 [00:59<00:19, 1135.57it/s] 74%|███████▎  | 63071/85641 [00:59<00:21, 1057.47it/s] 74%|███████▍  | 63180/85641 [00:59<00:21, 1066.24it/s] 74%|███████▍  | 63288/85641 [00:59<00:21, 1039.66it/s] 74%|███████▍  | 63405/85641 [00:59<00:20, 1075.90it/s] 74%|███████▍  | 63514/85641 [00:59<00:20, 1058.50it/s] 74%|███████▍  | 63621/85641 [00:59<00:20, 1052.69it/s] 74%|███████▍  | 63727/85641 [01:00<00:21, 1040.65it/s] 75%|███████▍  | 63832/85641 [01:00<00:21, 1025.76it/s] 75%|███████▍  | 63935/85641 [01:00<00:22, 975.37it/s]  75%|███████▍  | 64039/85641 [01:00<00:21, 992.47it/s] 75%|███████▍  | 64146/85641 [01:00<00:21, 1014.30it/s] 75%|███████▌  | 64248/85641 [01:00<00:21, 1008.86it/s] 75%|███████▌  | 64372/85641 [01:00<00:19, 1075.19it/s] 75%|███████▌  | 64488/85641 [01:00<00:19, 1095.27it/s] 75%|███████▌  | 64598/85641 [01:00<00:19, 1067.73it/s] 76%|███████▌  | 64724/85641 [01:00<00:18, 1123.13it/s] 76%|███████▌  | 64837/85641 [01:01<00:19, 1076.79it/s] 76%|███████▌  | 64946/85641 [01:01<00:20, 1024.70it/s] 76%|███████▌  | 65050/85641 [01:01<00:20, 985.12it/s]  76%|███████▌  | 65167/85641 [01:01<00:19, 1035.67it/s] 76%|███████▌  | 65272/85641 [01:01<00:19, 1027.29it/s] 76%|███████▋  | 65376/85641 [01:01<00:19, 1013.37it/s] 76%|███████▋  | 65494/85641 [01:01<00:18, 1060.55it/s] 77%|███████▋  | 65601/85641 [01:01<00:18, 1062.92it/s] 77%|███████▋  | 65708/85641 [01:01<00:18, 1050.72it/s] 77%|███████▋  | 65814/85641 [01:02<00:19, 995.49it/s]  77%|███████▋  | 65929/85641 [01:02<00:19, 1036.83it/s] 77%|███████▋  | 66040/85641 [01:02<00:18, 1055.82it/s] 77%|███████▋  | 66147/85641 [01:02<00:18, 1058.00it/s] 77%|███████▋  | 66254/85641 [01:02<00:18, 1049.49it/s] 77%|███████▋  | 66360/85641 [01:02<00:18, 1045.12it/s] 78%|███████▊  | 66477/85641 [01:02<00:17, 1078.34it/s] 78%|███████▊  | 66586/85641 [01:02<00:17, 1068.29it/s] 78%|███████▊  | 66709/85641 [01:02<00:17, 1111.01it/s] 78%|███████▊  | 66821/85641 [01:02<00:16, 1113.10it/s] 78%|███████▊  | 66949/85641 [01:03<00:16, 1160.50it/s] 78%|███████▊  | 67066/85641 [01:03<00:16, 1120.42it/s] 78%|███████▊  | 67179/85641 [01:03<00:17, 1078.60it/s] 79%|███████▊  | 67288/85641 [01:03<00:17, 1042.22it/s] 79%|███████▊  | 67393/85641 [01:03<00:17, 1039.04it/s] 79%|███████▉  | 67503/85641 [01:03<00:17, 1054.78it/s] 79%|███████▉  | 67609/85641 [01:03<00:17, 1042.93it/s] 79%|███████▉  | 67727/85641 [01:03<00:16, 1079.88it/s] 79%|███████▉  | 67836/85641 [01:03<00:17, 1007.34it/s] 79%|███████▉  | 67943/85641 [01:04<00:17, 1020.50it/s] 79%|███████▉  | 68062/85641 [01:04<00:16, 1066.94it/s] 80%|███████▉  | 68170/85641 [01:04<00:16, 1038.64it/s] 80%|███████▉  | 68286/85641 [01:04<00:16, 1073.02it/s] 80%|███████▉  | 68394/85641 [01:04<00:16, 1056.24it/s] 80%|███████▉  | 68501/85641 [01:04<00:16, 1041.01it/s] 80%|████████  | 68610/85641 [01:04<00:16, 1054.68it/s] 80%|████████  | 68725/85641 [01:04<00:15, 1078.90it/s] 80%|████████  | 68834/85641 [01:04<00:15, 1078.56it/s] 81%|████████  | 68951/85641 [01:04<00:15, 1104.77it/s] 81%|████████  | 69062/85641 [01:05<00:15, 1100.55it/s] 81%|████████  | 69173/85641 [01:05<00:16, 1017.89it/s] 81%|████████  | 69277/85641 [01:05<00:16, 978.82it/s]  81%|████████  | 69376/85641 [01:05<00:16, 964.83it/s] 81%|████████  | 69491/85641 [01:05<00:15, 1016.47it/s] 81%|████████▏ | 69594/85641 [01:05<00:16, 992.01it/s]  81%|████████▏ | 69712/85641 [01:05<00:15, 1042.29it/s] 82%|████████▏ | 69817/85641 [01:05<00:15, 998.91it/s]  82%|████████▏ | 69936/85641 [01:05<00:14, 1051.02it/s] 82%|████████▏ | 70050/85641 [01:06<00:14, 1072.17it/s] 82%|████████▏ | 70158/85641 [01:06<00:14, 1063.18it/s] 82%|████████▏ | 70265/85641 [01:06<00:14, 1037.31it/s] 82%|████████▏ | 70380/85641 [01:06<00:14, 1068.85it/s] 82%|████████▏ | 70488/85641 [01:06<00:14, 1047.52it/s] 82%|████████▏ | 70594/85641 [01:06<00:14, 1008.50it/s] 83%|████████▎ | 70705/85641 [01:06<00:14, 1035.11it/s] 83%|████████▎ | 70809/85641 [01:06<00:14, 1021.22it/s] 83%|████████▎ | 70924/85641 [01:06<00:13, 1058.07it/s] 83%|████████▎ | 71031/85641 [01:07<00:14, 1016.09it/s] 83%|████████▎ | 71136/85641 [01:07<00:14, 1024.00it/s] 83%|████████▎ | 71239/85641 [01:07<00:14, 1018.83it/s] 83%|████████▎ | 71356/85641 [01:07<00:13, 1059.78it/s] 83%|████████▎ | 71466/85641 [01:07<00:13, 1060.12it/s] 84%|████████▎ | 71573/85641 [01:07<00:14, 997.97it/s]  84%|████████▎ | 71680/85641 [01:07<00:13, 1018.08it/s] 84%|████████▍ | 71783/85641 [01:07<00:14, 980.68it/s]  84%|████████▍ | 71882/85641 [01:07<00:14, 974.10it/s] 84%|████████▍ | 72000/85641 [01:07<00:13, 1029.14it/s] 84%|████████▍ | 72109/85641 [01:08<00:12, 1046.49it/s] 84%|████████▍ | 72218/85641 [01:08<00:12, 1057.25it/s] 84%|████████▍ | 72325/85641 [01:08<00:12, 1048.33it/s] 85%|████████▍ | 72444/85641 [01:08<00:12, 1089.78it/s] 85%|████████▍ | 72554/85641 [01:08<00:12, 1077.40it/s] 85%|████████▍ | 72662/85641 [01:08<00:12, 1077.42it/s] 85%|████████▍ | 72770/85641 [01:08<00:12, 1052.75it/s] 85%|████████▌ | 72886/85641 [01:08<00:11, 1072.71it/s] 85%|████████▌ | 73004/85641 [01:08<00:11, 1094.89it/s] 85%|████████▌ | 73114/85641 [01:08<00:11, 1073.00it/s] 86%|████████▌ | 73236/85641 [01:09<00:11, 1110.97it/s] 86%|████████▌ | 73376/85641 [01:09<00:10, 1188.51it/s] 86%|████████▌ | 73506/85641 [01:09<00:09, 1217.94it/s] 86%|████████▌ | 73628/85641 [01:09<00:10, 1178.70it/s] 86%|████████▌ | 73747/85641 [01:09<00:10, 1143.78it/s] 86%|████████▌ | 73862/85641 [01:09<00:10, 1137.17it/s] 86%|████████▋ | 73976/85641 [01:09<00:10, 1107.42it/s] 87%|████████▋ | 74087/85641 [01:09<00:10, 1099.09it/s] 87%|████████▋ | 74206/85641 [01:09<00:10, 1125.24it/s] 87%|████████▋ | 74319/85641 [01:10<00:10, 1115.83it/s] 87%|████████▋ | 74431/85641 [01:10<00:10, 1055.62it/s] 87%|████████▋ | 74563/85641 [01:10<00:09, 1129.48it/s] 87%|████████▋ | 74677/85641 [01:10<00:10, 1037.52it/s] 87%|████████▋ | 74785/85641 [01:10<00:10, 1047.01it/s] 87%|████████▋ | 74892/85641 [01:10<00:10, 999.59it/s]  88%|████████▊ | 74994/85641 [01:10<00:10, 970.50it/s] 88%|████████▊ | 75110/85641 [01:10<00:10, 1018.05it/s] 88%|████████▊ | 75213/85641 [01:10<00:10, 1021.00it/s] 88%|████████▊ | 75316/85641 [01:11<00:10, 1012.46it/s] 88%|████████▊ | 75424/85641 [01:11<00:09, 1030.07it/s] 88%|████████▊ | 75552/85641 [01:11<00:09, 1101.86it/s] 88%|████████▊ | 75666/85641 [01:11<00:08, 1109.81it/s] 88%|████████▊ | 75778/85641 [01:11<00:09, 1093.58it/s] 89%|████████▊ | 75899/85641 [01:11<00:08, 1126.37it/s] 89%|████████▉ | 76012/85641 [01:11<00:08, 1098.95it/s] 89%|████████▉ | 76128/85641 [01:11<00:08, 1114.37it/s] 89%|████████▉ | 76240/85641 [01:11<00:08, 1067.81it/s] 89%|████████▉ | 76348/85641 [01:11<00:08, 1040.04it/s] 89%|████████▉ | 76453/85641 [01:12<00:09, 994.30it/s]  89%|████████▉ | 76561/85641 [01:12<00:08, 1017.95it/s] 90%|████████▉ | 76664/85641 [01:12<00:09, 996.66it/s]  90%|████████▉ | 76772/85641 [01:12<00:08, 1015.50it/s] 90%|████████▉ | 76882/85641 [01:12<00:08, 1039.43it/s] 90%|████████▉ | 76995/85641 [01:12<00:08, 1064.20it/s] 90%|█████████ | 77115/85641 [01:12<00:07, 1102.29it/s] 90%|█████████ | 77226/85641 [01:12<00:07, 1082.51it/s] 90%|█████████ | 77335/85641 [01:12<00:07, 1082.36it/s] 90%|█████████ | 77444/85641 [01:13<00:07, 1061.91it/s] 91%|█████████ | 77553/85641 [01:13<00:07, 1057.45it/s] 91%|█████████ | 77660/85641 [01:13<00:07, 1059.59it/s] 91%|█████████ | 77767/85641 [01:13<00:07, 1045.13it/s] 91%|█████████ | 77892/85641 [01:13<00:07, 1103.78it/s] 91%|█████████ | 78003/85641 [01:13<00:06, 1095.27it/s] 91%|█████████ | 78113/85641 [01:13<00:06, 1079.48it/s] 91%|█████████▏| 78222/85641 [01:13<00:06, 1075.06it/s] 91%|█████████▏| 78330/85641 [01:13<00:06, 1066.89it/s] 92%|█████████▏| 78445/85641 [01:13<00:06, 1087.11it/s] 92%|█████████▏| 78554/85641 [01:14<00:06, 1077.59it/s] 92%|█████████▏| 78670/85641 [01:14<00:06, 1097.91it/s] 92%|█████████▏| 78786/85641 [01:14<00:06, 1116.16it/s] 92%|█████████▏| 78898/85641 [01:14<00:06, 1059.84it/s] 92%|█████████▏| 79021/85641 [01:14<00:06, 1101.39it/s] 92%|█████████▏| 79132/85641 [01:14<00:06, 1060.30it/s] 93%|█████████▎| 79239/85641 [01:14<00:06, 1038.35it/s] 93%|█████████▎| 79351/85641 [01:14<00:05, 1059.16it/s] 93%|█████████▎| 79478/85641 [01:14<00:05, 1119.26it/s] 93%|█████████▎| 79591/85641 [01:14<00:05, 1120.53it/s] 93%|█████████▎| 79704/85641 [01:15<00:05, 1101.32it/s] 93%|█████████▎| 79815/85641 [01:15<00:05, 1081.53it/s] 93%|█████████▎| 79924/85641 [01:15<00:05, 1053.85it/s] 93%|█████████▎| 80030/85641 [01:15<00:05, 1009.20it/s] 94%|█████████▎| 80151/85641 [01:15<00:05, 1065.63it/s] 94%|█████████▎| 80259/85641 [01:15<00:05, 1031.49it/s] 94%|█████████▍| 80370/85641 [01:15<00:05, 1050.46it/s] 94%|█████████▍| 80476/85641 [01:15<00:05, 1020.01it/s] 94%|█████████▍| 80579/85641 [01:15<00:05, 979.14it/s]  94%|█████████▍| 80679/85641 [01:16<00:05, 983.42it/s] 94%|█████████▍| 80778/85641 [01:16<00:05, 962.35it/s] 94%|█████████▍| 80895/85641 [01:16<00:04, 1021.24it/s] 95%|█████████▍| 80998/85641 [01:16<00:04, 1004.18it/s] 95%|█████████▍| 81110/85641 [01:16<00:04, 1037.01it/s] 95%|█████████▍| 81216/85641 [01:16<00:04, 1041.71it/s] 95%|█████████▍| 81324/85641 [01:16<00:04, 1050.71it/s] 95%|█████████▌| 81438/85641 [01:16<00:03, 1072.41it/s] 95%|█████████▌| 81557/85641 [01:16<00:03, 1100.90it/s] 95%|█████████▌| 81669/85641 [01:16<00:03, 1101.48it/s] 95%|█████████▌| 81781/85641 [01:17<00:03, 1104.81it/s] 96%|█████████▌| 81892/85641 [01:17<00:03, 1077.91it/s] 96%|█████████▌| 82007/85641 [01:17<00:03, 1095.81it/s] 96%|█████████▌| 82117/85641 [01:17<00:03, 1091.38it/s] 96%|█████████▌| 82227/85641 [01:17<00:03, 1062.10it/s] 96%|█████████▌| 82353/85641 [01:17<00:02, 1118.79it/s] 96%|█████████▋| 82466/85641 [01:17<00:02, 1118.42it/s] 96%|█████████▋| 82579/85641 [01:17<00:02, 1089.86it/s] 97%|█████████▋| 82689/85641 [01:17<00:02, 1014.02it/s] 97%|█████████▋| 82792/85641 [01:18<00:02, 1000.58it/s] 97%|█████████▋| 82903/85641 [01:18<00:02, 1030.55it/s] 97%|█████████▋| 83031/85641 [01:18<00:02, 1101.03it/s] 97%|█████████▋| 83143/85641 [01:18<00:02, 1103.93it/s] 97%|█████████▋| 83254/85641 [01:18<00:02, 1100.49it/s] 97%|█████████▋| 83365/85641 [01:18<00:02, 1098.22it/s] 97%|█████████▋| 83476/85641 [01:18<00:01, 1092.78it/s] 98%|█████████▊| 83595/85641 [01:18<00:01, 1119.61it/s] 98%|█████████▊| 83708/85641 [01:18<00:01, 1073.79it/s] 98%|█████████▊| 83816/85641 [01:19<00:01, 1012.40it/s] 98%|█████████▊| 83919/85641 [01:19<00:01, 1008.75it/s] 98%|█████████▊| 84041/85641 [01:19<00:01, 1065.33it/s] 98%|█████████▊| 84149/85641 [01:19<00:01, 1063.75it/s] 98%|█████████▊| 84256/85641 [01:19<00:01, 990.93it/s]  99%|█████████▊| 84357/85641 [01:19<00:01, 989.75it/s] 99%|█████████▊| 84486/85641 [01:19<00:01, 1074.67it/s] 99%|█████████▉| 84595/85641 [01:19<00:00, 1071.72it/s] 99%|█████████▉| 84720/85641 [01:19<00:00, 1121.21it/s] 99%|█████████▉| 84833/85641 [01:19<00:00, 1078.81it/s] 99%|█████████▉| 84942/85641 [01:20<00:00, 1051.24it/s] 99%|█████████▉| 85048/85641 [01:20<00:00, 1048.76it/s] 99%|█████████▉| 85160/85641 [01:20<00:00, 1065.81it/s]100%|█████████▉| 85284/85641 [01:20<00:00, 1115.77it/s]100%|█████████▉| 85396/85641 [01:20<00:00, 1083.43it/s]100%|█████████▉| 85505/85641 [01:20<00:00, 1044.42it/s]100%|█████████▉| 85610/85641 [01:20<00:00, 1020.34it/s]100%|██████████| 85641/85641 [01:20<00:00, 1060.66it/s]
Shape of temporal_X: (85641, 2, 1000)
  0%|          | 0/9516 [00:00<?, ?it/s]  1%|          | 88/9516 [00:00<00:10, 872.53it/s]  2%|▏         | 187/9516 [00:00<00:09, 938.55it/s]  3%|▎         | 281/9516 [00:00<00:10, 896.10it/s]  4%|▍         | 383/9516 [00:00<00:09, 943.32it/s]  5%|▌         | 485/9516 [00:00<00:09, 966.28it/s]  6%|▌         | 582/9516 [00:00<00:09, 961.28it/s]  7%|▋         | 679/9516 [00:00<00:09, 939.89it/s]  8%|▊         | 789/9516 [00:00<00:08, 986.10it/s]  9%|▉         | 897/9516 [00:00<00:08, 1014.07it/s] 10%|█         | 999/9516 [00:01<00:08, 982.27it/s]  12%|█▏        | 1111/9516 [00:01<00:08, 1020.59it/s] 13%|█▎        | 1220/9516 [00:01<00:07, 1038.54it/s] 14%|█▍        | 1328/9516 [00:01<00:07, 1046.59it/s] 15%|█▌        | 1433/9516 [00:01<00:07, 1014.12it/s] 16%|█▌        | 1542/9516 [00:01<00:07, 1034.41it/s] 17%|█▋        | 1646/9516 [00:01<00:07, 1009.26it/s] 18%|█▊        | 1755/9516 [00:01<00:07, 1029.23it/s] 20%|█▉        | 1867/9516 [00:01<00:07, 1049.93it/s] 21%|██        | 1978/9516 [00:01<00:07, 1061.07it/s] 22%|██▏       | 2085/9516 [00:02<00:07, 1052.17it/s] 23%|██▎       | 2201/9516 [00:02<00:06, 1083.56it/s] 24%|██▍       | 2310/9516 [00:02<00:06, 1037.30it/s] 25%|██▌       | 2415/9516 [00:02<00:07, 998.55it/s]  26%|██▋       | 2516/9516 [00:02<00:07, 983.91it/s] 28%|██▊       | 2619/9516 [00:02<00:06, 995.41it/s] 29%|██▊       | 2719/9516 [00:02<00:06, 991.92it/s] 30%|██▉       | 2836/9516 [00:02<00:06, 1042.14it/s] 31%|███       | 2941/9516 [00:02<00:06, 1031.94it/s] 32%|███▏      | 3046/9516 [00:03<00:06, 1033.01it/s] 33%|███▎      | 3150/9516 [00:03<00:06, 960.43it/s]  34%|███▍      | 3248/9516 [00:03<00:06, 960.42it/s] 35%|███▌      | 3345/9516 [00:03<00:06, 953.91it/s] 36%|███▌      | 3441/9516 [00:03<00:06, 941.42it/s] 37%|███▋      | 3554/9516 [00:03<00:05, 994.10it/s] 39%|███▉      | 3690/9516 [00:03<00:05, 1096.71it/s] 40%|███▉      | 3801/9516 [00:03<00:05, 1054.15it/s] 41%|████      | 3908/9516 [00:03<00:05, 1027.28it/s] 42%|████▏     | 4014/9516 [00:03<00:05, 1034.29it/s] 43%|████▎     | 4118/9516 [00:04<00:05, 992.34it/s]  45%|████▍     | 4264/9516 [00:04<00:04, 1122.67it/s] 46%|████▌     | 4378/9516 [00:04<00:04, 1088.15it/s] 47%|████▋     | 4488/9516 [00:04<00:04, 1067.77it/s] 48%|████▊     | 4596/9516 [00:04<00:04, 1059.30it/s] 49%|████▉     | 4703/9516 [00:04<00:04, 1016.78it/s] 51%|█████     | 4806/9516 [00:04<00:04, 988.35it/s]  52%|█████▏    | 4913/9516 [00:04<00:04, 1008.19it/s] 53%|█████▎    | 5022/9516 [00:04<00:04, 1031.03it/s] 54%|█████▍    | 5126/9516 [00:05<00:04, 1014.57it/s] 55%|█████▍    | 5233/9516 [00:05<00:04, 1023.32it/s] 56%|█████▌    | 5339/9516 [00:05<00:04, 1033.76it/s] 57%|█████▋    | 5444/9516 [00:05<00:03, 1031.98it/s] 58%|█████▊    | 5559/9516 [00:05<00:03, 1064.52it/s] 60%|█████▉    | 5679/9516 [00:05<00:03, 1103.40it/s] 61%|██████    | 5790/9516 [00:05<00:03, 1077.00it/s] 62%|██████▏   | 5898/9516 [00:05<00:03, 1035.45it/s] 63%|██████▎   | 6013/9516 [00:05<00:03, 1067.19it/s] 64%|██████▍   | 6121/9516 [00:05<00:03, 1058.16it/s] 66%|██████▌   | 6238/9516 [00:06<00:03, 1090.32it/s] 67%|██████▋   | 6357/9516 [00:06<00:02, 1114.16it/s] 68%|██████▊   | 6469/9516 [00:06<00:02, 1085.04it/s] 69%|██████▉   | 6578/9516 [00:06<00:02, 1041.32it/s] 70%|███████   | 6685/9516 [00:06<00:02, 1041.38it/s] 71%|███████▏  | 6801/9516 [00:06<00:02, 1072.98it/s] 73%|███████▎  | 6911/9516 [00:06<00:02, 1074.70it/s] 74%|███████▍  | 7025/9516 [00:06<00:02, 1091.75it/s] 75%|███████▌  | 7141/9516 [00:06<00:02, 1108.95it/s] 76%|███████▌  | 7253/9516 [00:07<00:02, 1083.03it/s] 77%|███████▋  | 7362/9516 [00:07<00:02, 1069.94it/s] 78%|███████▊  | 7470/9516 [00:07<00:01, 1052.38it/s] 80%|███████▉  | 7596/9516 [00:07<00:01, 1110.30it/s] 81%|████████  | 7708/9516 [00:07<00:01, 1096.59it/s] 82%|████████▏ | 7820/9516 [00:07<00:01, 1101.56it/s] 83%|████████▎ | 7934/9516 [00:07<00:01, 1112.56it/s] 85%|████████▍ | 8046/9516 [00:07<00:01, 1063.13it/s] 86%|████████▌ | 8162/9516 [00:07<00:01, 1087.72it/s] 87%|████████▋ | 8275/9516 [00:07<00:01, 1097.77it/s] 88%|████████▊ | 8386/9516 [00:08<00:01, 1017.91it/s] 89%|████████▉ | 8498/9516 [00:08<00:00, 1044.34it/s] 91%|█████████ | 8632/9516 [00:08<00:00, 1127.35it/s] 92%|█████████▏| 8746/9516 [00:08<00:00, 1099.80it/s] 93%|█████████▎| 8857/9516 [00:08<00:00, 1084.76it/s] 94%|█████████▍| 8967/9516 [00:08<00:00, 1059.00it/s] 95%|█████████▌| 9074/9516 [00:08<00:00, 1017.55it/s] 97%|█████████▋| 9212/9516 [00:08<00:00, 1116.03it/s] 98%|█████████▊| 9325/9516 [00:08<00:00, 1118.72it/s] 99%|█████████▉| 9438/9516 [00:09<00:00, 1079.88it/s]100%|██████████| 9516/9516 [00:09<00:00, 1043.10it/s]
Shape of temporal_X: (9516, 2, 1000)
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Train: X=torch.Size([85641, 1, 2, 1000]), y=torch.Size([85641])
Valid: X=torch.Size([9516, 1, 2, 1000]), y=torch.Size([9516])
num_classes: 95
No pre-trained model
epoch: 0
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]going through batches for holmes training:   0%|          | 1/428 [00:02<14:26,  2.03s/it]going through batches for holmes training:   1%|▏         | 6/428 [00:02<01:53,  3.72it/s]going through batches for holmes training:   3%|▎         | 12/428 [00:02<00:49,  8.48it/s]going through batches for holmes training:   4%|▍         | 18/428 [00:02<00:29, 14.01it/s]going through batches for holmes training:   6%|▌         | 24/428 [00:02<00:20, 20.12it/s]going through batches for holmes training:   7%|▋         | 30/428 [00:02<00:15, 26.41it/s]going through batches for holmes training:   8%|▊         | 36/428 [00:02<00:12, 32.55it/s]going through batches for holmes training:  10%|▉         | 42/428 [00:02<00:10, 38.15it/s]going through batches for holmes training:  11%|█         | 48/428 [00:02<00:08, 43.02it/s]going through batches for holmes training:  13%|█▎        | 54/428 [00:02<00:07, 47.02it/s]going through batches for holmes training:  14%|█▍        | 60/428 [00:03<00:07, 50.17it/s]going through batches for holmes training:  15%|█▌        | 66/428 [00:03<00:06, 52.56it/s]going through batches for holmes training:  17%|█▋        | 72/428 [00:03<00:06, 54.32it/s]going through batches for holmes training:  18%|█▊        | 78/428 [00:03<00:06, 55.63it/s]going through batches for holmes training:  20%|█▉        | 84/428 [00:03<00:06, 56.62it/s]going through batches for holmes training:  21%|██        | 90/428 [00:03<00:05, 57.24it/s]going through batches for holmes training:  22%|██▏       | 96/428 [00:03<00:05, 57.73it/s]going through batches for holmes training:  24%|██▍       | 102/428 [00:03<00:05, 58.14it/s]going through batches for holmes training:  25%|██▌       | 108/428 [00:03<00:05, 58.38it/s]going through batches for holmes training:  27%|██▋       | 114/428 [00:03<00:05, 58.56it/s]going through batches for holmes training:  28%|██▊       | 120/428 [00:04<00:05, 58.71it/s]going through batches for holmes training:  29%|██▉       | 126/428 [00:04<00:05, 58.77it/s]going through batches for holmes training:  31%|███       | 132/428 [00:04<00:05, 58.78it/s]going through batches for holmes training:  32%|███▏      | 138/428 [00:04<00:04, 58.86it/s]going through batches for holmes training:  34%|███▎      | 144/428 [00:04<00:04, 58.80it/s]going through batches for holmes training:  35%|███▌      | 150/428 [00:04<00:04, 58.86it/s]going through batches for holmes training:  36%|███▋      | 156/428 [00:04<00:04, 58.93it/s]going through batches for holmes training:  38%|███▊      | 162/428 [00:04<00:04, 58.99it/s]going through batches for holmes training:  39%|███▉      | 168/428 [00:04<00:04, 58.97it/s]going through batches for holmes training:  41%|████      | 174/428 [00:04<00:04, 58.87it/s]going through batches for holmes training:  42%|████▏     | 180/428 [00:05<00:04, 58.85it/s]going through batches for holmes training:  43%|████▎     | 186/428 [00:05<00:04, 58.81it/s]going through batches for holmes training:  45%|████▍     | 192/428 [00:05<00:04, 58.79it/s]going through batches for holmes training:  46%|████▋     | 198/428 [00:05<00:03, 58.80it/s]going through batches for holmes training:  48%|████▊     | 204/428 [00:05<00:03, 58.85it/s]going through batches for holmes training:  49%|████▉     | 210/428 [00:05<00:03, 58.90it/s]going through batches for holmes training:  50%|█████     | 216/428 [00:05<00:03, 55.65it/s]going through batches for holmes training:  52%|█████▏    | 222/428 [00:05<00:03, 56.39it/s]going through batches for holmes training:  53%|█████▎    | 228/428 [00:05<00:03, 57.00it/s]going through batches for holmes training:  55%|█████▍    | 234/428 [00:06<00:03, 57.52it/s]going through batches for holmes training:  56%|█████▌    | 240/428 [00:06<00:03, 57.90it/s]going through batches for holmes training:  57%|█████▋    | 246/428 [00:06<00:03, 58.12it/s]going through batches for holmes training:  59%|█████▉    | 252/428 [00:06<00:03, 58.34it/s]going through batches for holmes training:  60%|██████    | 258/428 [00:06<00:02, 58.52it/s]going through batches for holmes training:  62%|██████▏   | 264/428 [00:06<00:02, 58.65it/s]going through batches for holmes training:  63%|██████▎   | 270/428 [00:06<00:02, 58.72it/s]going through batches for holmes training:  64%|██████▍   | 276/428 [00:06<00:02, 58.73it/s]going through batches for holmes training:  66%|██████▌   | 282/428 [00:06<00:02, 58.84it/s]going through batches for holmes training:  67%|██████▋   | 288/428 [00:06<00:02, 58.83it/s]going through batches for holmes training:  69%|██████▊   | 294/428 [00:07<00:02, 58.89it/s]going through batches for holmes training:  70%|███████   | 300/428 [00:07<00:02, 58.94it/s]going through batches for holmes training:  71%|███████▏  | 306/428 [00:07<00:02, 58.90it/s]going through batches for holmes training:  73%|███████▎  | 312/428 [00:07<00:01, 58.46it/s]going through batches for holmes training:  74%|███████▍  | 318/428 [00:07<00:01, 58.44it/s]going through batches for holmes training:  76%|███████▌  | 324/428 [00:07<00:01, 58.54it/s]going through batches for holmes training:  77%|███████▋  | 330/428 [00:07<00:01, 58.55it/s]going through batches for holmes training:  79%|███████▊  | 336/428 [00:07<00:01, 58.62it/s]going through batches for holmes training:  80%|███████▉  | 342/428 [00:07<00:01, 58.66it/s]going through batches for holmes training:  81%|████████▏ | 348/428 [00:07<00:01, 58.72it/s]going through batches for holmes training:  83%|████████▎ | 354/428 [00:08<00:01, 58.74it/s]going through batches for holmes training:  84%|████████▍ | 360/428 [00:08<00:01, 58.75it/s]going through batches for holmes training:  86%|████████▌ | 366/428 [00:08<00:01, 58.62it/s]going through batches for holmes training:  87%|████████▋ | 372/428 [00:08<00:00, 58.72it/s]going through batches for holmes training:  88%|████████▊ | 378/428 [00:08<00:00, 58.66it/s]going through batches for holmes training:  90%|████████▉ | 384/428 [00:08<00:00, 58.68it/s]going through batches for holmes training:  91%|█████████ | 390/428 [00:08<00:00, 58.73it/s]going through batches for holmes training:  93%|█████████▎| 396/428 [00:08<00:00, 58.76it/s]going through batches for holmes training:  94%|█████████▍| 402/428 [00:08<00:00, 58.74it/s]going through batches for holmes training:  95%|█████████▌| 408/428 [00:08<00:00, 58.77it/s]going through batches for holmes training:  97%|█████████▋| 414/428 [00:09<00:00, 58.40it/s]going through batches for holmes training:  98%|█████████▊| 420/428 [00:09<00:00, 58.55it/s]going through batches for holmes training: 100%|█████████▉| 426/428 [00:09<00:00, 58.84it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:09<00:00, 45.48it/s]
epoch 0: train_loss = 2.497
0: {'Accuracy': 0.8887, 'Precision': 0.8991, 'Recall': 0.8886, 'F1-score': 0.8887}
epoch: 1
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:14,  1.30s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:03,  6.59it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:31, 12.98it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.82it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.50it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.80it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.24it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.29it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.86it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.35it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.53it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.45it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 54.13it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 55.28it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 56.05it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:05, 56.57it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 57.07it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 57.53it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 57.81it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 58.01it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 58.27it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 58.23it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 58.19it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:04, 58.37it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 58.30it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 58.41it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 58.40it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 58.50it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 58.66it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 58.35it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 58.49it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 58.56it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 58.57it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:03, 58.42it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 58.51it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 58.59it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 58.70it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 58.57it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 58.45it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 58.53it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 58.27it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 58.34it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:02, 58.36it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 58.51it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 58.37it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:05<00:02, 58.49it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 58.54it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 58.62it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 58.60it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 58.40it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 58.58it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 58.35it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:01, 58.42it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 58.38it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 57.51it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 57.75it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 57.84it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 58.10it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 57.77it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 57.61it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 57.67it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 57.92it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 57.59it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 57.46it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:07<00:00, 57.24it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 57.43it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 57.50it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 57.61it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 57.42it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 57.51it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 57.99it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 58.19it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.96it/s]
epoch 1: train_loss = 1.085
1: {'Accuracy': 0.9264, 'Precision': 0.9329, 'Recall': 0.9264, 'F1-score': 0.927}
epoch: 2
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:26,  1.33s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:05,  6.45it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.87it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.70it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.48it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.77it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.46it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:08, 43.20it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 47.08it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 50.07it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 52.13it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 53.89it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 55.19it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 56.04it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 56.67it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:05, 57.05it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 57.40it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 57.59it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 57.90it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 58.00it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 57.98it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 58.06it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 58.15it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:04, 58.19it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 58.19it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 58.16it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 58.21it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 58.26it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 58.20it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 58.17it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 58.12it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 58.10it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 58.07it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:03, 58.06it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 58.02it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 57.93it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 58.04it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 58.10it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 57.98it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 58.09it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 58.02it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 57.88it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 58.02it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 58.11it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 57.96it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:05<00:02, 57.89it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 57.98it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 58.16it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 58.21it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 58.24it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 58.23it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 58.35it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:01, 58.30it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 58.18it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 58.21it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 58.14it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 58.27it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 58.23it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 58.30it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 58.25it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 58.32it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 58.21it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 58.23it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 58.23it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:07<00:00, 58.21it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 58.10it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 58.12it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 58.21it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 57.66it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 57.87it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 57.99it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 58.24it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.99it/s]
epoch 2: train_loss = 0.703
2: {'Accuracy': 0.9455, 'Precision': 0.9507, 'Recall': 0.9454, 'F1-score': 0.9465}
epoch: 3
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:43,  1.37s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:06,  6.29it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:33, 12.50it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:21, 19.17it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 25.71it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 31.88it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:02<00:10, 36.89it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 41.45it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 44.26it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 47.34it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 49.48it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:07, 50.98it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 51.81it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 52.44it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 53.53it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:03<00:06, 54.29it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:06, 54.59it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 55.51it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.34it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 55.68it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 55.26it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 51.46it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 52.25it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 53.10it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:04<00:05, 53.47it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:04<00:05, 53.99it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 54.44it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 54.75it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 54.98it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 55.05it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 55.14it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 55.25it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 55.31it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 55.27it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:05<00:04, 55.13it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 55.04it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 55.17it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 55.16it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 55.21it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 55.20it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 55.08it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 55.14it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 55.23it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:06<00:03, 55.19it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:06<00:02, 55.17it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 55.14it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 54.71it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 54.92it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 55.06it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 55.24it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 54.81it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 54.96it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:07<00:02, 55.01it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:07<00:01, 54.58it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 54.66it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 54.52it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 54.59it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 54.67it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 54.29it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 54.35it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 53.71it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:08<00:01, 54.06it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:08<00:01, 54.30it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:08<00:00, 54.44it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 54.41it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 54.34it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 54.39it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 54.55it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 53.43it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 53.90it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:09<00:00, 54.30it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:09<00:00, 54.57it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:09<00:00, 46.32it/s]
epoch 3: train_loss = 0.521
3: {'Accuracy': 0.9557, 'Precision': 0.9595, 'Recall': 0.9556, 'F1-score': 0.9563}
epoch: 4
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:50,  1.24s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:01,  6.83it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:30, 13.48it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 20.41it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:14, 27.18it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:11, 33.34it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.63it/s]going through batches for holmes training:  10%|█         | 43/428 [00:01<00:08, 43.11it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 46.56it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 49.33it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 51.50it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 53.05it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 54.19it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.53it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 55.22it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.79it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 56.18it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.46it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.63it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.79it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.88it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 57.00it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 57.02it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 57.07it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 57.11it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 57.07it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:03<00:04, 57.09it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 57.06it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 57.05it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 57.07it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 57.08it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 57.07it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 57.16it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 57.14it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 57.11it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 57.10it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.09it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 57.07it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 57.04it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 57.04it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 57.04it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.99it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.90it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.86it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 56.97it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:05<00:02, 56.99it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.89it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.94it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.92it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.86it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.97it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 57.03it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.59it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.69it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 56.83it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.81it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.86it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.92it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.83it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.84it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.90it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.97it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 57.03it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 55.75it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.10it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.42it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.58it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.72it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 56.26it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.62it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.83it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 57.04it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.48it/s]
epoch 4: train_loss = 0.412
4: {'Accuracy': 0.9611, 'Precision': 0.9645, 'Recall': 0.9611, 'F1-score': 0.9618}
epoch: 5
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:59,  1.26s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:02,  6.75it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:31, 13.34it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 20.26it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:14, 26.92it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.85it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.14it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.39it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.63it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.34it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 49.94it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 51.63it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 52.96it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.07it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 54.72it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.21it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 55.44it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 55.75it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.08it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 55.63it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.04it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.39it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.60it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.75it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.89it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.91it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.93it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 57.01it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 57.01it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.92it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 57.02it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 57.03it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 57.02it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 57.04it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 57.04it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 56.39it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 56.64it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 56.79it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.72it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.87it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.92it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 57.02it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 57.06it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 57.11it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 57.18it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 57.08it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 57.15it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 57.19it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 57.25it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 57.13it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 57.12it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.86it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.68it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.21it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 55.92it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.11it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.30it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.26it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.43it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.57it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.50it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.52it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 56.59it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 56.31it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.36it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.50it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.51it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.67it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 55.85it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.36it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.60it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 56.78it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.16it/s]
epoch 5: train_loss = 0.34
5: {'Accuracy': 0.9657, 'Precision': 0.9685, 'Recall': 0.9657, 'F1-score': 0.9662}
epoch: 6
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:24,  1.32s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:04,  6.50it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.80it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.58it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.15it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.26it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 36.58it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 41.45it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.38it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.09it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.32it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 51.62it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.08it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.21it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 54.94it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.47it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 55.96it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.31it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.44it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.67it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.12it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.31it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.51it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.65it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.74it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.76it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.88it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.72it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.86it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.80it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.79it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.89it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.94it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.96it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 56.97it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 56.98it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.03it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 57.05it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.88it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.93it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.90it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.94it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 56.95it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.95it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.98it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 57.00it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 57.00it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.98it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.47it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.71it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.64it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.78it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 56.64it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.62it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.80it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.80it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.90it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.91it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.82it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.87it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 56.77it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:08<00:00, 56.92it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.89it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.75it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.84it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.89it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 56.48it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.80it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.84it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 57.04it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 47.89it/s]
epoch 6: train_loss = 0.289
6: {'Accuracy': 0.9676, 'Precision': 0.9695, 'Recall': 0.9676, 'F1-score': 0.968}
epoch: 7
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:08,  1.28s/it]going through batches for holmes training:   1%|▏         | 6/428 [00:01<01:14,  5.64it/s]going through batches for holmes training:   3%|▎         | 12/428 [00:01<00:34, 12.18it/s]going through batches for holmes training:   4%|▍         | 18/428 [00:01<00:21, 19.06it/s]going through batches for holmes training:   6%|▌         | 24/428 [00:01<00:15, 25.69it/s]going through batches for holmes training:   7%|▋         | 30/428 [00:01<00:12, 31.93it/s]going through batches for holmes training:   8%|▊         | 36/428 [00:01<00:10, 37.33it/s]going through batches for holmes training:  10%|▉         | 42/428 [00:02<00:09, 41.90it/s]going through batches for holmes training:  11%|█         | 48/428 [00:02<00:08, 45.66it/s]going through batches for holmes training:  13%|█▎        | 54/428 [00:02<00:07, 48.55it/s]going through batches for holmes training:  14%|█▍        | 60/428 [00:02<00:07, 50.82it/s]going through batches for holmes training:  15%|█▌        | 66/428 [00:02<00:06, 52.35it/s]going through batches for holmes training:  17%|█▋        | 72/428 [00:02<00:06, 53.50it/s]going through batches for holmes training:  18%|█▊        | 78/428 [00:02<00:06, 54.39it/s]going through batches for holmes training:  20%|█▉        | 84/428 [00:02<00:06, 55.03it/s]going through batches for holmes training:  21%|██        | 90/428 [00:02<00:06, 55.61it/s]going through batches for holmes training:  22%|██▏       | 96/428 [00:03<00:05, 55.83it/s]going through batches for holmes training:  24%|██▍       | 102/428 [00:03<00:05, 55.99it/s]going through batches for holmes training:  25%|██▌       | 108/428 [00:03<00:05, 56.37it/s]going through batches for holmes training:  27%|██▋       | 114/428 [00:03<00:05, 56.53it/s]going through batches for holmes training:  28%|██▊       | 120/428 [00:03<00:05, 56.64it/s]going through batches for holmes training:  29%|██▉       | 126/428 [00:03<00:05, 56.68it/s]going through batches for holmes training:  31%|███       | 132/428 [00:03<00:05, 56.63it/s]going through batches for holmes training:  32%|███▏      | 138/428 [00:03<00:05, 56.81it/s]going through batches for holmes training:  34%|███▎      | 144/428 [00:03<00:04, 56.81it/s]going through batches for holmes training:  35%|███▌      | 150/428 [00:03<00:04, 56.26it/s]going through batches for holmes training:  36%|███▋      | 156/428 [00:04<00:04, 56.32it/s]going through batches for holmes training:  38%|███▊      | 162/428 [00:04<00:04, 56.45it/s]going through batches for holmes training:  39%|███▉      | 168/428 [00:04<00:04, 56.55it/s]going through batches for holmes training:  41%|████      | 174/428 [00:04<00:04, 56.57it/s]going through batches for holmes training:  42%|████▏     | 180/428 [00:04<00:04, 56.59it/s]going through batches for holmes training:  43%|████▎     | 186/428 [00:04<00:04, 56.56it/s]going through batches for holmes training:  45%|████▍     | 192/428 [00:04<00:04, 56.62it/s]going through batches for holmes training:  46%|████▋     | 198/428 [00:04<00:04, 56.74it/s]going through batches for holmes training:  48%|████▊     | 204/428 [00:04<00:03, 56.63it/s]going through batches for holmes training:  49%|████▉     | 210/428 [00:05<00:03, 56.74it/s]going through batches for holmes training:  50%|█████     | 216/428 [00:05<00:03, 56.72it/s]going through batches for holmes training:  52%|█████▏    | 222/428 [00:05<00:03, 56.65it/s]going through batches for holmes training:  53%|█████▎    | 228/428 [00:05<00:03, 56.78it/s]going through batches for holmes training:  55%|█████▍    | 234/428 [00:05<00:03, 56.71it/s]going through batches for holmes training:  56%|█████▌    | 240/428 [00:05<00:03, 56.82it/s]going through batches for holmes training:  57%|█████▋    | 246/428 [00:05<00:03, 56.68it/s]going through batches for holmes training:  59%|█████▉    | 252/428 [00:05<00:03, 56.68it/s]going through batches for holmes training:  60%|██████    | 258/428 [00:05<00:03, 56.62it/s]going through batches for holmes training:  62%|██████▏   | 264/428 [00:05<00:02, 56.66it/s]going through batches for holmes training:  63%|██████▎   | 270/428 [00:06<00:02, 56.71it/s]going through batches for holmes training:  64%|██████▍   | 276/428 [00:06<00:02, 56.64it/s]going through batches for holmes training:  66%|██████▌   | 282/428 [00:06<00:02, 56.61it/s]going through batches for holmes training:  67%|██████▋   | 288/428 [00:06<00:02, 53.15it/s]going through batches for holmes training:  69%|██████▊   | 294/428 [00:06<00:02, 53.86it/s]going through batches for holmes training:  70%|███████   | 300/428 [00:06<00:02, 54.49it/s]going through batches for holmes training:  71%|███████▏  | 306/428 [00:06<00:02, 54.94it/s]going through batches for holmes training:  73%|███████▎  | 312/428 [00:06<00:02, 55.39it/s]going through batches for holmes training:  74%|███████▍  | 318/428 [00:06<00:01, 55.69it/s]going through batches for holmes training:  76%|███████▌  | 324/428 [00:07<00:01, 55.91it/s]going through batches for holmes training:  77%|███████▋  | 330/428 [00:07<00:01, 56.24it/s]going through batches for holmes training:  79%|███████▊  | 336/428 [00:07<00:01, 56.23it/s]going through batches for holmes training:  80%|███████▉  | 342/428 [00:07<00:01, 56.29it/s]going through batches for holmes training:  81%|████████▏ | 348/428 [00:07<00:01, 56.46it/s]going through batches for holmes training:  83%|████████▎ | 354/428 [00:07<00:01, 56.43it/s]going through batches for holmes training:  84%|████████▍ | 360/428 [00:07<00:01, 56.54it/s]going through batches for holmes training:  86%|████████▌ | 366/428 [00:07<00:01, 56.50it/s]going through batches for holmes training:  87%|████████▋ | 372/428 [00:07<00:00, 56.52it/s]going through batches for holmes training:  88%|████████▊ | 378/428 [00:08<00:00, 56.41it/s]going through batches for holmes training:  90%|████████▉ | 384/428 [00:08<00:00, 56.24it/s]going through batches for holmes training:  91%|█████████ | 390/428 [00:08<00:00, 56.36it/s]going through batches for holmes training:  93%|█████████▎| 396/428 [00:08<00:00, 56.31it/s]going through batches for holmes training:  94%|█████████▍| 402/428 [00:08<00:00, 56.30it/s]going through batches for holmes training:  95%|█████████▌| 408/428 [00:08<00:00, 56.37it/s]going through batches for holmes training:  97%|█████████▋| 414/428 [00:08<00:00, 55.89it/s]going through batches for holmes training:  98%|█████████▊| 420/428 [00:08<00:00, 56.28it/s]going through batches for holmes training: 100%|█████████▉| 426/428 [00:08<00:00, 56.49it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 47.79it/s]
epoch 7: train_loss = 0.248
7: {'Accuracy': 0.9732, 'Precision': 0.9759, 'Recall': 0.9732, 'F1-score': 0.9738}
epoch: 8
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:30,  1.34s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:05,  6.41it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.77it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.48it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.11it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.19it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 37.62it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.06it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.30it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.20it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 49.83it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 51.65it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 52.96it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 53.97it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 54.76it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.30it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 55.75it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.12it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.45it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.67it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.75it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.84it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.83it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.87it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.98it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.95it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 57.04it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 57.02it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 57.05it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 57.07it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 57.09it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 57.06it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 57.08it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 57.08it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 57.06it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 56.59it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 56.70it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 56.83it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.77it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.82it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.85it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.90it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.89it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.88it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:06<00:02, 56.96it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.97it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.94it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.89it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.86it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.92it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.92it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.95it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.88it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.77it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 56.70it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.57it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.51it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.45it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.38it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 55.80it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.06it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 55.70it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 55.89it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:08<00:00, 55.98it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.05it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.28it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.24it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.29it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 55.85it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 55.65it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.10it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 55.98it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 47.66it/s]
epoch 8: train_loss = 0.217
8: {'Accuracy': 0.9761, 'Precision': 0.9773, 'Recall': 0.9761, 'F1-score': 0.9764}
epoch: 9
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:00,  1.26s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:02,  6.68it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:31, 13.14it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.86it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.34it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.46it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 37.70it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.07it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.53it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.30it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 49.81it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:07, 51.56it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 52.96it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 53.48it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 54.34it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.03it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 55.40it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 55.80it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.13it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.24it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.37it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.53it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.60it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.49it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.77it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.72it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.86it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.84it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.85it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 55.95it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.16it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.45it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.55it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.55it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 56.77it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 55.76it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 56.00it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 56.17it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.41it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.62it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.70it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.71it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.76it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.87it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 56.95it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.96it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.97it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.99it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.87it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.93it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.42it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.47it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.45it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.41it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 56.54it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.65it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.68it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.75it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.73it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.84it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.84it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.88it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 56.89it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 56.83it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.89it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.86it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.91it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.96it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 56.33it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.73it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.98it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 57.04it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 47.99it/s]
epoch 9: train_loss = 0.192
9: {'Accuracy': 0.976, 'Precision': 0.9779, 'Recall': 0.976, 'F1-score': 0.9764}
epoch: 10
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:03,  1.13s/it]going through batches for holmes training:   1%|          | 5/428 [00:01<01:20,  5.25it/s]going through batches for holmes training:   3%|▎         | 11/428 [00:01<00:33, 12.61it/s]going through batches for holmes training:   4%|▍         | 17/428 [00:01<00:20, 20.22it/s]going through batches for holmes training:   5%|▌         | 23/428 [00:01<00:14, 27.51it/s]going through batches for holmes training:   7%|▋         | 29/428 [00:01<00:11, 34.18it/s]going through batches for holmes training:   8%|▊         | 35/428 [00:01<00:09, 39.58it/s]going through batches for holmes training:  10%|▉         | 41/428 [00:01<00:08, 44.01it/s]going through batches for holmes training:  11%|█         | 47/428 [00:01<00:08, 47.44it/s]going through batches for holmes training:  12%|█▏        | 53/428 [00:02<00:07, 50.03it/s]going through batches for holmes training:  14%|█▍        | 59/428 [00:02<00:07, 52.02it/s]going through batches for holmes training:  15%|█▌        | 65/428 [00:02<00:06, 53.40it/s]going through batches for holmes training:  17%|█▋        | 71/428 [00:02<00:06, 54.51it/s]going through batches for holmes training:  18%|█▊        | 77/428 [00:02<00:06, 55.15it/s]going through batches for holmes training:  19%|█▉        | 83/428 [00:02<00:06, 55.71it/s]going through batches for holmes training:  21%|██        | 89/428 [00:02<00:06, 56.19it/s]going through batches for holmes training:  22%|██▏       | 95/428 [00:02<00:05, 56.37it/s]going through batches for holmes training:  24%|██▎       | 101/428 [00:02<00:05, 56.56it/s]going through batches for holmes training:  25%|██▌       | 107/428 [00:03<00:05, 56.64it/s]going through batches for holmes training:  26%|██▋       | 113/428 [00:03<00:05, 56.81it/s]going through batches for holmes training:  28%|██▊       | 119/428 [00:03<00:05, 56.94it/s]going through batches for holmes training:  29%|██▉       | 125/428 [00:03<00:05, 57.02it/s]going through batches for holmes training:  31%|███       | 131/428 [00:03<00:05, 57.09it/s]going through batches for holmes training:  32%|███▏      | 137/428 [00:03<00:05, 56.81it/s]going through batches for holmes training:  33%|███▎      | 143/428 [00:03<00:05, 56.97it/s]going through batches for holmes training:  35%|███▍      | 149/428 [00:03<00:04, 57.11it/s]going through batches for holmes training:  36%|███▌      | 155/428 [00:03<00:04, 57.16it/s]going through batches for holmes training:  38%|███▊      | 161/428 [00:03<00:04, 57.25it/s]going through batches for holmes training:  39%|███▉      | 167/428 [00:04<00:04, 57.24it/s]going through batches for holmes training:  40%|████      | 173/428 [00:04<00:04, 56.94it/s]going through batches for holmes training:  42%|████▏     | 179/428 [00:04<00:04, 57.12it/s]going through batches for holmes training:  43%|████▎     | 185/428 [00:04<00:04, 57.19it/s]going through batches for holmes training:  45%|████▍     | 191/428 [00:04<00:04, 57.14it/s]going through batches for holmes training:  46%|████▌     | 197/428 [00:04<00:04, 57.16it/s]going through batches for holmes training:  47%|████▋     | 203/428 [00:04<00:03, 57.16it/s]going through batches for holmes training:  49%|████▉     | 209/428 [00:04<00:03, 57.08it/s]going through batches for holmes training:  50%|█████     | 215/428 [00:04<00:03, 54.44it/s]going through batches for holmes training:  52%|█████▏    | 221/428 [00:05<00:03, 55.22it/s]going through batches for holmes training:  53%|█████▎    | 227/428 [00:05<00:03, 55.75it/s]going through batches for holmes training:  54%|█████▍    | 233/428 [00:05<00:03, 56.20it/s]going through batches for holmes training:  56%|█████▌    | 239/428 [00:05<00:03, 56.54it/s]going through batches for holmes training:  57%|█████▋    | 245/428 [00:05<00:03, 56.77it/s]going through batches for holmes training:  59%|█████▊    | 251/428 [00:05<00:03, 56.93it/s]going through batches for holmes training:  60%|██████    | 257/428 [00:05<00:02, 57.04it/s]going through batches for holmes training:  61%|██████▏   | 263/428 [00:05<00:02, 57.12it/s]going through batches for holmes training:  63%|██████▎   | 269/428 [00:05<00:02, 57.21it/s]going through batches for holmes training:  64%|██████▍   | 275/428 [00:05<00:02, 57.23it/s]going through batches for holmes training:  66%|██████▌   | 281/428 [00:06<00:02, 57.05it/s]going through batches for holmes training:  67%|██████▋   | 287/428 [00:06<00:02, 57.11it/s]going through batches for holmes training:  68%|██████▊   | 293/428 [00:06<00:02, 57.06it/s]going through batches for holmes training:  70%|██████▉   | 299/428 [00:06<00:02, 57.20it/s]going through batches for holmes training:  71%|███████▏  | 305/428 [00:06<00:02, 57.23it/s]going through batches for holmes training:  73%|███████▎  | 311/428 [00:06<00:02, 57.25it/s]going through batches for holmes training:  74%|███████▍  | 317/428 [00:06<00:01, 57.09it/s]going through batches for holmes training:  75%|███████▌  | 323/428 [00:06<00:01, 56.96it/s]going through batches for holmes training:  77%|███████▋  | 329/428 [00:06<00:01, 56.99it/s]going through batches for holmes training:  78%|███████▊  | 335/428 [00:07<00:01, 56.93it/s]going through batches for holmes training:  80%|███████▉  | 341/428 [00:07<00:01, 56.96it/s]going through batches for holmes training:  81%|████████  | 347/428 [00:07<00:01, 56.89it/s]going through batches for holmes training:  82%|████████▏ | 353/428 [00:07<00:01, 56.83it/s]going through batches for holmes training:  84%|████████▍ | 359/428 [00:07<00:01, 56.98it/s]going through batches for holmes training:  85%|████████▌ | 365/428 [00:07<00:01, 56.85it/s]going through batches for holmes training:  87%|████████▋ | 371/428 [00:07<00:01, 56.95it/s]going through batches for holmes training:  88%|████████▊ | 377/428 [00:07<00:00, 56.86it/s]going through batches for holmes training:  89%|████████▉ | 383/428 [00:07<00:00, 56.85it/s]going through batches for holmes training:  91%|█████████ | 389/428 [00:07<00:00, 56.96it/s]going through batches for holmes training:  92%|█████████▏| 395/428 [00:08<00:00, 56.16it/s]going through batches for holmes training:  94%|█████████▎| 401/428 [00:08<00:00, 56.39it/s]going through batches for holmes training:  95%|█████████▌| 407/428 [00:08<00:00, 56.61it/s]going through batches for holmes training:  96%|█████████▋| 413/428 [00:08<00:00, 56.32it/s]going through batches for holmes training:  98%|█████████▊| 419/428 [00:08<00:00, 56.60it/s]going through batches for holmes training:  99%|█████████▉| 425/428 [00:08<00:00, 56.97it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.95it/s]
epoch 10: train_loss = 0.17
10: {'Accuracy': 0.9781, 'Precision': 0.9796, 'Recall': 0.9781, 'F1-score': 0.9784}
epoch: 11
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<11:06,  1.56s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:15,  5.60it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:36, 11.30it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:23, 17.60it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:16, 23.99it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:02<00:13, 30.21it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:02<00:10, 35.85it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 40.71it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 44.83it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 47.93it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.34it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.25it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.45it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.45it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:03<00:06, 55.11it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:03<00:06, 55.54it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 56.03it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.35it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.59it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.76it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.72it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.90it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.98it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.98it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:04<00:04, 56.89it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:04<00:04, 56.88it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.96it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.81it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.89it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.78it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.74it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.86it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.93it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:05<00:04, 56.96it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:05<00:03, 56.97it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 57.05it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.13it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 57.04it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 57.16it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 57.18it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 57.24it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 57.26it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 57.03it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:06<00:02, 57.07it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:06<00:02, 57.05it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 57.04it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 57.09it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.93it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 57.02it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.88it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.71it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.92it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:07<00:02, 56.83it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:07<00:01, 56.92it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 56.89it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.87it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 57.04it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.92it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.96it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.91it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.84it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.99it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:08<00:00, 56.90it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:08<00:00, 56.98it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 57.00it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.90it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.98it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.81it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 56.43it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.50it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.77it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:09<00:00, 56.98it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:09<00:00, 46.87it/s]
epoch 11: train_loss = 0.153
11: {'Accuracy': 0.9792, 'Precision': 0.9802, 'Recall': 0.9791, 'F1-score': 0.9794}
epoch: 12
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<10:20,  1.45s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:10,  5.96it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:34, 11.91it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:22, 18.25it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:16, 24.71it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 30.78it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:02<00:10, 36.34it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 41.02it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 44.97it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 47.98it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.37it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.11it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.41it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.49it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 55.15it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:03<00:06, 55.69it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 56.15it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.24it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.44it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.59it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.67it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.52it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.67it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.75it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:04<00:04, 56.70it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:04<00:04, 56.80it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.31it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.38it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.53it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.53it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.61it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.69it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.76it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.81it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:05<00:03, 56.70it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 56.58it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 56.69it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 56.75it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.82it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.93it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.74it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.78it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:06<00:02, 56.83it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:06<00:02, 56.88it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.91it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.89it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.88it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.99it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.94it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.99it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 57.02it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.78it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:07<00:01, 56.78it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 56.54it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.68it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.33it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.24it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.45it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.62it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.67it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.65it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:08<00:00, 56.53it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:08<00:00, 56.51it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.62it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.42it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.38it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.31it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 55.64it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.10it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.23it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 56.53it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:09<00:00, 47.15it/s]
epoch 12: train_loss = 0.137
12: {'Accuracy': 0.9817, 'Precision': 0.9825, 'Recall': 0.9817, 'F1-score': 0.9818}
epoch: 13
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:52,  1.25s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:02,  6.79it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:31, 13.33it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 20.19it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.76it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.79it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.10it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.59it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 46.32it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.59it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.85it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.49it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.53it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.46it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 55.12it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 54.49it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 55.19it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 55.73it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.11it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.42it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.64it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.65it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.63it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.76it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.85it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.88it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.95it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.94it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.97it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.98it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 57.00it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 57.02it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 57.01it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 57.03it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 57.06it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 57.12it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.06it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 57.12it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 57.13it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 57.11it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 57.05it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 57.08it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 57.11it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.98it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 56.97it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 57.00it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.98it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.98it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.94it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.92it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.80it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.78it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.76it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.87it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 56.49it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.27it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 55.57it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 55.79it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.24it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.00it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.14it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.06it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 56.12it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 56.31it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.39it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.58it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.67it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.78it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 56.27it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.49it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.64it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 56.76it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.25it/s]
epoch 13: train_loss = 0.123
13: {'Accuracy': 0.9809, 'Precision': 0.9817, 'Recall': 0.9808, 'F1-score': 0.981}
epoch: 14
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:55,  1.40s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:08,  6.16it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:34, 12.19it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:21, 18.75it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:16, 25.17it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 31.31it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:02<00:10, 36.78it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 41.52it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.25it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.24it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.59it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.16it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.31it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.04it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 54.39it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:03<00:06, 54.81it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:06, 54.91it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 55.48it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 55.84it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.18it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.18it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.25it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.49it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.37it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:05, 56.56it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:04<00:04, 56.56it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.69it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.77it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.88it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.93it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.95it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.96it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.92it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.95it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:05<00:03, 57.00it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 57.01it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.03it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 57.10it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 57.17it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 57.06it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 57.05it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 57.07it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 57.10it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.82it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:06<00:02, 56.86it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.91it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.79it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.78it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.51it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.16it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 55.34it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 55.36it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 55.77it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:07<00:01, 55.61it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 55.43it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 55.59it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 54.32it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 54.68it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 54.40it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 54.50it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 54.77it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 55.24it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:08<00:00, 55.70it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:08<00:00, 55.69it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.01it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.13it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 55.82it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.02it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 55.56it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 55.90it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.21it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 56.55it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:09<00:00, 47.24it/s]
epoch 14: train_loss = 0.112
14: {'Accuracy': 0.981, 'Precision': 0.9821, 'Recall': 0.9809, 'F1-score': 0.9811}
epoch: 15
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:29,  1.33s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:05,  6.42it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.66it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:21, 19.30it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 25.87it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.01it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 37.42it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 41.94it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.67it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.54it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.69it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.35it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.59it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.50it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 55.06it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.60it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 55.90it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.20it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.39it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.09it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.16it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.34it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.56it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.70it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.77it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:04<00:04, 56.84it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.80it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.84it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.90it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.91it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 57.01it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.97it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.96it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.99it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 57.05it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 57.06it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.02it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 57.04it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.82it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.91it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.98it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.70it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.78it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:06<00:02, 56.83it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.92it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.90it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.98it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 57.02it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 57.02it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 57.06it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.68it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.41it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.35it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 56.24it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.32it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.11it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 55.98it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.17it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.31it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.43it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.16it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 56.31it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:08<00:00, 56.44it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.44it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.49it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.54it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.63it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 56.09it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.34it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.50it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 56.60it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 47.81it/s]
epoch 15: train_loss = 0.101
15: {'Accuracy': 0.9822, 'Precision': 0.9834, 'Recall': 0.9822, 'F1-score': 0.9824}
epoch: 16
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:10,  1.15s/it]going through batches for holmes training:   1%|          | 4/428 [00:01<01:46,  3.99it/s]going through batches for holmes training:   2%|▏         | 10/428 [00:01<00:36, 11.32it/s]going through batches for holmes training:   4%|▎         | 16/428 [00:01<00:21, 18.89it/s]going through batches for holmes training:   5%|▌         | 22/428 [00:01<00:15, 26.15it/s]going through batches for holmes training:   7%|▋         | 28/428 [00:01<00:12, 32.73it/s]going through batches for holmes training:   8%|▊         | 34/428 [00:01<00:10, 38.20it/s]going through batches for holmes training:   9%|▉         | 40/428 [00:01<00:09, 42.80it/s]going through batches for holmes training:  11%|█         | 46/428 [00:02<00:08, 46.19it/s]going through batches for holmes training:  12%|█▏        | 52/428 [00:02<00:07, 48.83it/s]going through batches for holmes training:  14%|█▎        | 58/428 [00:02<00:07, 51.00it/s]going through batches for holmes training:  15%|█▍        | 64/428 [00:02<00:06, 52.40it/s]going through batches for holmes training:  16%|█▋        | 70/428 [00:02<00:06, 53.58it/s]going through batches for holmes training:  18%|█▊        | 76/428 [00:02<00:06, 54.52it/s]going through batches for holmes training:  19%|█▉        | 82/428 [00:02<00:06, 55.15it/s]going through batches for holmes training:  21%|██        | 88/428 [00:02<00:06, 55.55it/s]going through batches for holmes training:  22%|██▏       | 94/428 [00:02<00:05, 55.91it/s]going through batches for holmes training:  23%|██▎       | 100/428 [00:02<00:05, 56.30it/s]going through batches for holmes training:  25%|██▍       | 106/428 [00:03<00:05, 56.44it/s]going through batches for holmes training:  26%|██▌       | 112/428 [00:03<00:05, 56.69it/s]going through batches for holmes training:  28%|██▊       | 118/428 [00:03<00:05, 56.66it/s]going through batches for holmes training:  29%|██▉       | 124/428 [00:03<00:05, 56.72it/s]going through batches for holmes training:  30%|███       | 130/428 [00:03<00:05, 56.83it/s]going through batches for holmes training:  32%|███▏      | 136/428 [00:03<00:05, 56.92it/s]going through batches for holmes training:  33%|███▎      | 142/428 [00:03<00:05, 57.00it/s]going through batches for holmes training:  35%|███▍      | 148/428 [00:03<00:04, 56.78it/s]going through batches for holmes training:  36%|███▌      | 154/428 [00:03<00:04, 56.81it/s]going through batches for holmes training:  37%|███▋      | 160/428 [00:04<00:04, 56.87it/s]going through batches for holmes training:  39%|███▉      | 166/428 [00:04<00:04, 56.73it/s]going through batches for holmes training:  40%|████      | 172/428 [00:04<00:04, 56.88it/s]going through batches for holmes training:  42%|████▏     | 178/428 [00:04<00:04, 56.95it/s]going through batches for holmes training:  43%|████▎     | 184/428 [00:04<00:04, 56.90it/s]going through batches for holmes training:  44%|████▍     | 190/428 [00:04<00:04, 56.88it/s]going through batches for holmes training:  46%|████▌     | 196/428 [00:04<00:04, 57.00it/s]going through batches for holmes training:  47%|████▋     | 202/428 [00:04<00:03, 57.08it/s]going through batches for holmes training:  49%|████▊     | 208/428 [00:04<00:03, 57.06it/s]going through batches for holmes training:  50%|█████     | 214/428 [00:04<00:03, 56.85it/s]going through batches for holmes training:  51%|█████▏    | 220/428 [00:05<00:03, 56.93it/s]going through batches for holmes training:  53%|█████▎    | 226/428 [00:05<00:03, 56.98it/s]going through batches for holmes training:  54%|█████▍    | 232/428 [00:05<00:03, 57.03it/s]going through batches for holmes training:  56%|█████▌    | 238/428 [00:05<00:03, 57.03it/s]going through batches for holmes training:  57%|█████▋    | 244/428 [00:05<00:03, 57.00it/s]going through batches for holmes training:  58%|█████▊    | 250/428 [00:05<00:03, 57.02it/s]going through batches for holmes training:  60%|█████▉    | 256/428 [00:05<00:03, 56.80it/s]going through batches for holmes training:  61%|██████    | 262/428 [00:05<00:02, 56.84it/s]going through batches for holmes training:  63%|██████▎   | 268/428 [00:05<00:02, 56.94it/s]going through batches for holmes training:  64%|██████▍   | 274/428 [00:06<00:02, 56.98it/s]going through batches for holmes training:  65%|██████▌   | 280/428 [00:06<00:02, 57.01it/s]going through batches for holmes training:  67%|██████▋   | 286/428 [00:06<00:02, 56.97it/s]going through batches for holmes training:  68%|██████▊   | 292/428 [00:06<00:02, 56.64it/s]going through batches for holmes training:  70%|██████▉   | 298/428 [00:06<00:02, 56.72it/s]going through batches for holmes training:  71%|███████   | 304/428 [00:06<00:02, 56.49it/s]going through batches for holmes training:  72%|███████▏  | 310/428 [00:06<00:02, 56.57it/s]going through batches for holmes training:  74%|███████▍  | 316/428 [00:06<00:01, 56.42it/s]going through batches for holmes training:  75%|███████▌  | 322/428 [00:06<00:01, 56.25it/s]going through batches for holmes training:  77%|███████▋  | 328/428 [00:06<00:01, 56.40it/s]going through batches for holmes training:  78%|███████▊  | 334/428 [00:07<00:01, 56.20it/s]going through batches for holmes training:  79%|███████▉  | 340/428 [00:07<00:01, 56.29it/s]going through batches for holmes training:  81%|████████  | 346/428 [00:07<00:01, 56.43it/s]going through batches for holmes training:  82%|████████▏ | 352/428 [00:07<00:01, 56.40it/s]going through batches for holmes training:  84%|████████▎ | 358/428 [00:07<00:01, 56.62it/s]going through batches for holmes training:  85%|████████▌ | 364/428 [00:07<00:01, 56.22it/s]going through batches for holmes training:  86%|████████▋ | 370/428 [00:07<00:01, 56.49it/s]going through batches for holmes training:  88%|████████▊ | 376/428 [00:07<00:00, 56.50it/s]going through batches for holmes training:  89%|████████▉ | 382/428 [00:07<00:00, 56.63it/s]going through batches for holmes training:  91%|█████████ | 388/428 [00:08<00:00, 56.60it/s]going through batches for holmes training:  92%|█████████▏| 394/428 [00:08<00:00, 56.62it/s]going through batches for holmes training:  93%|█████████▎| 400/428 [00:08<00:00, 56.62it/s]going through batches for holmes training:  95%|█████████▍| 406/428 [00:08<00:00, 56.74it/s]going through batches for holmes training:  96%|█████████▋| 412/428 [00:08<00:00, 56.06it/s]going through batches for holmes training:  98%|█████████▊| 418/428 [00:08<00:00, 56.36it/s]going through batches for holmes training:  99%|█████████▉| 424/428 [00:08<00:00, 56.59it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.52it/s]
epoch 16: train_loss = 0.094
16: {'Accuracy': 0.9809, 'Precision': 0.9818, 'Recall': 0.9808, 'F1-score': 0.981}
epoch: 17
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:42,  1.22s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:00,  6.91it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:30, 13.60it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:19, 20.54it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:14, 27.26it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:11, 33.38it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.67it/s]going through batches for holmes training:  10%|█         | 43/428 [00:01<00:08, 42.92it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 46.47it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.98it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 51.14it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.49it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.51it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.42it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 54.85it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.53it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 55.87it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.16it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.38it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 55.64it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.11it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.17it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.45it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.50it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:05, 56.57it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.70it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:03<00:04, 56.66it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.77it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.82it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.72it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.81it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.57it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.72it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.72it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 56.40it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 56.56it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 56.56it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 56.70it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.72it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.63it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.73it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 54.99it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 55.50it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:03, 55.91it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 56.04it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.36it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.49it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.54it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.63it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.50it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.60it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.64it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.38it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.41it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 56.31it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.38it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.29it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.11it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.33it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 55.72it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 55.90it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 55.96it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 55.91it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 55.91it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.02it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 55.95it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 55.60it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 55.62it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 54.99it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 55.07it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 55.21it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 55.72it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.16it/s]
epoch 17: train_loss = 0.085
17: {'Accuracy': 0.9829, 'Precision': 0.9837, 'Recall': 0.9828, 'F1-score': 0.983}
epoch: 18
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:55,  1.26s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:02,  6.79it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:31, 13.33it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 20.20it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.74it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.96it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.25it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.64it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 46.21it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.90it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.91it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.35it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.56it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.46it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 55.00it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.62it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 55.73it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 55.98it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.29it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.45it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.62it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.61it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.78it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.82it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.94it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.87it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.87it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.80it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.86it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.89it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.94it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.98it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 57.01it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.97it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 57.01it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 57.01it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 56.97it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 56.89it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.92it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.96it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.88it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.79it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 56.86it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.91it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.86it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.79it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.83it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.66it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.65it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.22it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.21it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.18it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 56.28it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.52it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 55.25it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 55.39it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 55.67it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 55.91it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.12it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.09it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 55.62it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 55.86it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.03it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.16it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.29it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.33it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 55.75it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.21it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.42it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 56.69it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.07it/s]
epoch 18: train_loss = 0.079
18: {'Accuracy': 0.9823, 'Precision': 0.9831, 'Recall': 0.9823, 'F1-score': 0.9824}
epoch: 19
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:51,  1.25s/it]going through batches for holmes training:   1%|▏         | 6/428 [00:01<01:12,  5.83it/s]going through batches for holmes training:   3%|▎         | 12/428 [00:01<00:32, 12.62it/s]going through batches for holmes training:   4%|▍         | 18/428 [00:01<00:20, 19.67it/s]going through batches for holmes training:   6%|▌         | 24/428 [00:01<00:15, 26.55it/s]going through batches for holmes training:   7%|▋         | 30/428 [00:01<00:12, 32.63it/s]going through batches for holmes training:   8%|▊         | 36/428 [00:01<00:10, 37.88it/s]going through batches for holmes training:  10%|▉         | 42/428 [00:02<00:09, 41.94it/s]going through batches for holmes training:  11%|█         | 48/428 [00:02<00:08, 45.48it/s]going through batches for holmes training:  13%|█▎        | 54/428 [00:02<00:07, 47.78it/s]going through batches for holmes training:  14%|█▍        | 60/428 [00:02<00:07, 49.95it/s]going through batches for holmes training:  15%|█▌        | 66/428 [00:02<00:07, 51.57it/s]going through batches for holmes training:  17%|█▋        | 72/428 [00:02<00:06, 52.79it/s]going through batches for holmes training:  18%|█▊        | 78/428 [00:02<00:06, 53.69it/s]going through batches for holmes training:  20%|█▉        | 84/428 [00:02<00:06, 53.81it/s]going through batches for holmes training:  21%|██        | 90/428 [00:02<00:06, 54.47it/s]going through batches for holmes training:  22%|██▏       | 96/428 [00:02<00:06, 54.81it/s]going through batches for holmes training:  24%|██▍       | 102/428 [00:03<00:05, 54.89it/s]going through batches for holmes training:  25%|██▌       | 108/428 [00:03<00:05, 55.17it/s]going through batches for holmes training:  27%|██▋       | 114/428 [00:03<00:05, 55.35it/s]going through batches for holmes training:  28%|██▊       | 120/428 [00:03<00:05, 55.19it/s]going through batches for holmes training:  29%|██▉       | 126/428 [00:03<00:05, 55.42it/s]going through batches for holmes training:  31%|███       | 132/428 [00:03<00:05, 55.66it/s]going through batches for holmes training:  32%|███▏      | 138/428 [00:03<00:05, 55.60it/s]going through batches for holmes training:  34%|███▎      | 144/428 [00:03<00:05, 55.82it/s]going through batches for holmes training:  35%|███▌      | 150/428 [00:03<00:05, 55.50it/s]going through batches for holmes training:  36%|███▋      | 156/428 [00:04<00:04, 55.62it/s]going through batches for holmes training:  38%|███▊      | 162/428 [00:04<00:04, 55.55it/s]going through batches for holmes training:  39%|███▉      | 168/428 [00:04<00:04, 55.44it/s]going through batches for holmes training:  41%|████      | 174/428 [00:04<00:04, 55.56it/s]going through batches for holmes training:  42%|████▏     | 180/428 [00:04<00:04, 55.14it/s]going through batches for holmes training:  43%|████▎     | 186/428 [00:04<00:04, 55.25it/s]going through batches for holmes training:  45%|████▍     | 192/428 [00:04<00:04, 55.18it/s]going through batches for holmes training:  46%|████▋     | 198/428 [00:04<00:04, 55.06it/s]going through batches for holmes training:  48%|████▊     | 204/428 [00:04<00:04, 55.31it/s]going through batches for holmes training:  49%|████▉     | 210/428 [00:05<00:03, 55.06it/s]going through batches for holmes training:  50%|█████     | 216/428 [00:05<00:03, 55.04it/s]going through batches for holmes training:  52%|█████▏    | 222/428 [00:05<00:03, 55.05it/s]going through batches for holmes training:  53%|█████▎    | 228/428 [00:05<00:03, 55.03it/s]going through batches for holmes training:  55%|█████▍    | 234/428 [00:05<00:03, 55.19it/s]going through batches for holmes training:  56%|█████▌    | 240/428 [00:05<00:03, 54.97it/s]going through batches for holmes training:  57%|█████▋    | 246/428 [00:05<00:03, 55.14it/s]going through batches for holmes training:  59%|█████▉    | 252/428 [00:05<00:03, 55.18it/s]going through batches for holmes training:  60%|██████    | 258/428 [00:05<00:03, 55.10it/s]going through batches for holmes training:  62%|██████▏   | 264/428 [00:06<00:02, 55.23it/s]going through batches for holmes training:  63%|██████▎   | 270/428 [00:06<00:02, 54.98it/s]going through batches for holmes training:  64%|██████▍   | 276/428 [00:06<00:02, 55.18it/s]going through batches for holmes training:  66%|██████▌   | 282/428 [00:06<00:02, 55.03it/s]going through batches for holmes training:  67%|██████▋   | 288/428 [00:06<00:02, 55.04it/s]going through batches for holmes training:  69%|██████▊   | 294/428 [00:06<00:02, 55.16it/s]going through batches for holmes training:  70%|███████   | 300/428 [00:06<00:02, 55.01it/s]going through batches for holmes training:  71%|███████▏  | 306/428 [00:06<00:02, 55.08it/s]going through batches for holmes training:  73%|███████▎  | 312/428 [00:06<00:02, 54.97it/s]going through batches for holmes training:  74%|███████▍  | 318/428 [00:06<00:01, 55.09it/s]going through batches for holmes training:  76%|███████▌  | 324/428 [00:07<00:01, 55.43it/s]going through batches for holmes training:  77%|███████▋  | 330/428 [00:07<00:01, 55.45it/s]going through batches for holmes training:  79%|███████▊  | 336/428 [00:07<00:01, 55.75it/s]going through batches for holmes training:  80%|███████▉  | 342/428 [00:07<00:01, 55.71it/s]going through batches for holmes training:  81%|████████▏ | 348/428 [00:07<00:01, 55.51it/s]going through batches for holmes training:  83%|████████▎ | 354/428 [00:07<00:01, 55.78it/s]going through batches for holmes training:  84%|████████▍ | 360/428 [00:07<00:01, 55.48it/s]going through batches for holmes training:  86%|████████▌ | 366/428 [00:07<00:01, 55.69it/s]going through batches for holmes training:  87%|████████▋ | 372/428 [00:07<00:01, 55.67it/s]going through batches for holmes training:  88%|████████▊ | 378/428 [00:08<00:00, 55.63it/s]going through batches for holmes training:  90%|████████▉ | 384/428 [00:08<00:00, 55.76it/s]going through batches for holmes training:  91%|█████████ | 390/428 [00:08<00:00, 55.58it/s]going through batches for holmes training:  93%|█████████▎| 396/428 [00:08<00:00, 55.71it/s]going through batches for holmes training:  94%|█████████▍| 402/428 [00:08<00:00, 55.59it/s]going through batches for holmes training:  95%|█████████▌| 408/428 [00:08<00:00, 55.36it/s]going through batches for holmes training:  97%|█████████▋| 414/428 [00:08<00:00, 54.99it/s]going through batches for holmes training:  98%|█████████▊| 420/428 [00:08<00:00, 55.03it/s]going through batches for holmes training: 100%|█████████▉| 426/428 [00:08<00:00, 55.58it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:09<00:00, 47.38it/s]
epoch 19: train_loss = 0.072
19: {'Accuracy': 0.9837, 'Precision': 0.9843, 'Recall': 0.9836, 'F1-score': 0.9838}
epoch: 20
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:17,  1.31s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:04,  6.49it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.90it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.67it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.33it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.55it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 37.97it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.55it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 46.26it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 49.04it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 51.16it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.74it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.94it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.71it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 55.44it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.67it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 55.95it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.23it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.49it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.71it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.70it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.77it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.85it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.83it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.90it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.95it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 55.83it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.23it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.47it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.69it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.74it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.87it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.98it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.93it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 57.02it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 57.00it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.01it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 56.85it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.86it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.90it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.85it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.93it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.97it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.97it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 57.04it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.95it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 57.03it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 57.04it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.98it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 57.05it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 57.01it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 57.02it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.94it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.88it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 56.89it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.92it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.98it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.98it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.96it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.92it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.96it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.97it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 57.00it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 56.95it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.99it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.99it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.99it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.86it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 56.37it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.73it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.93it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 57.19it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.19it/s]
epoch 20: train_loss = 0.068
20: {'Accuracy': 0.9827, 'Precision': 0.9833, 'Recall': 0.9826, 'F1-score': 0.9827}
epoch: 21
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:51,  1.39s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:07,  6.21it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:33, 12.34it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:21, 19.00it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 25.40it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 31.43it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:02<00:10, 36.48it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 40.85it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 44.46it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 47.39it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 49.62it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:07, 51.24it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 52.44it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 53.24it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 54.16it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:03<00:06, 54.55it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:06, 54.91it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 55.12it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 55.35it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 55.71it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 55.75it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 55.87it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 55.85it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 55.81it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:05, 56.01it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:04<00:04, 55.94it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 55.81it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 55.82it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 55.86it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.01it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.02it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.11it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.08it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.02it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:05<00:03, 55.75it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 55.71it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 55.81it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 55.79it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 55.81it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.04it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 55.70it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 55.80it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 55.84it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:06<00:03, 55.88it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:06<00:02, 56.09it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.02it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.06it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 55.94it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 55.93it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.06it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.00it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 55.93it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 55.81it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:07<00:01, 55.67it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 55.85it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 55.85it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 55.78it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 55.68it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 55.66it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 55.79it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 55.76it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 55.75it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:08<00:00, 55.69it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:08<00:00, 55.75it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 55.92it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 55.82it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 55.72it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 55.72it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 53.62it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 54.36it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 54.98it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:09<00:00, 55.62it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:09<00:00, 46.92it/s]
epoch 21: train_loss = 0.062
21: {'Accuracy': 0.9848, 'Precision': 0.9855, 'Recall': 0.9847, 'F1-score': 0.9849}
epoch: 22
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:18,  1.31s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:04,  6.56it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.92it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.65it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.24it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.38it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 37.65it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.12it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.88it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.81it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 50.83it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.52it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 53.63it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 54.58it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 55.07it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:06, 55.60it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 55.99it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 56.14it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 56.42it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 56.53it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 56.43it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 56.53it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 56.62it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:05, 56.67it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 56.77it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 56.82it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 56.85it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 56.92it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 56.68it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 56.81it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 56.87it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 56.80it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 56.86it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 56.91it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 56.87it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:05<00:03, 56.85it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 56.80it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 56.82it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 56.79it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.80it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.88it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.82it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 56.84it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 56.82it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 56.92it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 56.86it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 56.87it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 56.87it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 56.86it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 56.90it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 56.82it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 56.79it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:02, 56.83it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 56.86it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:07<00:01, 56.91it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 56.88it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 56.81it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 56.65it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 56.68it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 56.61it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 56.69it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 56.74it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 56.73it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 56.79it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:08<00:00, 56.87it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 56.92it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.72it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.84it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 56.35it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.50it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.80it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 56.94it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.00it/s]
epoch 22: train_loss = 0.058
22: {'Accuracy': 0.9842, 'Precision': 0.9851, 'Recall': 0.9842, 'F1-score': 0.9844}
epoch: 23
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:37,  1.21s/it]going through batches for holmes training:   1%|          | 5/428 [00:01<01:27,  4.86it/s]going through batches for holmes training:   3%|▎         | 11/428 [00:01<00:35, 11.70it/s]going through batches for holmes training:   4%|▍         | 17/428 [00:01<00:21, 18.77it/s]going through batches for holmes training:   5%|▌         | 23/428 [00:01<00:15, 25.44it/s]going through batches for holmes training:   7%|▋         | 29/428 [00:01<00:12, 31.89it/s]going through batches for holmes training:   8%|▊         | 35/428 [00:01<00:10, 37.38it/s]going through batches for holmes training:  10%|▉         | 41/428 [00:01<00:09, 41.99it/s]going through batches for holmes training:  11%|█         | 47/428 [00:02<00:08, 45.65it/s]going through batches for holmes training:  12%|█▏        | 53/428 [00:02<00:07, 48.46it/s]going through batches for holmes training:  14%|█▍        | 59/428 [00:02<00:07, 50.76it/s]going through batches for holmes training:  15%|█▌        | 65/428 [00:02<00:07, 50.98it/s]going through batches for holmes training:  17%|█▋        | 71/428 [00:02<00:06, 52.60it/s]going through batches for holmes training:  18%|█▊        | 77/428 [00:02<00:06, 53.65it/s]going through batches for holmes training:  19%|█▉        | 83/428 [00:02<00:06, 54.64it/s]going through batches for holmes training:  21%|██        | 89/428 [00:02<00:06, 55.42it/s]going through batches for holmes training:  22%|██▏       | 95/428 [00:02<00:05, 55.57it/s]going through batches for holmes training:  24%|██▎       | 101/428 [00:03<00:05, 56.05it/s]going through batches for holmes training:  25%|██▌       | 107/428 [00:03<00:05, 56.33it/s]going through batches for holmes training:  26%|██▋       | 113/428 [00:03<00:05, 55.86it/s]going through batches for holmes training:  28%|██▊       | 119/428 [00:03<00:05, 56.23it/s]going through batches for holmes training:  29%|██▉       | 125/428 [00:03<00:05, 56.39it/s]going through batches for holmes training:  31%|███       | 131/428 [00:03<00:05, 56.66it/s]going through batches for holmes training:  32%|███▏      | 137/428 [00:03<00:05, 56.63it/s]going through batches for holmes training:  33%|███▎      | 143/428 [00:03<00:05, 56.65it/s]going through batches for holmes training:  35%|███▍      | 149/428 [00:03<00:04, 56.15it/s]going through batches for holmes training:  36%|███▌      | 155/428 [00:04<00:04, 56.44it/s]going through batches for holmes training:  38%|███▊      | 161/428 [00:04<00:04, 56.71it/s]going through batches for holmes training:  39%|███▉      | 167/428 [00:04<00:04, 56.92it/s]going through batches for holmes training:  40%|████      | 173/428 [00:04<00:04, 56.91it/s]going through batches for holmes training:  42%|████▏     | 179/428 [00:04<00:04, 57.02it/s]going through batches for holmes training:  43%|████▎     | 185/428 [00:04<00:04, 56.99it/s]going through batches for holmes training:  45%|████▍     | 191/428 [00:04<00:04, 57.05it/s]going through batches for holmes training:  46%|████▌     | 197/428 [00:04<00:04, 57.03it/s]going through batches for holmes training:  47%|████▋     | 203/428 [00:04<00:03, 57.07it/s]going through batches for holmes training:  49%|████▉     | 209/428 [00:04<00:03, 57.12it/s]going through batches for holmes training:  50%|█████     | 215/428 [00:05<00:03, 57.11it/s]going through batches for holmes training:  52%|█████▏    | 221/428 [00:05<00:03, 57.12it/s]going through batches for holmes training:  53%|█████▎    | 227/428 [00:05<00:03, 57.10it/s]going through batches for holmes training:  54%|█████▍    | 233/428 [00:05<00:03, 57.09it/s]going through batches for holmes training:  56%|█████▌    | 239/428 [00:05<00:03, 56.92it/s]going through batches for holmes training:  57%|█████▋    | 245/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  59%|█████▊    | 251/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  60%|██████    | 257/428 [00:05<00:02, 57.00it/s]going through batches for holmes training:  61%|██████▏   | 263/428 [00:05<00:02, 57.01it/s]going through batches for holmes training:  63%|██████▎   | 269/428 [00:06<00:02, 57.05it/s]going through batches for holmes training:  64%|██████▍   | 275/428 [00:06<00:02, 57.07it/s]going through batches for holmes training:  66%|██████▌   | 281/428 [00:06<00:02, 57.15it/s]going through batches for holmes training:  67%|██████▋   | 287/428 [00:06<00:02, 57.08it/s]going through batches for holmes training:  68%|██████▊   | 293/428 [00:06<00:02, 56.95it/s]going through batches for holmes training:  70%|██████▉   | 299/428 [00:06<00:02, 56.97it/s]going through batches for holmes training:  71%|███████▏  | 305/428 [00:06<00:02, 56.87it/s]going through batches for holmes training:  73%|███████▎  | 311/428 [00:06<00:02, 56.89it/s]going through batches for holmes training:  74%|███████▍  | 317/428 [00:06<00:01, 56.85it/s]going through batches for holmes training:  75%|███████▌  | 323/428 [00:06<00:01, 56.38it/s]going through batches for holmes training:  77%|███████▋  | 329/428 [00:07<00:01, 56.61it/s]going through batches for holmes training:  78%|███████▊  | 335/428 [00:07<00:01, 56.29it/s]going through batches for holmes training:  80%|███████▉  | 341/428 [00:07<00:01, 56.54it/s]going through batches for holmes training:  81%|████████  | 347/428 [00:07<00:01, 56.56it/s]going through batches for holmes training:  82%|████████▏ | 353/428 [00:07<00:01, 56.48it/s]going through batches for holmes training:  84%|████████▍ | 359/428 [00:07<00:01, 56.66it/s]going through batches for holmes training:  85%|████████▌ | 365/428 [00:07<00:01, 56.72it/s]going through batches for holmes training:  87%|████████▋ | 371/428 [00:07<00:01, 56.82it/s]going through batches for holmes training:  88%|████████▊ | 377/428 [00:07<00:00, 56.73it/s]going through batches for holmes training:  89%|████████▉ | 383/428 [00:08<00:00, 56.72it/s]going through batches for holmes training:  91%|█████████ | 389/428 [00:08<00:00, 56.87it/s]going through batches for holmes training:  92%|█████████▏| 395/428 [00:08<00:00, 56.70it/s]going through batches for holmes training:  94%|█████████▎| 401/428 [00:08<00:00, 56.79it/s]going through batches for holmes training:  95%|█████████▌| 407/428 [00:08<00:00, 56.78it/s]going through batches for holmes training:  96%|█████████▋| 413/428 [00:08<00:00, 56.23it/s]going through batches for holmes training:  98%|█████████▊| 419/428 [00:08<00:00, 56.60it/s]going through batches for holmes training:  99%|█████████▉| 425/428 [00:08<00:00, 56.89it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.18it/s]
epoch 23: train_loss = 0.054
23: {'Accuracy': 0.9842, 'Precision': 0.9847, 'Recall': 0.9842, 'F1-score': 0.9843}
epoch: 24
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:02,  1.27s/it]going through batches for holmes training:   1%|          | 3/428 [00:01<02:41,  2.63it/s]going through batches for holmes training:   2%|▏         | 9/428 [00:01<00:43,  9.53it/s]going through batches for holmes training:   4%|▎         | 15/428 [00:01<00:24, 16.85it/s]going through batches for holmes training:   5%|▍         | 21/428 [00:01<00:16, 23.99it/s]going through batches for holmes training:   6%|▋         | 27/428 [00:01<00:13, 30.63it/s]going through batches for holmes training:   8%|▊         | 33/428 [00:01<00:10, 36.02it/s]going through batches for holmes training:   9%|▉         | 39/428 [00:02<00:09, 40.25it/s]going through batches for holmes training:  11%|█         | 45/428 [00:02<00:08, 44.00it/s]going through batches for holmes training:  12%|█▏        | 51/428 [00:02<00:08, 46.57it/s]going through batches for holmes training:  13%|█▎        | 57/428 [00:02<00:07, 49.06it/s]going through batches for holmes training:  15%|█▍        | 63/428 [00:02<00:07, 51.18it/s]going through batches for holmes training:  16%|█▌        | 69/428 [00:02<00:06, 52.88it/s]going through batches for holmes training:  18%|█▊        | 75/428 [00:02<00:06, 53.90it/s]going through batches for holmes training:  19%|█▉        | 81/428 [00:02<00:06, 54.78it/s]going through batches for holmes training:  20%|██        | 87/428 [00:02<00:06, 55.44it/s]going through batches for holmes training:  22%|██▏       | 93/428 [00:03<00:05, 55.96it/s]going through batches for holmes training:  23%|██▎       | 99/428 [00:03<00:05, 56.33it/s]going through batches for holmes training:  25%|██▍       | 105/428 [00:03<00:05, 56.56it/s]going through batches for holmes training:  26%|██▌       | 111/428 [00:03<00:05, 56.68it/s]going through batches for holmes training:  27%|██▋       | 117/428 [00:03<00:05, 56.92it/s]going through batches for holmes training:  29%|██▊       | 123/428 [00:03<00:05, 57.02it/s]going through batches for holmes training:  30%|███       | 129/428 [00:03<00:05, 57.14it/s]going through batches for holmes training:  32%|███▏      | 135/428 [00:03<00:05, 57.22it/s]going through batches for holmes training:  33%|███▎      | 141/428 [00:03<00:05, 57.22it/s]going through batches for holmes training:  34%|███▍      | 147/428 [00:03<00:04, 57.27it/s]going through batches for holmes training:  36%|███▌      | 153/428 [00:04<00:04, 57.02it/s]going through batches for holmes training:  37%|███▋      | 159/428 [00:04<00:04, 57.05it/s]going through batches for holmes training:  39%|███▊      | 165/428 [00:04<00:04, 57.15it/s]going through batches for holmes training:  40%|███▉      | 171/428 [00:04<00:04, 57.13it/s]going through batches for holmes training:  41%|████▏     | 177/428 [00:04<00:04, 57.16it/s]going through batches for holmes training:  43%|████▎     | 183/428 [00:04<00:04, 57.16it/s]going through batches for holmes training:  44%|████▍     | 189/428 [00:04<00:04, 57.22it/s]going through batches for holmes training:  46%|████▌     | 195/428 [00:04<00:04, 57.27it/s]going through batches for holmes training:  47%|████▋     | 201/428 [00:04<00:03, 57.33it/s]going through batches for holmes training:  48%|████▊     | 207/428 [00:05<00:03, 57.31it/s]going through batches for holmes training:  50%|████▉     | 213/428 [00:05<00:03, 57.15it/s]going through batches for holmes training:  51%|█████     | 219/428 [00:05<00:03, 57.19it/s]going through batches for holmes training:  53%|█████▎    | 225/428 [00:05<00:03, 57.21it/s]going through batches for holmes training:  54%|█████▍    | 231/428 [00:05<00:03, 57.13it/s]going through batches for holmes training:  55%|█████▌    | 237/428 [00:05<00:03, 57.10it/s]going through batches for holmes training:  57%|█████▋    | 243/428 [00:05<00:03, 57.19it/s]going through batches for holmes training:  58%|█████▊    | 249/428 [00:05<00:03, 57.20it/s]going through batches for holmes training:  60%|█████▉    | 255/428 [00:05<00:03, 57.26it/s]going through batches for holmes training:  61%|██████    | 261/428 [00:05<00:02, 57.12it/s]going through batches for holmes training:  62%|██████▏   | 267/428 [00:06<00:02, 57.22it/s]going through batches for holmes training:  64%|██████▍   | 273/428 [00:06<00:02, 57.04it/s]going through batches for holmes training:  65%|██████▌   | 279/428 [00:06<00:02, 57.05it/s]going through batches for holmes training:  67%|██████▋   | 285/428 [00:06<00:02, 57.15it/s]going through batches for holmes training:  68%|██████▊   | 291/428 [00:06<00:02, 57.08it/s]going through batches for holmes training:  69%|██████▉   | 297/428 [00:06<00:02, 57.02it/s]going through batches for holmes training:  71%|███████   | 303/428 [00:06<00:02, 57.08it/s]going through batches for holmes training:  72%|███████▏  | 309/428 [00:06<00:02, 53.90it/s]going through batches for holmes training:  74%|███████▎  | 315/428 [00:06<00:02, 54.57it/s]going through batches for holmes training:  75%|███████▌  | 321/428 [00:07<00:01, 55.32it/s]going through batches for holmes training:  76%|███████▋  | 327/428 [00:07<00:01, 55.66it/s]going through batches for holmes training:  78%|███████▊  | 333/428 [00:07<00:01, 55.86it/s]going through batches for holmes training:  79%|███████▉  | 339/428 [00:07<00:01, 56.29it/s]going through batches for holmes training:  81%|████████  | 345/428 [00:07<00:01, 56.30it/s]going through batches for holmes training:  82%|████████▏ | 351/428 [00:07<00:01, 56.30it/s]going through batches for holmes training:  83%|████████▎ | 357/428 [00:07<00:01, 56.42it/s]going through batches for holmes training:  85%|████████▍ | 363/428 [00:07<00:01, 56.50it/s]going through batches for holmes training:  86%|████████▌ | 369/428 [00:07<00:01, 56.66it/s]going through batches for holmes training:  88%|████████▊ | 375/428 [00:07<00:00, 56.73it/s]going through batches for holmes training:  89%|████████▉ | 381/428 [00:08<00:00, 56.71it/s]going through batches for holmes training:  90%|█████████ | 387/428 [00:08<00:00, 56.77it/s]going through batches for holmes training:  92%|█████████▏| 393/428 [00:08<00:00, 56.74it/s]going through batches for holmes training:  93%|█████████▎| 399/428 [00:08<00:00, 56.89it/s]going through batches for holmes training:  95%|█████████▍| 405/428 [00:08<00:00, 56.88it/s]going through batches for holmes training:  96%|█████████▌| 411/428 [00:08<00:00, 56.27it/s]going through batches for holmes training:  97%|█████████▋| 417/428 [00:08<00:00, 56.67it/s]going through batches for holmes training:  99%|█████████▉| 423/428 [00:08<00:00, 56.97it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 47.66it/s]
epoch 24: train_loss = 0.051
24: {'Accuracy': 0.9855, 'Precision': 0.9862, 'Recall': 0.9855, 'F1-score': 0.9856}
epoch: 25
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:38,  1.35s/it]going through batches for holmes training:   1%|▏         | 6/428 [00:01<01:17,  5.43it/s]going through batches for holmes training:   3%|▎         | 12/428 [00:01<00:35, 11.85it/s]going through batches for holmes training:   4%|▍         | 18/428 [00:01<00:21, 18.68it/s]going through batches for holmes training:   6%|▌         | 24/428 [00:01<00:15, 25.45it/s]going through batches for holmes training:   7%|▋         | 30/428 [00:01<00:12, 31.67it/s]going through batches for holmes training:   8%|▊         | 36/428 [00:01<00:10, 37.09it/s]going through batches for holmes training:  10%|▉         | 42/428 [00:02<00:09, 41.53it/s]going through batches for holmes training:  11%|█         | 48/428 [00:02<00:08, 45.13it/s]going through batches for holmes training:  13%|█▎        | 54/428 [00:02<00:07, 48.02it/s]going through batches for holmes training:  14%|█▍        | 60/428 [00:02<00:07, 50.13it/s]going through batches for holmes training:  15%|█▌        | 66/428 [00:02<00:07, 51.56it/s]going through batches for holmes training:  17%|█▋        | 72/428 [00:02<00:06, 52.80it/s]going through batches for holmes training:  18%|█▊        | 78/428 [00:02<00:06, 53.24it/s]going through batches for holmes training:  20%|█▉        | 84/428 [00:02<00:06, 53.64it/s]going through batches for holmes training:  21%|██        | 90/428 [00:02<00:06, 54.16it/s]going through batches for holmes training:  22%|██▏       | 96/428 [00:03<00:06, 54.53it/s]going through batches for holmes training:  24%|██▍       | 102/428 [00:03<00:05, 54.82it/s]going through batches for holmes training:  25%|██▌       | 108/428 [00:03<00:05, 55.11it/s]going through batches for holmes training:  27%|██▋       | 114/428 [00:03<00:05, 55.32it/s]going through batches for holmes training:  28%|██▊       | 120/428 [00:03<00:05, 55.44it/s]going through batches for holmes training:  29%|██▉       | 126/428 [00:03<00:05, 55.42it/s]going through batches for holmes training:  31%|███       | 132/428 [00:03<00:05, 55.46it/s]going through batches for holmes training:  32%|███▏      | 138/428 [00:03<00:05, 55.65it/s]going through batches for holmes training:  34%|███▎      | 144/428 [00:03<00:05, 55.81it/s]going through batches for holmes training:  35%|███▌      | 150/428 [00:04<00:04, 55.93it/s]going through batches for holmes training:  36%|███▋      | 156/428 [00:04<00:04, 56.00it/s]going through batches for holmes training:  38%|███▊      | 162/428 [00:04<00:04, 55.87it/s]going through batches for holmes training:  39%|███▉      | 168/428 [00:04<00:04, 55.92it/s]going through batches for holmes training:  41%|████      | 174/428 [00:04<00:04, 56.01it/s]going through batches for holmes training:  42%|████▏     | 180/428 [00:04<00:04, 55.05it/s]going through batches for holmes training:  43%|████▎     | 186/428 [00:04<00:04, 54.78it/s]going through batches for holmes training:  45%|████▍     | 192/428 [00:04<00:04, 54.93it/s]going through batches for holmes training:  46%|████▋     | 198/428 [00:04<00:04, 55.19it/s]going through batches for holmes training:  48%|████▊     | 204/428 [00:05<00:04, 55.40it/s]going through batches for holmes training:  49%|████▉     | 210/428 [00:05<00:03, 55.56it/s]going through batches for holmes training:  50%|█████     | 216/428 [00:05<00:03, 55.56it/s]going through batches for holmes training:  52%|█████▏    | 222/428 [00:05<00:03, 55.68it/s]going through batches for holmes training:  53%|█████▎    | 228/428 [00:05<00:03, 55.83it/s]going through batches for holmes training:  55%|█████▍    | 234/428 [00:05<00:03, 55.85it/s]going through batches for holmes training:  56%|█████▌    | 240/428 [00:05<00:03, 55.93it/s]going through batches for holmes training:  57%|█████▋    | 246/428 [00:05<00:03, 56.07it/s]going through batches for holmes training:  59%|█████▉    | 252/428 [00:05<00:03, 56.03it/s]going through batches for holmes training:  60%|██████    | 258/428 [00:05<00:03, 56.16it/s]going through batches for holmes training:  62%|██████▏   | 264/428 [00:06<00:02, 56.13it/s]going through batches for holmes training:  63%|██████▎   | 270/428 [00:06<00:02, 56.14it/s]going through batches for holmes training:  64%|██████▍   | 276/428 [00:06<00:02, 56.13it/s]going through batches for holmes training:  66%|██████▌   | 282/428 [00:06<00:02, 56.09it/s]going through batches for holmes training:  67%|██████▋   | 288/428 [00:06<00:02, 55.99it/s]going through batches for holmes training:  69%|██████▊   | 294/428 [00:06<00:02, 55.66it/s]going through batches for holmes training:  70%|███████   | 300/428 [00:06<00:02, 55.44it/s]going through batches for holmes training:  71%|███████▏  | 306/428 [00:06<00:02, 55.16it/s]going through batches for holmes training:  73%|███████▎  | 312/428 [00:06<00:02, 54.98it/s]going through batches for holmes training:  74%|███████▍  | 318/428 [00:07<00:02, 54.67it/s]going through batches for holmes training:  76%|███████▌  | 324/428 [00:07<00:01, 54.78it/s]going through batches for holmes training:  77%|███████▋  | 330/428 [00:07<00:01, 54.92it/s]going through batches for holmes training:  79%|███████▊  | 336/428 [00:07<00:01, 54.33it/s]going through batches for holmes training:  80%|███████▉  | 342/428 [00:07<00:01, 54.69it/s]going through batches for holmes training:  81%|████████▏ | 348/428 [00:07<00:01, 55.00it/s]going through batches for holmes training:  83%|████████▎ | 354/428 [00:07<00:01, 55.31it/s]going through batches for holmes training:  84%|████████▍ | 360/428 [00:07<00:01, 55.14it/s]going through batches for holmes training:  86%|████████▌ | 366/428 [00:07<00:01, 55.35it/s]going through batches for holmes training:  87%|████████▋ | 372/428 [00:08<00:01, 55.53it/s]going through batches for holmes training:  88%|████████▊ | 378/428 [00:08<00:00, 54.58it/s]going through batches for holmes training:  90%|████████▉ | 384/428 [00:08<00:00, 53.92it/s]going through batches for holmes training:  91%|█████████ | 390/428 [00:08<00:00, 54.21it/s]going through batches for holmes training:  93%|█████████▎| 396/428 [00:08<00:00, 54.45it/s]going through batches for holmes training:  94%|█████████▍| 402/428 [00:08<00:00, 54.66it/s]going through batches for holmes training:  95%|█████████▌| 408/428 [00:08<00:00, 55.01it/s]going through batches for holmes training:  97%|█████████▋| 414/428 [00:08<00:00, 54.60it/s]going through batches for holmes training:  98%|█████████▊| 420/428 [00:08<00:00, 55.15it/s]going through batches for holmes training: 100%|█████████▉| 426/428 [00:09<00:00, 55.62it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:09<00:00, 46.90it/s]
epoch 25: train_loss = 0.047
25: {'Accuracy': 0.9841, 'Precision': 0.9848, 'Recall': 0.9841, 'F1-score': 0.9842}
epoch: 26
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:33,  1.34s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:06,  6.37it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.74it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.55it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.37it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.69it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.39it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:08, 43.08it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 46.92it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 49.88it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 52.10it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 53.89it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 55.01it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 55.92it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 56.66it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:05, 57.09it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 57.63it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 57.89it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 58.13it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 57.71it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 57.87it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 58.11it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 58.27it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:04, 58.36it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 58.49it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 58.50it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 58.56it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 58.54it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 58.54it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 58.59it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 58.66it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 58.64it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 58.44it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:03, 58.45it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 58.43it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 58.52it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 58.53it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 58.55it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 58.55it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 58.54it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 58.55it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 58.37it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:02, 58.39it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 58.41it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 58.42it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:05<00:02, 58.41it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 58.41it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 58.45it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 58.48it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 58.55it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 58.59it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 58.57it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:01, 58.55it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 58.52it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 58.54it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 57.83it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 57.55it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 57.20it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 57.41it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 57.49it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 57.51it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 57.76it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 57.88it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 58.02it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:07<00:00, 57.80it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 57.72it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 56.53it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 56.58it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 55.74it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 56.41it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 56.95it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 57.36it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.76it/s]
epoch 26: train_loss = 0.044
26: {'Accuracy': 0.9861, 'Precision': 0.9867, 'Recall': 0.9861, 'F1-score': 0.9862}
epoch: 27
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:36,  1.35s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:06,  6.35it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.73it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.53it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.36it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.73it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.41it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:08, 43.15it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 47.08it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 50.02it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 52.42it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 54.14it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 55.47it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 56.42it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 57.03it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:05, 57.49it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 57.85it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 58.11it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 58.21it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 58.36it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 58.43it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 58.47it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 58.49it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:04, 58.57it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 58.63it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 58.61it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 58.62it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 58.63it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 58.61it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 58.59it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 58.53it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 58.58it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 58.64it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:04, 55.04it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 55.91it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 56.47it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.05it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 57.50it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 57.82it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 58.07it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 58.13it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 58.32it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:02, 58.49it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 58.55it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 58.56it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:05<00:02, 58.55it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 58.56it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 58.64it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 58.65it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 58.72it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 58.69it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 58.66it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:01, 58.70it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 58.69it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 58.70it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 58.57it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 58.63it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 58.65it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 58.65it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 58.64it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 58.63it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 58.59it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 58.60it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 58.62it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:07<00:00, 58.61it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 58.61it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 58.61it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 58.71it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 58.04it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 58.28it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 58.43it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 58.55it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.98it/s]
epoch 27: train_loss = 0.04
27: {'Accuracy': 0.9857, 'Precision': 0.986, 'Recall': 0.9857, 'F1-score': 0.9857}
epoch: 28
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<08:55,  1.25s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:02,  6.74it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:31, 13.34it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 20.10it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.60it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.77it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 38.28it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:08, 42.95it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 46.82it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 49.70it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 52.04it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 53.59it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 54.73it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 55.71it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 56.33it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:05, 56.84it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:02<00:05, 57.12it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 57.35it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 57.68it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 57.71it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 57.88it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 57.90it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 57.86it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:04, 58.01it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 57.91it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 57.90it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:03<00:04, 57.93it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 57.83it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 57.95it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 57.87it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 57.95it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 58.00it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 57.91it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:03, 57.97it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 58.00it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 57.90it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 57.56it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 57.64it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 57.01it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 56.48it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 56.80it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 56.95it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 57.10it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 57.34it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 57.40it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:05<00:02, 57.56it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 57.50it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 57.53it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 57.62it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 57.59it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 57.75it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 57.79it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:01, 57.74it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 57.82it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 57.31it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 57.55it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 57.63it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 57.66it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 57.83it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 57.84it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 57.94it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 57.91it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 57.88it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 57.95it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:07<00:00, 57.95it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 57.98it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 57.87it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 57.84it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 57.45it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 57.64it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 57.92it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 58.03it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.97it/s]
epoch 28: train_loss = 0.039
28: {'Accuracy': 0.9847, 'Precision': 0.985, 'Recall': 0.9846, 'F1-score': 0.9846}
epoch: 29
going through batches for holmes training:   0%|          | 0/428 [00:00<?, ?it/s]/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
going through batches for holmes training:   0%|          | 1/428 [00:01<09:35,  1.35s/it]going through batches for holmes training:   2%|▏         | 7/428 [00:01<01:06,  6.37it/s]going through batches for holmes training:   3%|▎         | 13/428 [00:01<00:32, 12.71it/s]going through batches for holmes training:   4%|▍         | 19/428 [00:01<00:20, 19.49it/s]going through batches for holmes training:   6%|▌         | 25/428 [00:01<00:15, 26.22it/s]going through batches for holmes training:   7%|▋         | 31/428 [00:01<00:12, 32.44it/s]going through batches for holmes training:   9%|▊         | 37/428 [00:01<00:10, 37.86it/s]going through batches for holmes training:  10%|█         | 43/428 [00:02<00:09, 42.45it/s]going through batches for holmes training:  11%|█▏        | 49/428 [00:02<00:08, 45.77it/s]going through batches for holmes training:  13%|█▎        | 55/428 [00:02<00:07, 48.69it/s]going through batches for holmes training:  14%|█▍        | 61/428 [00:02<00:07, 51.00it/s]going through batches for holmes training:  16%|█▌        | 67/428 [00:02<00:06, 52.61it/s]going through batches for holmes training:  17%|█▋        | 73/428 [00:02<00:06, 54.19it/s]going through batches for holmes training:  18%|█▊        | 79/428 [00:02<00:06, 55.42it/s]going through batches for holmes training:  20%|█▉        | 85/428 [00:02<00:06, 56.32it/s]going through batches for holmes training:  21%|██▏       | 91/428 [00:02<00:05, 56.84it/s]going through batches for holmes training:  23%|██▎       | 97/428 [00:03<00:05, 57.25it/s]going through batches for holmes training:  24%|██▍       | 103/428 [00:03<00:05, 57.59it/s]going through batches for holmes training:  25%|██▌       | 109/428 [00:03<00:05, 57.82it/s]going through batches for holmes training:  27%|██▋       | 115/428 [00:03<00:05, 58.02it/s]going through batches for holmes training:  28%|██▊       | 121/428 [00:03<00:05, 58.13it/s]going through batches for holmes training:  30%|██▉       | 127/428 [00:03<00:05, 58.20it/s]going through batches for holmes training:  31%|███       | 133/428 [00:03<00:05, 58.30it/s]going through batches for holmes training:  32%|███▏      | 139/428 [00:03<00:04, 58.33it/s]going through batches for holmes training:  34%|███▍      | 145/428 [00:03<00:04, 58.42it/s]going through batches for holmes training:  35%|███▌      | 151/428 [00:03<00:04, 58.37it/s]going through batches for holmes training:  37%|███▋      | 157/428 [00:04<00:04, 58.43it/s]going through batches for holmes training:  38%|███▊      | 163/428 [00:04<00:04, 58.50it/s]going through batches for holmes training:  39%|███▉      | 169/428 [00:04<00:04, 58.50it/s]going through batches for holmes training:  41%|████      | 175/428 [00:04<00:04, 58.51it/s]going through batches for holmes training:  42%|████▏     | 181/428 [00:04<00:04, 58.51it/s]going through batches for holmes training:  44%|████▎     | 187/428 [00:04<00:04, 58.43it/s]going through batches for holmes training:  45%|████▌     | 193/428 [00:04<00:04, 58.37it/s]going through batches for holmes training:  46%|████▋     | 199/428 [00:04<00:03, 58.39it/s]going through batches for holmes training:  48%|████▊     | 205/428 [00:04<00:03, 58.48it/s]going through batches for holmes training:  49%|████▉     | 211/428 [00:04<00:03, 58.42it/s]going through batches for holmes training:  51%|█████     | 217/428 [00:05<00:03, 58.43it/s]going through batches for holmes training:  52%|█████▏    | 223/428 [00:05<00:03, 58.47it/s]going through batches for holmes training:  54%|█████▎    | 229/428 [00:05<00:03, 58.43it/s]going through batches for holmes training:  55%|█████▍    | 235/428 [00:05<00:03, 58.51it/s]going through batches for holmes training:  56%|█████▋    | 241/428 [00:05<00:03, 58.50it/s]going through batches for holmes training:  58%|█████▊    | 247/428 [00:05<00:03, 58.46it/s]going through batches for holmes training:  59%|█████▉    | 253/428 [00:05<00:03, 58.31it/s]going through batches for holmes training:  61%|██████    | 259/428 [00:05<00:02, 58.33it/s]going through batches for holmes training:  62%|██████▏   | 265/428 [00:05<00:02, 58.41it/s]going through batches for holmes training:  63%|██████▎   | 271/428 [00:06<00:02, 58.45it/s]going through batches for holmes training:  65%|██████▍   | 277/428 [00:06<00:02, 58.49it/s]going through batches for holmes training:  66%|██████▌   | 283/428 [00:06<00:02, 58.57it/s]going through batches for holmes training:  68%|██████▊   | 289/428 [00:06<00:02, 58.54it/s]going through batches for holmes training:  69%|██████▉   | 295/428 [00:06<00:02, 58.55it/s]going through batches for holmes training:  70%|███████   | 301/428 [00:06<00:02, 58.50it/s]going through batches for holmes training:  72%|███████▏  | 307/428 [00:06<00:02, 58.44it/s]going through batches for holmes training:  73%|███████▎  | 313/428 [00:06<00:01, 58.42it/s]going through batches for holmes training:  75%|███████▍  | 319/428 [00:06<00:01, 58.29it/s]going through batches for holmes training:  76%|███████▌  | 325/428 [00:06<00:01, 58.35it/s]going through batches for holmes training:  77%|███████▋  | 331/428 [00:07<00:01, 58.36it/s]going through batches for holmes training:  79%|███████▊  | 337/428 [00:07<00:01, 58.34it/s]going through batches for holmes training:  80%|████████  | 343/428 [00:07<00:01, 58.41it/s]going through batches for holmes training:  82%|████████▏ | 349/428 [00:07<00:01, 58.40it/s]going through batches for holmes training:  83%|████████▎ | 355/428 [00:07<00:01, 58.38it/s]going through batches for holmes training:  84%|████████▍ | 361/428 [00:07<00:01, 58.38it/s]going through batches for holmes training:  86%|████████▌ | 367/428 [00:07<00:01, 58.39it/s]going through batches for holmes training:  87%|████████▋ | 373/428 [00:07<00:00, 58.35it/s]going through batches for holmes training:  89%|████████▊ | 379/428 [00:07<00:00, 58.39it/s]going through batches for holmes training:  90%|████████▉ | 385/428 [00:07<00:00, 58.46it/s]going through batches for holmes training:  91%|█████████▏| 391/428 [00:08<00:00, 58.47it/s]going through batches for holmes training:  93%|█████████▎| 397/428 [00:08<00:00, 58.54it/s]going through batches for holmes training:  94%|█████████▍| 403/428 [00:08<00:00, 58.48it/s]going through batches for holmes training:  96%|█████████▌| 409/428 [00:08<00:00, 57.98it/s]going through batches for holmes training:  97%|█████████▋| 415/428 [00:08<00:00, 58.26it/s]going through batches for holmes training:  98%|█████████▊| 421/428 [00:08<00:00, 58.41it/s]going through batches for holmes training: 100%|█████████▉| 427/428 [00:08<00:00, 58.52it/s]going through batches for holmes training: 100%|██████████| 428/428 [00:08<00:00, 48.89it/s]
epoch 29: train_loss = 0.039
29: {'Accuracy': 0.9844, 'Precision': 0.9851, 'Recall': 0.9844, 'F1-score': 0.9845}
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Valid: X=torch.Size([9516, 1, 2, 1000]), y=torch.Size([9516])
num_classes: 95
  0%|          | 0/95 [00:00<?, ?it/s]  1%|          | 1/95 [00:49<1:18:03, 49.82s/it]  2%|▏         | 2/95 [01:37<1:15:27, 48.68s/it]  3%|▎         | 3/95 [02:25<1:14:05, 48.32s/it]  4%|▍         | 4/95 [03:13<1:13:07, 48.21s/it]  5%|▌         | 5/95 [04:01<1:12:00, 48.00s/it]  6%|▋         | 6/95 [04:48<1:10:54, 47.80s/it]  7%|▋         | 7/95 [05:36<1:10:12, 47.87s/it]  8%|▊         | 8/95 [06:24<1:09:22, 47.84s/it]  9%|▉         | 9/95 [07:12<1:08:28, 47.77s/it] 11%|█         | 10/95 [07:59<1:07:43, 47.80s/it] 12%|█▏        | 11/95 [08:47<1:07:00, 47.86s/it] 13%|█▎        | 12/95 [09:35<1:06:12, 47.86s/it] 14%|█▎        | 13/95 [10:23<1:05:22, 47.83s/it] 15%|█▍        | 14/95 [11:11<1:04:34, 47.83s/it] 16%|█▌        | 15/95 [11:59<1:03:47, 47.85s/it] 17%|█▋        | 16/95 [12:47<1:02:57, 47.82s/it] 18%|█▊        | 17/95 [13:34<1:02:03, 47.73s/it] 19%|█▉        | 18/95 [14:22<1:01:18, 47.77s/it] 20%|██        | 19/95 [15:10<1:00:34, 47.82s/it] 21%|██        | 20/95 [15:58<59:45, 47.80s/it]   22%|██▏       | 21/95 [16:46<59:00, 47.84s/it] 23%|██▎       | 22/95 [17:33<58:11, 47.83s/it] 24%|██▍       | 23/95 [18:21<57:21, 47.79s/it] 25%|██▌       | 24/95 [19:09<56:30, 47.76s/it] 26%|██▋       | 25/95 [19:57<55:49, 47.86s/it] 27%|██▋       | 26/95 [20:45<55:04, 47.89s/it] 28%|██▊       | 27/95 [21:33<54:19, 47.94s/it] 29%|██▉       | 28/95 [22:20<53:24, 47.82s/it] 31%|███       | 29/95 [23:08<52:34, 47.79s/it] 32%|███▏      | 30/95 [23:56<51:45, 47.78s/it] 33%|███▎      | 31/95 [24:44<51:03, 47.86s/it] 34%|███▎      | 32/95 [25:32<50:13, 47.83s/it] 35%|███▍      | 33/95 [26:19<49:21, 47.77s/it] 36%|███▌      | 34/95 [27:07<48:37, 47.82s/it] 37%|███▋      | 35/95 [27:55<47:52, 47.87s/it] 38%|███▊      | 36/95 [28:43<47:00, 47.81s/it] 39%|███▉      | 37/95 [29:31<46:10, 47.77s/it] 40%|████      | 38/95 [30:19<45:26, 47.83s/it] 41%|████      | 39/95 [31:06<44:34, 47.76s/it] 42%|████▏     | 40/95 [31:54<43:47, 47.77s/it] 43%|████▎     | 41/95 [32:42<42:57, 47.73s/it] 44%|████▍     | 42/95 [33:29<42:10, 47.75s/it] 45%|████▌     | 43/95 [34:17<41:24, 47.77s/it] 46%|████▋     | 44/95 [35:05<40:37, 47.79s/it] 47%|████▋     | 45/95 [35:53<39:50, 47.82s/it] 48%|████▊     | 46/95 [36:41<39:03, 47.82s/it] 49%|████▉     | 47/95 [37:28<38:13, 47.77s/it] 51%|█████     | 48/95 [38:16<37:26, 47.80s/it] 52%|█████▏    | 49/95 [39:04<36:39, 47.80s/it] 53%|█████▎    | 50/95 [39:52<35:53, 47.85s/it] 54%|█████▎    | 51/95 [40:40<35:02, 47.78s/it] 55%|█████▍    | 52/95 [41:28<34:15, 47.81s/it] 56%|█████▌    | 53/95 [42:16<33:30, 47.86s/it] 57%|█████▋    | 54/95 [43:03<32:43, 47.89s/it] 58%|█████▊    | 55/95 [43:51<31:56, 47.91s/it] 59%|█████▉    | 56/95 [44:39<31:07, 47.90s/it] 60%|██████    | 57/95 [45:27<30:19, 47.88s/it] 61%|██████    | 58/95 [46:15<29:31, 47.87s/it] 62%|██████▏   | 59/95 [47:03<28:42, 47.85s/it] 63%|██████▎   | 60/95 [47:51<27:54, 47.85s/it] 64%|██████▍   | 61/95 [48:39<27:07, 47.87s/it] 65%|██████▌   | 62/95 [49:26<26:17, 47.82s/it] 66%|██████▋   | 63/95 [50:14<25:30, 47.81s/it] 67%|██████▋   | 64/95 [51:02<24:43, 47.86s/it] 68%|██████▊   | 65/95 [51:50<23:55, 47.86s/it] 69%|██████▉   | 66/95 [52:38<23:08, 47.88s/it] 71%|███████   | 67/95 [53:26<22:20, 47.87s/it] 72%|███████▏  | 68/95 [54:13<21:31, 47.83s/it] 73%|███████▎  | 69/95 [55:01<20:44, 47.87s/it] 74%|███████▎  | 70/95 [55:49<19:54, 47.78s/it] 75%|███████▍  | 71/95 [56:37<19:05, 47.75s/it] 76%|███████▌  | 72/95 [57:24<18:17, 47.73s/it] 77%|███████▋  | 73/95 [58:12<17:31, 47.78s/it] 78%|███████▊  | 74/95 [59:00<16:43, 47.80s/it] 79%|███████▉  | 75/95 [59:48<15:56, 47.81s/it] 80%|████████  | 76/95 [1:00:36<15:08, 47.80s/it] 81%|████████  | 77/95 [1:01:23<14:19, 47.76s/it] 82%|████████▏ | 78/95 [1:02:11<13:31, 47.74s/it] 83%|████████▎ | 79/95 [1:02:59<12:44, 47.78s/it] 84%|████████▍ | 80/95 [1:03:47<11:58, 47.89s/it] 85%|████████▌ | 81/95 [1:04:35<11:10, 47.87s/it] 86%|████████▋ | 82/95 [1:05:24<10:27, 48.24s/it] 87%|████████▋ | 83/95 [1:06:12<09:36, 48.08s/it] 88%|████████▊ | 84/95 [1:07:00<08:48, 48.06s/it] 89%|████████▉ | 85/95 [1:07:47<07:59, 47.93s/it] 91%|█████████ | 86/95 [1:08:35<07:11, 47.93s/it] 92%|█████████▏| 87/95 [1:09:23<06:23, 47.90s/it] 93%|█████████▎| 88/95 [1:10:11<05:34, 47.81s/it] 94%|█████████▎| 89/95 [1:10:58<04:46, 47.70s/it] 95%|█████████▍| 90/95 [1:11:46<03:58, 47.72s/it] 96%|█████████▌| 91/95 [1:12:34<03:10, 47.71s/it] 97%|█████████▋| 92/95 [1:13:21<02:23, 47.73s/it] 98%|█████████▊| 93/95 [1:14:09<01:35, 47.76s/it] 99%|█████████▉| 94/95 [1:14:57<00:47, 47.80s/it]100%|██████████| 95/95 [1:15:45<00:00, 47.73s/it]100%|██████████| 95/95 [1:15:45<00:00, 47.84s/it]
shape of attr_values: (95, 1000)
  0%|          | 0/85641 [00:00<?, ?it/s]  1%|          | 805/85641 [00:00<00:10, 8042.74it/s]  2%|▏         | 1614/85641 [00:00<00:10, 8068.72it/s]  3%|▎         | 2421/85641 [00:00<00:10, 8066.51it/s]  4%|▍         | 3228/85641 [00:00<00:10, 8063.42it/s]  5%|▍         | 4041/85641 [00:00<00:10, 8084.25it/s]  6%|▌         | 4852/85641 [00:00<00:09, 8092.12it/s]  7%|▋         | 5670/85641 [00:00<00:09, 8118.94it/s]  8%|▊         | 6486/85641 [00:00<00:09, 8129.13it/s]  9%|▊         | 7303/85641 [00:00<00:09, 8138.59it/s]  9%|▉         | 8117/85641 [00:01<00:09, 8135.08it/s] 10%|█         | 8936/85641 [00:01<00:09, 8150.33it/s] 11%|█▏        | 9752/85641 [00:01<00:09, 8145.24it/s] 12%|█▏        | 10568/85641 [00:01<00:09, 8149.64it/s] 13%|█▎        | 11385/85641 [00:01<00:09, 8154.10it/s] 14%|█▍        | 12201/85641 [00:01<00:09, 8137.53it/s] 15%|█▌        | 13015/85641 [00:01<00:08, 8131.28it/s] 16%|█▌        | 13829/85641 [00:01<00:08, 8102.15it/s] 17%|█▋        | 14640/85641 [00:01<00:08, 8098.14it/s] 18%|█▊        | 15454/85641 [00:01<00:08, 8108.78it/s] 19%|█▉        | 16269/85641 [00:02<00:08, 8120.89it/s] 20%|█▉        | 17082/85641 [00:02<00:08, 8105.33it/s] 21%|██        | 17895/85641 [00:02<00:08, 8109.95it/s] 22%|██▏       | 18707/85641 [00:02<00:08, 8110.09it/s] 23%|██▎       | 19519/85641 [00:02<00:08, 8103.04it/s] 24%|██▎       | 20330/85641 [00:02<00:08, 8099.58it/s] 25%|██▍       | 21141/85641 [00:02<00:07, 8100.37it/s] 26%|██▌       | 21955/85641 [00:02<00:07, 8111.86it/s] 27%|██▋       | 22768/85641 [00:02<00:07, 8114.28it/s] 28%|██▊       | 23580/85641 [00:02<00:07, 8110.24it/s] 28%|██▊       | 24394/85641 [00:03<00:07, 8116.40it/s] 29%|██▉       | 25210/85641 [00:03<00:07, 8127.99it/s] 30%|███       | 26029/85641 [00:03<00:07, 8146.20it/s] 31%|███▏      | 26851/85641 [00:03<00:07, 8165.81it/s] 32%|███▏      | 27668/85641 [00:03<00:07, 8162.27it/s] 33%|███▎      | 28485/85641 [00:03<00:07, 8161.24it/s] 34%|███▍      | 29302/85641 [00:03<00:06, 8162.33it/s] 35%|███▌      | 30119/85641 [00:03<00:06, 8163.27it/s] 36%|███▌      | 30936/85641 [00:03<00:06, 8161.16it/s] 37%|███▋      | 31755/85641 [00:03<00:06, 8168.26it/s] 38%|███▊      | 32572/85641 [00:04<00:06, 8168.08it/s] 39%|███▉      | 33395/85641 [00:04<00:06, 8186.40it/s] 40%|███▉      | 34214/85641 [00:04<00:06, 8178.95it/s] 41%|████      | 35037/85641 [00:04<00:06, 8191.53it/s] 42%|████▏     | 35857/85641 [00:04<00:06, 8177.15it/s] 43%|████▎     | 36681/85641 [00:04<00:05, 8193.63it/s] 44%|████▍     | 37504/85641 [00:04<00:05, 8202.03it/s] 45%|████▍     | 38325/85641 [00:04<00:05, 8202.89it/s] 46%|████▌     | 39146/85641 [00:04<00:05, 8188.61it/s] 47%|████▋     | 39965/85641 [00:04<00:05, 8188.95it/s] 48%|████▊     | 40787/85641 [00:05<00:05, 8196.58it/s] 49%|████▊     | 41607/85641 [00:05<00:05, 8155.40it/s] 50%|████▉     | 42423/85641 [00:05<00:05, 8152.88it/s] 50%|█████     | 43239/85641 [00:05<00:05, 8128.74it/s] 51%|█████▏    | 44052/85641 [00:05<00:05, 7059.52it/s] 52%|█████▏    | 44850/85641 [00:05<00:05, 7306.26it/s] 53%|█████▎    | 45668/85641 [00:05<00:05, 7547.92it/s] 54%|█████▍    | 46486/85641 [00:05<00:05, 7725.65it/s] 55%|█████▌    | 47312/85641 [00:05<00:04, 7877.51it/s] 56%|█████▌    | 48109/85641 [00:05<00:04, 7891.24it/s] 57%|█████▋    | 48928/85641 [00:06<00:04, 7978.73it/s] 58%|█████▊    | 49748/85641 [00:06<00:04, 8043.12it/s] 59%|█████▉    | 50563/85641 [00:06<00:04, 8072.62it/s] 60%|█████▉    | 51373/85641 [00:06<00:04, 8033.31it/s] 61%|██████    | 52195/85641 [00:06<00:04, 8085.73it/s] 62%|██████▏   | 53012/85641 [00:06<00:04, 8108.65it/s] 63%|██████▎   | 53826/85641 [00:06<00:03, 8116.33it/s] 64%|██████▍   | 54639/85641 [00:06<00:03, 8055.17it/s] 65%|██████▍   | 55458/85641 [00:06<00:03, 8094.07it/s] 66%|██████▌   | 56277/85641 [00:06<00:03, 8120.84it/s] 67%|██████▋   | 57092/85641 [00:07<00:03, 8127.12it/s] 68%|██████▊   | 57905/85641 [00:07<00:03, 8077.89it/s] 69%|██████▊   | 58725/85641 [00:07<00:03, 8113.02it/s] 70%|██████▉   | 59544/85641 [00:07<00:03, 8135.63it/s] 70%|███████   | 60365/85641 [00:07<00:03, 8156.53it/s] 71%|███████▏  | 61181/85641 [00:07<00:02, 8154.26it/s] 72%|███████▏  | 61997/85641 [00:07<00:02, 8097.45it/s] 73%|███████▎  | 62812/85641 [00:07<00:02, 8112.88it/s] 74%|███████▍  | 63631/85641 [00:07<00:02, 8134.72it/s] 75%|███████▌  | 64450/85641 [00:07<00:02, 8149.19it/s] 76%|███████▌  | 65265/85641 [00:08<00:02, 8087.62it/s] 77%|███████▋  | 66083/85641 [00:08<00:02, 8113.08it/s] 78%|███████▊  | 66906/85641 [00:08<00:02, 8147.67it/s] 79%|███████▉  | 67721/85641 [00:08<00:02, 8144.11it/s] 80%|████████  | 68536/85641 [00:08<00:02, 8082.60it/s] 81%|████████  | 69353/85641 [00:08<00:02, 8107.49it/s] 82%|████████▏ | 70171/85641 [00:08<00:01, 8127.94it/s] 83%|████████▎ | 70987/85641 [00:08<00:01, 8135.37it/s] 84%|████████▍ | 71801/85641 [00:08<00:01, 8053.37it/s] 85%|████████▍ | 72620/85641 [00:08<00:01, 8091.26it/s] 86%|████████▌ | 73439/85641 [00:09<00:01, 8117.04it/s] 87%|████████▋ | 74262/85641 [00:09<00:01, 8149.84it/s] 88%|████████▊ | 75078/85641 [00:09<00:01, 8084.42it/s] 89%|████████▊ | 75898/85641 [00:09<00:01, 8117.88it/s] 90%|████████▉ | 76720/85641 [00:09<00:01, 8148.25it/s] 91%|█████████ | 77539/85641 [00:09<00:00, 8159.46it/s] 91%|█████████▏| 78356/85641 [00:09<00:00, 8108.77it/s] 92%|█████████▏| 79174/85641 [00:09<00:00, 8127.92it/s] 93%|█████████▎| 79992/85641 [00:09<00:00, 8141.33it/s] 94%|█████████▍| 80807/85641 [00:09<00:00, 8136.58it/s] 95%|█████████▌| 81621/85641 [00:10<00:00, 8094.99it/s] 96%|█████████▋| 82449/85641 [00:10<00:00, 8148.24it/s] 97%|█████████▋| 83273/85641 [00:10<00:00, 8172.70it/s] 98%|█████████▊| 84095/85641 [00:10<00:00, 8184.34it/s] 99%|█████████▉| 84914/85641 [00:10<00:00, 8134.57it/s]100%|██████████| 85641/85641 [00:10<00:00, 8093.29it/s]
/localscratch/spool/slurmd/job36684738/slurm_script: line 68: 3940039 Killed                  python3 -m exp.dataset_process.data_augmentation --dataset ${dataset} --model RF --in_file ${filename} --attr_method ${attr_method} -cc True
  0%|          | 0/9516 [00:00<?, ?it/s]  8%|▊         | 795/9516 [00:00<00:01, 7946.22it/s] 17%|█▋        | 1590/9516 [00:00<00:01, 7834.83it/s] 25%|██▌       | 2389/9516 [00:00<00:00, 7903.88it/s] 33%|███▎      | 3184/9516 [00:00<00:00, 7918.49it/s] 42%|████▏     | 3980/9516 [00:00<00:00, 7931.38it/s] 50%|█████     | 4794/9516 [00:00<00:00, 8001.09it/s] 59%|█████▉    | 5609/9516 [00:00<00:00, 8047.00it/s] 67%|██████▋   | 6422/9516 [00:00<00:00, 8072.81it/s] 76%|███████▌  | 7244/9516 [00:00<00:00, 8115.84it/s] 85%|████████▍ | 8069/9516 [00:01<00:00, 8156.08it/s] 93%|█████████▎| 8889/9516 [00:01<00:00, 8168.09it/s]100%|██████████| 9516/9516 [00:01<00:00, 8067.97it/s]
Generate /home/kka151/scratch/holmes/datasets/CW/aug_valid.npz done.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/aug_train.npz'
  0%|          | 0/28548 [00:00<?, ?it/s]  0%|          | 133/28548 [00:00<00:23, 1202.42it/s]  1%|          | 254/28548 [00:00<01:32, 306.93it/s]   1%|          | 315/28548 [00:01<01:43, 274.02it/s]  1%|▏         | 359/28548 [00:01<01:44, 270.32it/s]  1%|▏         | 396/28548 [00:01<01:45, 266.34it/s]  2%|▏         | 429/28548 [00:01<01:48, 258.47it/s]  2%|▏         | 462/28548 [00:01<01:45, 267.23it/s]  2%|▏         | 492/28548 [00:01<01:50, 254.80it/s]  2%|▏         | 520/28548 [00:01<01:56, 240.09it/s]  2%|▏         | 546/28548 [00:01<01:56, 241.25it/s]  2%|▏         | 575/28548 [00:02<01:50, 252.01it/s]  2%|▏         | 608/28548 [00:02<01:51, 250.31it/s]  2%|▏         | 634/28548 [00:02<01:50, 252.11it/s]  2%|▏         | 669/28548 [00:02<01:42, 270.76it/s]  2%|▏         | 700/28548 [00:02<01:43, 269.91it/s]  3%|▎         | 750/28548 [00:02<01:23, 331.48it/s]  3%|▎         | 785/28548 [00:02<01:23, 332.72it/s]  3%|▎         | 819/28548 [00:02<01:37, 283.99it/s]  3%|▎         | 861/28548 [00:02<01:27, 317.30it/s]  3%|▎         | 897/28548 [00:03<01:28, 310.71it/s]  3%|▎         | 942/28548 [00:03<01:20, 343.82it/s]  3%|▎         | 978/28548 [00:03<01:26, 319.70it/s]  4%|▎         | 1012/28548 [00:03<01:25, 323.50it/s]  4%|▎         | 1046/28548 [00:03<01:31, 301.39it/s]  4%|▍         | 1087/28548 [00:03<01:25, 322.90it/s]  4%|▍         | 1124/28548 [00:03<01:23, 327.66it/s]  4%|▍         | 1158/28548 [00:03<01:26, 318.44it/s]  4%|▍         | 1195/28548 [00:04<01:22, 329.90it/s]  4%|▍         | 1229/28548 [00:04<01:34, 289.93it/s]  5%|▍         | 1288/28548 [00:04<01:17, 351.28it/s]  5%|▍         | 1325/28548 [00:04<01:20, 337.22it/s]  5%|▍         | 1375/28548 [00:04<01:13, 371.05it/s]  5%|▍         | 1420/28548 [00:04<01:10, 386.81it/s]  5%|▌         | 1467/28548 [00:04<01:06, 405.88it/s]  5%|▌         | 1509/28548 [00:04<01:08, 393.34it/s]  5%|▌         | 1549/28548 [00:04<01:15, 357.53it/s]  6%|▌         | 1595/28548 [00:05<01:10, 383.62it/s]  6%|▌         | 1635/28548 [00:05<01:11, 377.24it/s]  6%|▌         | 1674/28548 [00:05<01:17, 345.20it/s]  6%|▌         | 1711/28548 [00:05<01:16, 351.68it/s]  6%|▌         | 1752/28548 [00:05<01:13, 364.96it/s]  6%|▋         | 1790/28548 [00:05<01:17, 343.53it/s]  6%|▋         | 1825/28548 [00:05<01:25, 312.85it/s]  7%|▋         | 1858/28548 [00:05<01:30, 294.09it/s]  7%|▋         | 1905/28548 [00:06<01:21, 326.71it/s]  7%|▋         | 1944/28548 [00:06<01:17, 342.94it/s]  7%|▋         | 1979/28548 [00:06<01:36, 275.57it/s]  7%|▋         | 2038/28548 [00:06<01:16, 346.11it/s]  7%|▋         | 2076/28548 [00:06<01:16, 347.64it/s]  7%|▋         | 2121/28548 [00:06<01:11, 367.37it/s]  8%|▊         | 2160/28548 [00:06<01:16, 344.42it/s]  8%|▊         | 2196/28548 [00:06<01:36, 273.03it/s]  8%|▊         | 2242/28548 [00:07<01:24, 312.34it/s]  8%|▊         | 2284/28548 [00:07<01:18, 336.52it/s]  8%|▊         | 2321/28548 [00:07<01:18, 335.70it/s]  8%|▊         | 2357/28548 [00:07<01:17, 339.90it/s]  8%|▊         | 2393/28548 [00:07<01:25, 305.70it/s]  9%|▊         | 2428/28548 [00:07<01:23, 314.62it/s]  9%|▊         | 2461/28548 [00:07<01:24, 307.96it/s]  9%|▉         | 2498/28548 [00:07<01:20, 322.30it/s]  9%|▉         | 2542/28548 [00:07<01:13, 351.49it/s]  9%|▉         | 2578/28548 [00:08<01:25, 304.45it/s]  9%|▉         | 2620/28548 [00:08<01:17, 334.02it/s]  9%|▉         | 2672/28548 [00:08<01:10, 366.96it/s] 10%|▉         | 2713/28548 [00:08<01:08, 377.60it/s] 10%|▉         | 2752/28548 [00:08<01:20, 319.83it/s] 10%|▉         | 2786/28548 [00:08<01:35, 269.19it/s] 10%|▉         | 2851/28548 [00:08<01:13, 350.66it/s] 10%|█         | 2890/28548 [00:09<01:14, 346.09it/s] 10%|█         | 2928/28548 [00:09<01:19, 322.98it/s] 10%|█         | 2977/28548 [00:09<01:10, 363.91it/s] 11%|█         | 3016/28548 [00:09<01:10, 360.45it/s] 11%|█         | 3054/28548 [00:09<01:10, 363.16it/s] 11%|█         | 3092/28548 [00:09<01:11, 357.45it/s] 11%|█         | 3129/28548 [00:09<01:20, 316.58it/s] 11%|█         | 3162/28548 [00:09<01:26, 291.92it/s] 11%|█▏        | 3217/28548 [00:09<01:13, 345.57it/s] 11%|█▏        | 3253/28548 [00:10<01:23, 302.24it/s] 12%|█▏        | 3288/28548 [00:10<01:21, 311.72it/s] 12%|█▏        | 3322/28548 [00:10<01:19, 318.75it/s] 12%|█▏        | 3355/28548 [00:10<01:22, 306.33it/s] 12%|█▏        | 3391/28548 [00:10<01:19, 314.99it/s] 12%|█▏        | 3438/28548 [00:10<01:10, 356.96it/s] 12%|█▏        | 3477/28548 [00:10<01:09, 362.62it/s] 12%|█▏        | 3525/28548 [00:10<01:04, 390.28it/s] 13%|█▎        | 3582/28548 [00:11<00:59, 419.38it/s] 13%|█▎        | 3625/28548 [00:11<01:03, 391.83it/s] 13%|█▎        | 3665/28548 [00:11<01:11, 347.38it/s] 13%|█▎        | 3711/28548 [00:11<01:06, 370.80it/s] 13%|█▎        | 3750/28548 [00:11<01:13, 335.64it/s] 13%|█▎        | 3803/28548 [00:11<01:04, 381.96it/s] 13%|█▎        | 3843/28548 [00:11<01:08, 363.07it/s] 14%|█▎        | 3881/28548 [00:11<01:10, 352.36it/s] 14%|█▎        | 3925/28548 [00:11<01:06, 371.35it/s] 14%|█▍        | 3963/28548 [00:12<01:06, 370.99it/s] 14%|█▍        | 4011/28548 [00:12<01:01, 398.36it/s] 14%|█▍        | 4052/28548 [00:12<01:11, 342.75it/s] 14%|█▍        | 4091/28548 [00:12<01:08, 354.81it/s] 14%|█▍        | 4128/28548 [00:12<01:20, 302.15it/s] 15%|█▍        | 4177/28548 [00:12<01:12, 334.50it/s] 15%|█▍        | 4213/28548 [00:12<01:21, 297.29it/s] 15%|█▍        | 4245/28548 [00:13<01:25, 285.49it/s] 15%|█▌        | 4298/28548 [00:13<01:10, 342.51it/s] 15%|█▌        | 4335/28548 [00:13<01:13, 330.70it/s] 15%|█▌        | 4385/28548 [00:13<01:10, 342.70it/s] 15%|█▌        | 4424/28548 [00:13<01:08, 353.58it/s] 16%|█▌        | 4466/28548 [00:13<01:05, 366.38it/s] 16%|█▌        | 4504/28548 [00:13<01:05, 368.07it/s] 16%|█▌        | 4542/28548 [00:13<01:08, 349.73it/s] 16%|█▌        | 4589/28548 [00:13<01:02, 381.27it/s] 16%|█▌        | 4628/28548 [00:14<01:08, 350.07it/s] 16%|█▋        | 4673/28548 [00:14<01:05, 365.63it/s] 17%|█▋        | 4711/28548 [00:14<01:06, 357.41it/s] 17%|█▋        | 4757/28548 [00:14<01:02, 381.55it/s] 17%|█▋        | 4796/28548 [00:14<01:11, 334.05it/s] 17%|█▋        | 4831/28548 [00:14<01:13, 320.81it/s] 17%|█▋        | 4890/28548 [00:14<01:01, 385.83it/s] 17%|█▋        | 4930/28548 [00:14<01:05, 361.43it/s] 17%|█▋        | 4968/28548 [00:15<01:09, 337.67it/s] 18%|█▊        | 5005/28548 [00:15<01:12, 323.55it/s] 18%|█▊        | 5039/28548 [00:15<01:20, 293.64it/s] 18%|█▊        | 5070/28548 [00:15<01:19, 295.29it/s] 18%|█▊        | 5106/28548 [00:15<01:16, 305.63it/s] 18%|█▊        | 5158/28548 [00:15<01:04, 362.57it/s] 18%|█▊        | 5196/28548 [00:15<01:06, 353.36it/s] 18%|█▊        | 5233/28548 [00:15<01:07, 346.33it/s] 18%|█▊        | 5269/28548 [00:15<01:13, 315.90it/s] 19%|█▊        | 5311/28548 [00:16<01:07, 342.71it/s] 19%|█▊        | 5351/28548 [00:16<01:05, 352.94it/s] 19%|█▉        | 5388/28548 [00:16<01:10, 330.82it/s] 19%|█▉        | 5432/28548 [00:16<01:05, 353.19it/s] 19%|█▉        | 5468/28548 [00:16<01:14, 310.06it/s] 19%|█▉        | 5508/28548 [00:16<01:09, 329.66it/s] 19%|█▉        | 5557/28548 [00:16<01:06, 345.47it/s] 20%|█▉        | 5593/28548 [00:16<01:08, 334.04it/s] 20%|█▉        | 5628/28548 [00:17<01:09, 330.59it/s] 20%|█▉        | 5681/28548 [00:17<01:01, 372.23it/s] 20%|██        | 5719/28548 [00:17<01:02, 367.91it/s] 20%|██        | 5763/28548 [00:17<00:59, 383.78it/s] 20%|██        | 5802/28548 [00:17<01:02, 361.99it/s] 20%|██        | 5839/28548 [00:17<01:02, 363.59it/s] 21%|██        | 5889/28548 [00:17<00:58, 385.28it/s] 21%|██        | 5928/28548 [00:17<01:02, 361.16it/s] 21%|██        | 5966/28548 [00:17<01:03, 353.73it/s] 21%|██        | 6017/28548 [00:18<00:58, 386.46it/s] 21%|██        | 6056/28548 [00:18<01:07, 334.34it/s] 21%|██▏       | 6093/28548 [00:18<01:05, 340.91it/s] 21%|██▏       | 6135/28548 [00:18<01:02, 357.40it/s] 22%|██▏       | 6172/28548 [00:18<01:02, 356.92it/s] 22%|██▏       | 6209/28548 [00:18<01:05, 343.35it/s] 22%|██▏       | 6244/28548 [00:18<01:07, 331.91it/s] 22%|██▏       | 6290/28548 [00:18<01:01, 359.97it/s] 22%|██▏       | 6328/28548 [00:18<01:04, 344.68it/s] 22%|██▏       | 6363/28548 [00:19<01:05, 338.16it/s] 22%|██▏       | 6423/28548 [00:19<00:55, 401.62it/s] 23%|██▎       | 6464/28548 [00:19<00:58, 379.55it/s] 23%|██▎       | 6503/28548 [00:19<00:59, 373.18it/s] 23%|██▎       | 6543/28548 [00:19<00:59, 372.90it/s] 23%|██▎       | 6583/28548 [00:19<00:58, 378.16it/s] 23%|██▎       | 6621/28548 [00:19<01:06, 332.08it/s] 23%|██▎       | 6656/28548 [00:19<01:12, 302.60it/s] 23%|██▎       | 6708/28548 [00:20<01:04, 339.36it/s] 24%|██▎       | 6743/28548 [00:20<01:13, 297.42it/s] 24%|██▍       | 6786/28548 [00:20<01:07, 324.57it/s] 24%|██▍       | 6836/28548 [00:20<01:00, 359.78it/s] 24%|██▍       | 6874/28548 [00:20<01:04, 334.49it/s] 24%|██▍       | 6909/28548 [00:20<01:04, 334.12it/s] 24%|██▍       | 6944/28548 [00:20<01:08, 315.89it/s] 24%|██▍       | 6990/28548 [00:20<01:05, 329.50it/s] 25%|██▍       | 7037/28548 [00:21<00:58, 365.33it/s] 25%|██▍       | 7075/28548 [00:21<01:03, 337.57it/s] 25%|██▍       | 7110/28548 [00:21<01:06, 322.52it/s] 25%|██▌       | 7143/28548 [00:21<01:09, 308.81it/s] 25%|██▌       | 7175/28548 [00:21<01:09, 309.30it/s] 25%|██▌       | 7212/28548 [00:21<01:05, 324.51it/s] 25%|██▌       | 7262/28548 [00:21<01:00, 351.03it/s] 26%|██▌       | 7298/28548 [00:21<01:04, 331.62it/s] 26%|██▌       | 7352/28548 [00:21<00:54, 386.47it/s] 26%|██▌       | 7392/28548 [00:22<01:02, 339.66it/s] 26%|██▌       | 7428/28548 [00:22<01:06, 319.73it/s] 26%|██▌       | 7476/28548 [00:22<00:58, 360.21it/s] 26%|██▋       | 7514/28548 [00:22<00:59, 351.04it/s] 26%|██▋       | 7563/28548 [00:22<00:55, 380.08it/s] 27%|██▋       | 7602/28548 [00:22<00:57, 363.03it/s] 27%|██▋       | 7643/28548 [00:22<00:57, 361.33it/s] 27%|██▋       | 7689/28548 [00:22<00:53, 387.81it/s] 27%|██▋       | 7731/28548 [00:22<00:52, 393.21it/s] 27%|██▋       | 7771/28548 [00:23<00:56, 368.73it/s] 27%|██▋       | 7810/28548 [00:23<00:55, 373.51it/s] 27%|██▋       | 7848/28548 [00:23<00:58, 356.51it/s] 28%|██▊       | 7887/28548 [00:23<00:58, 355.94it/s] 28%|██▊       | 7931/28548 [00:23<00:54, 375.38it/s] 28%|██▊       | 7969/28548 [00:23<01:03, 321.68it/s] 28%|██▊       | 8014/28548 [00:23<00:58, 352.94it/s] 28%|██▊       | 8063/28548 [00:23<00:53, 385.22it/s] 28%|██▊       | 8103/28548 [00:24<00:54, 376.65it/s] 29%|██▊       | 8142/28548 [00:24<00:59, 340.99it/s] 29%|██▊       | 8179/28548 [00:24<01:00, 335.29it/s] 29%|██▉       | 8214/28548 [00:24<01:02, 324.37it/s] 29%|██▉       | 8252/28548 [00:24<01:00, 336.49it/s] 29%|██▉       | 8287/28548 [00:24<01:02, 325.29it/s] 29%|██▉       | 8320/28548 [00:24<01:02, 323.05it/s] 29%|██▉       | 8363/28548 [00:24<00:57, 350.80it/s] 29%|██▉       | 8399/28548 [00:24<00:58, 346.83it/s] 30%|██▉       | 8434/28548 [00:25<01:09, 288.87it/s] 30%|██▉       | 8467/28548 [00:25<01:11, 282.61it/s] 30%|██▉       | 8504/28548 [00:25<01:05, 304.11it/s] 30%|██▉       | 8541/28548 [00:25<01:02, 321.07it/s] 30%|███       | 8575/28548 [00:25<01:01, 322.16it/s] 30%|███       | 8608/28548 [00:25<01:06, 298.76it/s] 30%|███       | 8665/28548 [00:25<00:55, 355.16it/s] 31%|███       | 8710/28548 [00:25<00:54, 366.27it/s] 31%|███       | 8748/28548 [00:25<00:54, 365.54it/s] 31%|███       | 8796/28548 [00:26<00:50, 394.46it/s] 31%|███       | 8836/28548 [00:26<00:52, 375.52it/s] 31%|███       | 8875/28548 [00:26<00:53, 365.66it/s] 31%|███       | 8917/28548 [00:26<00:52, 371.58it/s] 31%|███▏      | 8955/28548 [00:26<00:56, 345.89it/s] 32%|███▏      | 8993/28548 [00:26<00:57, 340.23it/s] 32%|███▏      | 9032/28548 [00:26<00:55, 349.76it/s] 32%|███▏      | 9068/28548 [00:26<00:57, 340.93it/s] 32%|███▏      | 9103/28548 [00:27<01:04, 302.23it/s] 32%|███▏      | 9135/28548 [00:27<01:05, 298.23it/s] 32%|███▏      | 9185/28548 [00:27<00:55, 347.61it/s] 32%|███▏      | 9221/28548 [00:27<01:00, 318.96it/s] 32%|███▏      | 9256/28548 [00:27<00:59, 325.67it/s] 33%|███▎      | 9307/28548 [00:27<00:51, 374.78it/s] 33%|███▎      | 9346/28548 [00:27<00:55, 344.66it/s] 33%|███▎      | 9382/28548 [00:27<00:58, 327.36it/s] 33%|███▎      | 9416/28548 [00:27<01:00, 318.36it/s] 33%|███▎      | 9449/28548 [00:28<01:00, 315.66it/s] 33%|███▎      | 9499/28548 [00:28<00:54, 352.72it/s] 33%|███▎      | 9535/28548 [00:28<01:00, 315.00it/s] 34%|███▎      | 9577/28548 [00:28<00:56, 337.20it/s] 34%|███▎      | 9612/28548 [00:28<00:59, 315.78it/s] 34%|███▍      | 9655/28548 [00:28<00:55, 343.42it/s] 34%|███▍      | 9709/28548 [00:28<00:47, 393.82it/s] 34%|███▍      | 9750/28548 [00:28<01:01, 307.66it/s] 34%|███▍      | 9802/28548 [00:29<00:56, 334.30it/s] 34%|███▍      | 9847/28548 [00:29<00:51, 361.74it/s] 35%|███▍      | 9886/28548 [00:29<00:54, 345.04it/s] 35%|███▍      | 9923/28548 [00:29<00:54, 342.15it/s] 35%|███▍      | 9984/28548 [00:29<00:46, 397.89it/s] 35%|███▌      | 10025/28548 [00:29<00:46, 397.02it/s] 35%|███▌      | 10066/28548 [00:29<00:54, 338.57it/s] 35%|███▌      | 10111/28548 [00:29<00:50, 363.44it/s] 36%|███▌      | 10150/28548 [00:30<00:52, 349.73it/s] 36%|███▌      | 10189/28548 [00:30<00:51, 353.77it/s] 36%|███▌      | 10236/28548 [00:30<00:49, 366.84it/s] 36%|███▌      | 10274/28548 [00:30<00:54, 336.29it/s] 36%|███▌      | 10309/28548 [00:30<00:54, 336.98it/s] 36%|███▌      | 10344/28548 [00:30<00:54, 334.84it/s] 36%|███▋      | 10396/28548 [00:30<00:47, 380.96it/s] 37%|███▋      | 10435/28548 [00:30<00:48, 374.05it/s] 37%|███▋      | 10473/28548 [00:30<00:48, 370.86it/s] 37%|███▋      | 10511/28548 [00:31<00:51, 352.96it/s] 37%|███▋      | 10547/28548 [00:31<00:53, 336.23it/s] 37%|███▋      | 10582/28548 [00:31<00:55, 322.17it/s] 37%|███▋      | 10615/28548 [00:31<00:55, 320.64it/s] 37%|███▋      | 10655/28548 [00:31<00:52, 340.11it/s] 38%|███▊      | 10725/28548 [00:31<00:41, 428.16it/s] 38%|███▊      | 10777/28548 [00:31<00:39, 452.86it/s] 38%|███▊      | 10823/28548 [00:31<00:44, 399.32it/s] 38%|███▊      | 10872/28548 [00:31<00:41, 422.89it/s] 38%|███▊      | 10916/28548 [00:32<00:45, 387.55it/s] 38%|███▊      | 10961/28548 [00:32<00:44, 395.73it/s] 39%|███▊      | 11002/28548 [00:32<00:44, 392.15it/s] 39%|███▊      | 11043/28548 [00:32<00:44, 389.88it/s] 39%|███▉      | 11083/28548 [00:32<00:47, 368.01it/s] 39%|███▉      | 11121/28548 [00:32<00:51, 335.33it/s] 39%|███▉      | 11164/28548 [00:32<00:50, 346.10it/s] 39%|███▉      | 11205/28548 [00:32<00:49, 347.64it/s] 39%|███▉      | 11246/28548 [00:33<00:49, 348.66it/s] 40%|███▉      | 11285/28548 [00:33<00:48, 359.15it/s] 40%|███▉      | 11322/28548 [00:33<00:49, 348.73it/s] 40%|███▉      | 11358/28548 [00:33<00:54, 314.95it/s] 40%|███▉      | 11391/28548 [00:33<00:55, 309.23it/s] 40%|████      | 11443/28548 [00:33<00:48, 350.79it/s] 40%|████      | 11479/28548 [00:33<00:50, 336.18it/s] 40%|████      | 11513/28548 [00:33<00:57, 296.89it/s] 41%|████      | 11577/28548 [00:34<00:45, 370.66it/s] 41%|████      | 11621/28548 [00:34<00:43, 384.77it/s] 41%|████      | 11661/28548 [00:34<00:44, 380.24it/s] 41%|████      | 11702/28548 [00:34<00:44, 382.35it/s] 41%|████      | 11741/28548 [00:34<00:45, 368.86it/s] 41%|████▏     | 11783/28548 [00:34<00:43, 382.33it/s] 41%|████▏     | 11822/28548 [00:34<00:44, 376.84it/s] 42%|████▏     | 11863/28548 [00:34<00:44, 375.10it/s] 42%|████▏     | 11901/28548 [00:34<00:45, 365.92it/s] 42%|████▏     | 11938/28548 [00:35<00:47, 348.01it/s] 42%|████▏     | 11975/28548 [00:35<00:49, 334.70it/s] 42%|████▏     | 12016/28548 [00:35<00:50, 326.85it/s] 42%|████▏     | 12051/28548 [00:35<00:49, 330.72it/s] 42%|████▏     | 12085/28548 [00:35<00:51, 319.29it/s] 42%|████▏     | 12118/28548 [00:35<00:52, 310.94it/s] 43%|████▎     | 12157/28548 [00:35<00:50, 321.84it/s] 43%|████▎     | 12194/28548 [00:35<00:48, 334.37it/s] 43%|████▎     | 12228/28548 [00:35<00:52, 313.34it/s] 43%|████▎     | 12274/28548 [00:36<00:46, 352.81it/s] 43%|████▎     | 12314/28548 [00:36<00:46, 352.10it/s] 43%|████▎     | 12350/28548 [00:36<00:48, 335.64it/s] 43%|████▎     | 12393/28548 [00:36<00:45, 357.15it/s] 44%|████▎     | 12430/28548 [00:36<00:48, 335.66it/s] 44%|████▎     | 12481/28548 [00:36<00:43, 371.89it/s] 44%|████▍     | 12519/28548 [00:36<00:44, 356.39it/s] 44%|████▍     | 12555/28548 [00:36<00:45, 354.09it/s] 44%|████▍     | 12618/28548 [00:36<00:37, 428.77it/s] 44%|████▍     | 12669/28548 [00:37<00:35, 447.68it/s] 45%|████▍     | 12715/28548 [00:37<00:38, 407.35it/s] 45%|████▍     | 12757/28548 [00:37<00:41, 377.87it/s] 45%|████▍     | 12813/28548 [00:37<00:39, 401.32it/s] 45%|████▌     | 12855/28548 [00:37<00:38, 403.15it/s] 45%|████▌     | 12909/28548 [00:37<00:36, 433.81it/s] 45%|████▌     | 12953/28548 [00:37<00:45, 345.90it/s] 46%|████▌     | 13007/28548 [00:37<00:40, 387.87it/s] 46%|████▌     | 13049/28548 [00:38<00:39, 391.74it/s] 46%|████▌     | 13092/28548 [00:38<00:39, 393.49it/s] 46%|████▌     | 13134/28548 [00:38<00:38, 398.45it/s] 46%|████▌     | 13184/28548 [00:38<00:36, 423.12it/s] 46%|████▋     | 13228/28548 [00:38<00:36, 421.28it/s] 46%|████▋     | 13273/28548 [00:38<00:35, 429.08it/s] 47%|████▋     | 13317/28548 [00:38<00:38, 394.90it/s] 47%|████▋     | 13361/28548 [00:38<00:37, 404.46it/s] 47%|████▋     | 13403/28548 [00:38<00:41, 365.94it/s] 47%|████▋     | 13443/28548 [00:39<00:41, 367.21it/s] 47%|████▋     | 13491/28548 [00:39<00:38, 386.81it/s] 47%|████▋     | 13535/28548 [00:39<00:39, 376.86it/s] 48%|████▊     | 13574/28548 [00:39<00:41, 359.34it/s] 48%|████▊     | 13631/28548 [00:39<00:38, 390.38it/s] 48%|████▊     | 13677/28548 [00:39<00:36, 402.10it/s] 48%|████▊     | 13724/28548 [00:39<00:35, 415.67it/s] 48%|████▊     | 13766/28548 [00:39<00:40, 361.41it/s] 48%|████▊     | 13804/28548 [00:40<00:40, 360.79it/s] 48%|████▊     | 13841/28548 [00:40<00:44, 330.97it/s] 49%|████▊     | 13875/28548 [00:40<00:47, 309.15it/s] 49%|████▊     | 13911/28548 [00:40<00:46, 316.53it/s] 49%|████▉     | 13944/28548 [00:40<00:46, 314.00it/s] 49%|████▉     | 13977/28548 [00:40<00:47, 304.82it/s] 49%|████▉     | 14018/28548 [00:40<00:44, 323.18it/s] 49%|████▉     | 14053/28548 [00:40<00:44, 327.78it/s] 49%|████▉     | 14086/28548 [00:40<00:44, 328.18it/s] 49%|████▉     | 14127/28548 [00:41<00:41, 348.64it/s] 50%|████▉     | 14167/28548 [00:41<00:41, 347.46it/s] 50%|████▉     | 14202/28548 [00:41<00:42, 339.70it/s] 50%|████▉     | 14237/28548 [00:41<00:46, 307.55it/s] 50%|████▉     | 14273/28548 [00:41<00:45, 315.87it/s] 50%|█████     | 14312/28548 [00:41<00:42, 333.94it/s] 50%|█████     | 14346/28548 [00:41<00:46, 307.57it/s] 50%|█████     | 14398/28548 [00:41<00:38, 362.84it/s] 51%|█████     | 14436/28548 [00:41<00:44, 318.36it/s] 51%|█████     | 14470/28548 [00:42<00:44, 315.36it/s] 51%|█████     | 14524/28548 [00:42<00:39, 353.09it/s] 51%|█████     | 14568/28548 [00:42<00:37, 371.43it/s] 51%|█████     | 14606/28548 [00:42<00:41, 338.48it/s] 51%|█████▏    | 14641/28548 [00:42<00:43, 318.89it/s] 51%|█████▏    | 14674/28548 [00:42<00:47, 294.93it/s] 52%|█████▏    | 14710/28548 [00:42<00:44, 307.76it/s] 52%|█████▏    | 14760/28548 [00:42<00:40, 344.15it/s] 52%|█████▏    | 14801/28548 [00:43<00:38, 356.85it/s] 52%|█████▏    | 14846/28548 [00:43<00:36, 374.98it/s] 52%|█████▏    | 14884/28548 [00:43<00:37, 360.07it/s] 52%|█████▏    | 14921/28548 [00:43<00:39, 345.79it/s] 52%|█████▏    | 14956/28548 [00:43<00:39, 345.53it/s] 53%|█████▎    | 15003/28548 [00:43<00:35, 376.70it/s] 53%|█████▎    | 15049/28548 [00:43<00:34, 395.49it/s] 53%|█████▎    | 15089/28548 [00:43<00:34, 393.82it/s] 53%|█████▎    | 15131/28548 [00:43<00:33, 396.65it/s] 53%|█████▎    | 15171/28548 [00:44<00:35, 373.04it/s] 53%|█████▎    | 15221/28548 [00:44<00:34, 387.09it/s] 53%|█████▎    | 15265/28548 [00:44<00:33, 396.20it/s] 54%|█████▎    | 15306/28548 [00:44<00:33, 399.59it/s] 54%|█████▍    | 15347/28548 [00:44<00:37, 352.15it/s] 54%|█████▍    | 15384/28548 [00:44<00:37, 352.20it/s] 54%|█████▍    | 15427/28548 [00:44<00:35, 364.70it/s] 54%|█████▍    | 15480/28548 [00:44<00:32, 399.68it/s] 54%|█████▍    | 15521/28548 [00:44<00:34, 381.68it/s] 55%|█████▍    | 15563/28548 [00:45<00:34, 381.75it/s] 55%|█████▍    | 15602/28548 [00:45<00:33, 383.56it/s] 55%|█████▍    | 15641/28548 [00:45<00:35, 363.38it/s] 55%|█████▍    | 15683/28548 [00:45<00:34, 370.41it/s] 55%|█████▌    | 15722/28548 [00:45<00:34, 374.58it/s] 55%|█████▌    | 15760/28548 [00:45<00:34, 373.86it/s] 55%|█████▌    | 15804/28548 [00:45<00:32, 392.07it/s] 56%|█████▌    | 15848/28548 [00:45<00:32, 392.11it/s] 56%|█████▌    | 15888/28548 [00:45<00:34, 363.06it/s] 56%|█████▌    | 15930/28548 [00:46<00:34, 361.95it/s] 56%|█████▌    | 15967/28548 [00:46<00:35, 353.82it/s] 56%|█████▌    | 16003/28548 [00:46<00:35, 353.16it/s] 56%|█████▌    | 16039/28548 [00:46<00:36, 346.24it/s] 56%|█████▋    | 16082/28548 [00:46<00:34, 356.68it/s] 56%|█████▋    | 16118/28548 [00:46<00:36, 336.02it/s] 57%|█████▋    | 16160/28548 [00:46<00:34, 358.69it/s] 57%|█████▋    | 16220/28548 [00:46<00:28, 426.08it/s] 57%|█████▋    | 16264/28548 [00:46<00:32, 380.48it/s] 57%|█████▋    | 16304/28548 [00:47<00:32, 381.24it/s] 57%|█████▋    | 16344/28548 [00:47<00:35, 339.91it/s] 57%|█████▋    | 16399/28548 [00:47<00:31, 381.35it/s] 58%|█████▊    | 16440/28548 [00:47<00:31, 381.23it/s] 58%|█████▊    | 16488/28548 [00:47<00:30, 400.73it/s] 58%|█████▊    | 16530/28548 [00:47<00:30, 399.39it/s] 58%|█████▊    | 16591/28548 [00:47<00:26, 447.90it/s] 58%|█████▊    | 16637/28548 [00:47<00:27, 437.10it/s] 58%|█████▊    | 16682/28548 [00:48<00:33, 349.41it/s] 59%|█████▊    | 16744/28548 [00:48<00:28, 414.19it/s] 59%|█████▉    | 16790/28548 [00:48<00:30, 384.67it/s] 59%|█████▉    | 16832/28548 [00:48<00:31, 366.16it/s] 59%|█████▉    | 16871/28548 [00:48<00:32, 357.67it/s] 59%|█████▉    | 16909/28548 [00:48<00:34, 333.63it/s] 59%|█████▉    | 16965/28548 [00:48<00:30, 385.62it/s] 60%|█████▉    | 17006/28548 [00:48<00:29, 390.67it/s] 60%|█████▉    | 17049/28548 [00:48<00:28, 398.96it/s] 60%|█████▉    | 17090/28548 [00:49<00:28, 397.34it/s] 60%|██████    | 17131/28548 [00:49<00:34, 334.99it/s] 60%|██████    | 17178/28548 [00:49<00:31, 364.08it/s] 60%|██████    | 17217/28548 [00:49<00:30, 370.29it/s] 60%|██████    | 17256/28548 [00:49<00:31, 363.46it/s] 61%|██████    | 17295/28548 [00:49<00:31, 356.16it/s] 61%|██████    | 17332/28548 [00:49<00:31, 354.74it/s] 61%|██████    | 17383/28548 [00:49<00:29, 374.94it/s] 61%|██████    | 17426/28548 [00:50<00:28, 386.97it/s] 61%|██████    | 17465/28548 [00:50<00:34, 323.90it/s] 61%|██████▏   | 17500/28548 [00:50<00:34, 319.19it/s] 61%|██████▏   | 17544/28548 [00:50<00:32, 343.30it/s] 62%|██████▏   | 17580/28548 [00:50<00:34, 320.64it/s] 62%|██████▏   | 17613/28548 [00:50<00:36, 302.45it/s] 62%|██████▏   | 17668/28548 [00:50<00:30, 361.50it/s] 62%|██████▏   | 17706/28548 [00:50<00:30, 360.67it/s] 62%|██████▏   | 17743/28548 [00:51<00:32, 329.93it/s] 62%|██████▏   | 17780/28548 [00:51<00:31, 338.59it/s] 62%|██████▏   | 17815/28548 [00:51<00:33, 325.00it/s] 63%|██████▎   | 17869/28548 [00:51<00:28, 374.75it/s] 63%|██████▎   | 17936/28548 [00:51<00:23, 450.52it/s] 63%|██████▎   | 17983/28548 [00:51<00:28, 367.04it/s] 63%|██████▎   | 18039/28548 [00:51<00:25, 407.98it/s] 63%|██████▎   | 18083/28548 [00:51<00:28, 371.54it/s] 63%|██████▎   | 18123/28548 [00:51<00:27, 375.82it/s] 64%|██████▎   | 18175/28548 [00:52<00:25, 408.52it/s] 64%|██████▍   | 18218/28548 [00:52<00:26, 386.58it/s] 64%|██████▍   | 18258/28548 [00:52<00:27, 380.38it/s] 64%|██████▍   | 18298/28548 [00:52<00:27, 374.62it/s] 64%|██████▍   | 18337/28548 [00:52<00:30, 330.08it/s] 64%|██████▍   | 18383/28548 [00:52<00:28, 360.98it/s] 65%|██████▍   | 18428/28548 [00:52<00:26, 381.15it/s] 65%|██████▍   | 18468/28548 [00:52<00:26, 382.50it/s] 65%|██████▍   | 18508/28548 [00:53<00:27, 366.53it/s] 65%|██████▍   | 18549/28548 [00:53<00:26, 376.02it/s] 65%|██████▌   | 18588/28548 [00:53<00:27, 361.52it/s] 65%|██████▌   | 18625/28548 [00:53<00:28, 344.81it/s] 65%|██████▌   | 18671/28548 [00:53<00:26, 368.16it/s] 66%|██████▌   | 18709/28548 [00:53<00:30, 322.30it/s] 66%|██████▌   | 18755/28548 [00:53<00:27, 353.83it/s] 66%|██████▌   | 18792/28548 [00:53<00:32, 297.22it/s] 66%|██████▌   | 18825/28548 [00:54<00:32, 299.27it/s] 66%|██████▌   | 18868/28548 [00:54<00:29, 327.10it/s] 66%|██████▌   | 18905/28548 [00:54<00:28, 336.43it/s] 66%|██████▋   | 18953/28548 [00:54<00:25, 374.50it/s] 67%|██████▋   | 18992/28548 [00:54<00:26, 355.18it/s] 67%|██████▋   | 19029/28548 [00:54<00:29, 327.27it/s] 67%|██████▋   | 19083/28548 [00:54<00:24, 380.96it/s] 67%|██████▋   | 19123/28548 [00:54<00:28, 332.60it/s] 67%|██████▋   | 19182/28548 [00:54<00:23, 396.03it/s] 67%|██████▋   | 19225/28548 [00:55<00:30, 306.86it/s] 67%|██████▋   | 19261/28548 [00:55<00:32, 286.63it/s] 68%|██████▊   | 19298/28548 [00:55<00:31, 290.51it/s] 68%|██████▊   | 19345/28548 [00:55<00:27, 331.33it/s] 68%|██████▊   | 19381/28548 [00:55<00:27, 328.42it/s] 68%|██████▊   | 19416/28548 [00:55<00:30, 304.24it/s] 68%|██████▊   | 19460/28548 [00:55<00:27, 329.35it/s] 68%|██████▊   | 19502/28548 [00:56<00:27, 326.57it/s] 68%|██████▊   | 19550/28548 [00:56<00:25, 354.72it/s] 69%|██████▊   | 19587/28548 [00:56<00:26, 334.17it/s] 69%|██████▊   | 19626/28548 [00:56<00:25, 346.28it/s] 69%|██████▉   | 19663/28548 [00:56<00:25, 346.18it/s] 69%|██████▉   | 19699/28548 [00:56<00:26, 329.81it/s] 69%|██████▉   | 19733/28548 [00:56<00:28, 308.30it/s] 69%|██████▉   | 19774/28548 [00:56<00:26, 334.97it/s] 69%|██████▉   | 19809/28548 [00:57<00:30, 288.94it/s] 70%|██████▉   | 19841/28548 [00:57<00:30, 289.94it/s] 70%|██████▉   | 19872/28548 [00:57<00:30, 288.73it/s] 70%|██████▉   | 19910/28548 [00:57<00:27, 311.17it/s] 70%|██████▉   | 19962/28548 [00:57<00:23, 362.08it/s] 70%|███████   | 19999/28548 [00:57<00:23, 363.97it/s] 70%|███████   | 20036/28548 [00:57<00:23, 361.88it/s] 70%|███████   | 20073/28548 [00:57<00:24, 341.31it/s] 70%|███████   | 20108/28548 [00:57<00:26, 318.14it/s] 71%|███████   | 20151/28548 [00:57<00:24, 344.75it/s] 71%|███████   | 20187/28548 [00:58<00:24, 340.44it/s] 71%|███████   | 20227/28548 [00:58<00:23, 348.83it/s] 71%|███████   | 20263/28548 [00:58<00:23, 345.82it/s] 71%|███████   | 20298/28548 [00:58<00:24, 341.38it/s] 71%|███████▏  | 20351/28548 [00:58<00:20, 392.19it/s] 71%|███████▏  | 20391/28548 [00:58<00:20, 392.65it/s] 72%|███████▏  | 20431/28548 [00:58<00:20, 386.92it/s] 72%|███████▏  | 20470/28548 [00:58<00:21, 379.96it/s] 72%|███████▏  | 20509/28548 [00:58<00:23, 347.98it/s] 72%|███████▏  | 20545/28548 [00:59<00:23, 333.87it/s] 72%|███████▏  | 20588/28548 [00:59<00:22, 349.29it/s] 72%|███████▏  | 20631/28548 [00:59<00:21, 371.17it/s] 72%|███████▏  | 20675/28548 [00:59<00:20, 384.37it/s] 73%|███████▎  | 20714/28548 [00:59<00:21, 364.60it/s] 73%|███████▎  | 20762/28548 [00:59<00:19, 392.50it/s] 73%|███████▎  | 20804/28548 [00:59<00:19, 398.21it/s] 73%|███████▎  | 20845/28548 [00:59<00:20, 381.73it/s] 73%|███████▎  | 20891/28548 [00:59<00:19, 386.56it/s] 73%|███████▎  | 20930/28548 [01:00<00:21, 361.62it/s] 73%|███████▎  | 20981/28548 [01:00<00:20, 371.46it/s] 74%|███████▎  | 21019/28548 [01:00<00:20, 361.87it/s] 74%|███████▍  | 21056/28548 [01:00<00:21, 344.15it/s] 74%|███████▍  | 21092/28548 [01:00<00:21, 343.27it/s] 74%|███████▍  | 21127/28548 [01:00<00:22, 332.50it/s] 74%|███████▍  | 21161/28548 [01:00<00:22, 322.10it/s] 74%|███████▍  | 21212/28548 [01:00<00:19, 368.90it/s] 74%|███████▍  | 21250/28548 [01:01<00:20, 352.87it/s] 75%|███████▍  | 21287/28548 [01:01<00:20, 353.40it/s] 75%|███████▍  | 21328/28548 [01:01<00:19, 363.33it/s] 75%|███████▍  | 21365/28548 [01:01<00:20, 358.78it/s] 75%|███████▍  | 21409/28548 [01:01<00:19, 373.35it/s] 75%|███████▌  | 21455/28548 [01:01<00:17, 394.45it/s] 75%|███████▌  | 21499/28548 [01:01<00:17, 396.60it/s] 75%|███████▌  | 21539/28548 [01:01<00:18, 387.54it/s] 76%|███████▌  | 21578/28548 [01:01<00:19, 350.15it/s] 76%|███████▌  | 21615/28548 [01:02<00:19, 347.85it/s] 76%|███████▌  | 21657/28548 [01:02<00:19, 360.00it/s] 76%|███████▌  | 21700/28548 [01:02<00:18, 367.38it/s] 76%|███████▌  | 21737/28548 [01:02<00:21, 322.68it/s] 76%|███████▋  | 21789/28548 [01:02<00:18, 370.53it/s] 76%|███████▋  | 21828/28548 [01:02<00:18, 369.05it/s] 77%|███████▋  | 21866/28548 [01:02<00:18, 356.64it/s] 77%|███████▋  | 21915/28548 [01:02<00:17, 381.15it/s] 77%|███████▋  | 21958/28548 [01:02<00:16, 391.78it/s] 77%|███████▋  | 21999/28548 [01:03<00:17, 378.41it/s] 77%|███████▋  | 22038/28548 [01:03<00:18, 343.26it/s] 77%|███████▋  | 22074/28548 [01:03<00:18, 341.61it/s] 78%|███████▊  | 22148/28548 [01:03<00:14, 434.51it/s] 78%|███████▊  | 22199/28548 [01:03<00:14, 444.24it/s] 78%|███████▊  | 22244/28548 [01:03<00:16, 381.71it/s] 78%|███████▊  | 22284/28548 [01:03<00:17, 359.35it/s] 78%|███████▊  | 22325/28548 [01:03<00:16, 370.35it/s] 78%|███████▊  | 22364/28548 [01:04<00:17, 359.13it/s] 78%|███████▊  | 22401/28548 [01:04<00:20, 305.09it/s] 79%|███████▊  | 22441/28548 [01:04<00:19, 318.32it/s] 79%|███████▊  | 22478/28548 [01:04<00:18, 330.71it/s] 79%|███████▉  | 22513/28548 [01:04<00:19, 302.17it/s] 79%|███████▉  | 22558/28548 [01:04<00:17, 339.37it/s] 79%|███████▉  | 22596/28548 [01:04<00:17, 344.10it/s] 79%|███████▉  | 22632/28548 [01:04<00:17, 336.83it/s] 79%|███████▉  | 22675/28548 [01:04<00:16, 359.21it/s] 80%|███████▉  | 22712/28548 [01:05<00:16, 344.17it/s] 80%|███████▉  | 22767/28548 [01:05<00:14, 397.51it/s] 80%|███████▉  | 22816/28548 [01:05<00:14, 394.70it/s] 80%|████████  | 22857/28548 [01:05<00:16, 337.77it/s] 80%|████████  | 22893/28548 [01:05<00:16, 342.25it/s] 80%|████████  | 22935/28548 [01:05<00:15, 359.13it/s] 81%|████████  | 22995/28548 [01:05<00:13, 400.31it/s] 81%|████████  | 23036/28548 [01:05<00:13, 400.61it/s] 81%|████████  | 23077/28548 [01:06<00:14, 370.13it/s] 81%|████████  | 23126/28548 [01:06<00:13, 395.23it/s] 81%|████████  | 23167/28548 [01:06<00:14, 379.54it/s] 81%|████████▏ | 23208/28548 [01:06<00:14, 356.56it/s] 81%|████████▏ | 23258/28548 [01:06<00:13, 378.52it/s] 82%|████████▏ | 23297/28548 [01:06<00:14, 355.58it/s] 82%|████████▏ | 23339/28548 [01:06<00:14, 367.67it/s] 82%|████████▏ | 23378/28548 [01:06<00:14, 367.90it/s] 82%|████████▏ | 23418/28548 [01:06<00:14, 358.23it/s] 82%|████████▏ | 23455/28548 [01:07<00:14, 354.36it/s] 82%|████████▏ | 23501/28548 [01:07<00:13, 379.94it/s] 82%|████████▏ | 23540/28548 [01:07<00:13, 370.21it/s] 83%|████████▎ | 23582/28548 [01:07<00:12, 383.45it/s] 83%|████████▎ | 23633/28548 [01:07<00:11, 415.32it/s] 83%|████████▎ | 23675/28548 [01:07<00:12, 377.62it/s] 83%|████████▎ | 23714/28548 [01:07<00:13, 358.78it/s] 83%|████████▎ | 23773/28548 [01:07<00:11, 418.84it/s] 83%|████████▎ | 23816/28548 [01:07<00:12, 389.52it/s] 84%|████████▎ | 23864/28548 [01:08<00:11, 413.31it/s] 84%|████████▎ | 23907/28548 [01:08<00:11, 400.14it/s] 84%|████████▍ | 23948/28548 [01:08<00:11, 385.74it/s] 84%|████████▍ | 23988/28548 [01:08<00:12, 351.02it/s] 84%|████████▍ | 24024/28548 [01:08<00:13, 339.58it/s] 84%|████████▍ | 24070/28548 [01:08<00:12, 371.03it/s] 85%|████████▍ | 24126/28548 [01:08<00:10, 411.66it/s] 85%|████████▍ | 24168/28548 [01:08<00:11, 392.15it/s] 85%|████████▍ | 24208/28548 [01:09<00:11, 388.41it/s] 85%|████████▍ | 24248/28548 [01:09<00:11, 390.46it/s] 85%|████████▌ | 24293/28548 [01:09<00:10, 399.83it/s] 85%|████████▌ | 24334/28548 [01:09<00:10, 395.86it/s] 85%|████████▌ | 24387/28548 [01:09<00:09, 428.41it/s] 86%|████████▌ | 24438/28548 [01:09<00:09, 451.71it/s] 86%|████████▌ | 24484/28548 [01:09<00:09, 419.91it/s] 86%|████████▌ | 24528/28548 [01:09<00:09, 420.75it/s] 86%|████████▌ | 24571/28548 [01:09<00:09, 408.31it/s] 86%|████████▌ | 24613/28548 [01:09<00:09, 401.06it/s] 86%|████████▋ | 24654/28548 [01:10<00:10, 354.58it/s] 86%|████████▋ | 24691/28548 [01:10<00:11, 321.66it/s] 87%|████████▋ | 24730/28548 [01:10<00:11, 324.48it/s] 87%|████████▋ | 24773/28548 [01:10<00:10, 349.96it/s] 87%|████████▋ | 24809/28548 [01:10<00:11, 316.85it/s] 87%|████████▋ | 24854/28548 [01:10<00:10, 341.94it/s] 87%|████████▋ | 24900/28548 [01:10<00:09, 365.32it/s] 87%|████████▋ | 24948/28548 [01:10<00:09, 369.20it/s] 88%|████████▊ | 24986/28548 [01:11<00:11, 316.87it/s] 88%|████████▊ | 25037/28548 [01:11<00:09, 351.29it/s] 88%|████████▊ | 25074/28548 [01:11<00:11, 308.46it/s] 88%|████████▊ | 25107/28548 [01:11<00:11, 307.27it/s] 88%|████████▊ | 25143/28548 [01:11<00:10, 317.46it/s] 88%|████████▊ | 25189/28548 [01:11<00:09, 348.48it/s] 88%|████████▊ | 25225/28548 [01:11<00:09, 342.53it/s] 89%|████████▊ | 25266/28548 [01:11<00:09, 356.92it/s] 89%|████████▊ | 25307/28548 [01:12<00:08, 365.44it/s] 89%|████████▉ | 25344/28548 [01:12<00:08, 364.70it/s] 89%|████████▉ | 25386/28548 [01:12<00:08, 373.27it/s] 89%|████████▉ | 25430/28548 [01:12<00:08, 380.18it/s] 89%|████████▉ | 25469/28548 [01:12<00:08, 376.02it/s] 89%|████████▉ | 25525/28548 [01:12<00:07, 426.55it/s] 90%|████████▉ | 25571/28548 [01:12<00:06, 429.01it/s] 90%|████████▉ | 25624/28548 [01:12<00:06, 457.13it/s] 90%|████████▉ | 25670/28548 [01:12<00:07, 391.29it/s] 90%|█████████ | 25711/28548 [01:13<00:07, 390.63it/s] 90%|█████████ | 25764/28548 [01:13<00:06, 423.38it/s] 90%|█████████ | 25808/28548 [01:13<00:06, 398.20it/s] 91%|█████████ | 25849/28548 [01:13<00:07, 375.39it/s] 91%|█████████ | 25893/28548 [01:13<00:06, 389.98it/s] 91%|█████████ | 25944/28548 [01:13<00:06, 410.36it/s] 91%|█████████ | 25986/28548 [01:13<00:07, 355.75it/s] 91%|█████████ | 26024/28548 [01:13<00:07, 342.54it/s] 91%|█████████▏| 26060/28548 [01:14<00:08, 279.30it/s] 91%|█████████▏| 26113/28548 [01:14<00:07, 329.60it/s] 92%|█████████▏| 26167/28548 [01:14<00:06, 378.21it/s] 92%|█████████▏| 26208/28548 [01:14<00:06, 334.55it/s] 92%|█████████▏| 26247/28548 [01:14<00:06, 346.78it/s] 92%|█████████▏| 26293/28548 [01:14<00:06, 360.64it/s] 92%|█████████▏| 26331/28548 [01:14<00:06, 360.72it/s] 92%|█████████▏| 26371/28548 [01:14<00:05, 364.82it/s] 93%|█████████▎| 26409/28548 [01:15<00:05, 358.97it/s] 93%|█████████▎| 26454/28548 [01:15<00:05, 369.76it/s] 93%|█████████▎| 26492/28548 [01:15<00:05, 361.23it/s] 93%|█████████▎| 26533/28548 [01:15<00:05, 370.77it/s] 93%|█████████▎| 26571/28548 [01:15<00:05, 346.50it/s] 93%|█████████▎| 26610/28548 [01:15<00:05, 349.50it/s] 93%|█████████▎| 26646/28548 [01:15<00:05, 347.32it/s] 93%|█████████▎| 26691/28548 [01:15<00:05, 367.03it/s] 94%|█████████▎| 26728/28548 [01:15<00:05, 357.81it/s] 94%|█████████▍| 26769/28548 [01:16<00:04, 362.63it/s] 94%|█████████▍| 26812/28548 [01:16<00:04, 365.81it/s] 94%|█████████▍| 26849/28548 [01:16<00:05, 321.99it/s] 94%|█████████▍| 26893/28548 [01:16<00:04, 350.41it/s] 94%|█████████▍| 26935/28548 [01:16<00:04, 363.02it/s] 94%|█████████▍| 26973/28548 [01:16<00:04, 356.71it/s] 95%|█████████▍| 27010/28548 [01:16<00:04, 317.43it/s] 95%|█████████▍| 27047/28548 [01:16<00:04, 322.03it/s] 95%|█████████▍| 27104/28548 [01:16<00:03, 386.85it/s] 95%|█████████▌| 27145/28548 [01:17<00:03, 371.20it/s] 95%|█████████▌| 27184/28548 [01:17<00:03, 358.29it/s] 95%|█████████▌| 27225/28548 [01:17<00:03, 371.95it/s] 95%|█████████▌| 27263/28548 [01:17<00:03, 363.59it/s] 96%|█████████▌| 27302/28548 [01:17<00:03, 369.45it/s] 96%|█████████▌| 27344/28548 [01:17<00:03, 362.83it/s] 96%|█████████▌| 27387/28548 [01:17<00:03, 380.68it/s] 96%|█████████▌| 27426/28548 [01:17<00:02, 382.26it/s] 96%|█████████▌| 27465/28548 [01:17<00:02, 372.38it/s] 96%|█████████▋| 27506/28548 [01:18<00:02, 381.86it/s] 96%|█████████▋| 27548/28548 [01:18<00:02, 392.26it/s] 97%|█████████▋| 27588/28548 [01:18<00:02, 388.59it/s] 97%|█████████▋| 27627/28548 [01:18<00:02, 351.17it/s] 97%|█████████▋| 27665/28548 [01:18<00:02, 349.72it/s] 97%|█████████▋| 27718/28548 [01:18<00:02, 394.73it/s] 97%|█████████▋| 27759/28548 [01:18<00:02, 387.75it/s] 97%|█████████▋| 27801/28548 [01:18<00:01, 394.15it/s] 98%|█████████▊| 27841/28548 [01:18<00:01, 370.84it/s] 98%|█████████▊| 27879/28548 [01:19<00:01, 353.82it/s] 98%|█████████▊| 27927/28548 [01:19<00:01, 385.72it/s] 98%|█████████▊| 27981/28548 [01:19<00:01, 416.39it/s] 98%|█████████▊| 28024/28548 [01:19<00:01, 413.40it/s] 98%|█████████▊| 28066/28548 [01:19<00:01, 381.02it/s] 98%|█████████▊| 28105/28548 [01:19<00:01, 368.83it/s] 99%|█████████▊| 28160/28548 [01:19<00:00, 409.74it/s] 99%|█████████▉| 28202/28548 [01:19<00:00, 361.15it/s] 99%|█████████▉| 28242/28548 [01:20<00:00, 361.42it/s] 99%|█████████▉| 28279/28548 [01:20<00:00, 359.56it/s] 99%|█████████▉| 28316/28548 [01:20<00:00, 340.81it/s] 99%|█████████▉| 28352/28548 [01:20<00:00, 339.72it/s] 99%|█████████▉| 28392/28548 [01:20<00:00, 348.83it/s]100%|█████████▉| 28450/28548 [01:20<00:00, 399.37it/s]100%|█████████▉| 28509/28548 [01:20<00:00, 438.67it/s]100%|██████████| 28548/28548 [01:20<00:00, 353.49it/s]
aug_valid process done: X = (28548, 3, 2, 2000), y = (28548,)
  0%|          | 0/10573 [00:00<?, ?it/s]  0%|          | 27/10573 [00:00<00:44, 236.91it/s]  1%|          | 58/10573 [00:00<00:38, 275.36it/s]  1%|          | 86/10573 [00:00<00:50, 208.55it/s]  1%|          | 109/10573 [00:00<00:50, 205.50it/s]  1%|          | 131/10573 [00:00<00:51, 201.48it/s]  1%|▏         | 152/10573 [00:00<00:55, 187.32it/s]  2%|▏         | 177/10573 [00:00<00:51, 201.96it/s]  2%|▏         | 198/10573 [00:00<00:54, 190.49it/s]  2%|▏         | 218/10573 [00:01<01:00, 171.49it/s]  2%|▏         | 237/10573 [00:01<00:59, 174.17it/s]  2%|▏         | 257/10573 [00:01<00:57, 180.95it/s]  3%|▎         | 276/10573 [00:01<01:01, 167.53it/s]  3%|▎         | 294/10573 [00:01<01:02, 164.01it/s]  3%|▎         | 313/10573 [00:01<01:00, 169.14it/s]  3%|▎         | 333/10573 [00:01<00:58, 175.34it/s]  3%|▎         | 351/10573 [00:01<00:58, 173.47it/s]  3%|▎         | 369/10573 [00:02<01:00, 169.85it/s]  4%|▎         | 387/10573 [00:02<01:02, 163.25it/s]  4%|▍         | 409/10573 [00:02<01:01, 165.92it/s]  4%|▍         | 428/10573 [00:02<01:00, 167.37it/s]  4%|▍         | 446/10573 [00:02<00:59, 169.45it/s]  4%|▍         | 463/10573 [00:02<01:02, 162.52it/s]  5%|▍         | 484/10573 [00:02<01:02, 160.89it/s]  5%|▍         | 506/10573 [00:02<00:58, 172.40it/s]  5%|▌         | 535/10573 [00:02<00:49, 202.81it/s]  5%|▌         | 559/10573 [00:03<00:47, 211.85it/s]  5%|▌         | 581/10573 [00:03<00:55, 178.85it/s]  6%|▌         | 606/10573 [00:03<00:51, 192.10it/s]  6%|▌         | 645/10573 [00:03<00:43, 228.40it/s]  6%|▋         | 672/10573 [00:03<00:42, 232.98it/s]  7%|▋         | 696/10573 [00:03<00:43, 226.88it/s]  7%|▋         | 719/10573 [00:03<00:49, 199.51it/s]  7%|▋         | 751/10573 [00:03<00:44, 221.42it/s]  7%|▋         | 781/10573 [00:04<00:40, 239.64it/s]  8%|▊         | 816/10573 [00:04<00:37, 260.01it/s]  8%|▊         | 845/10573 [00:04<00:36, 265.77it/s]  8%|▊         | 873/10573 [00:04<00:38, 249.37it/s]  9%|▊         | 899/10573 [00:04<00:39, 243.08it/s]  9%|▊         | 924/10573 [00:04<00:42, 227.42it/s]  9%|▉         | 957/10573 [00:04<00:38, 247.94it/s]  9%|▉         | 983/10573 [00:04<00:39, 242.76it/s] 10%|▉         | 1015/10573 [00:04<00:38, 247.69it/s] 10%|▉         | 1040/10573 [00:05<00:39, 240.98it/s] 10%|█         | 1076/10573 [00:05<00:37, 252.91it/s] 10%|█         | 1102/10573 [00:05<00:39, 238.65it/s] 11%|█         | 1137/10573 [00:05<00:39, 238.85it/s] 11%|█         | 1168/10573 [00:05<00:37, 251.59it/s] 11%|█▏        | 1194/10573 [00:05<00:38, 242.02it/s] 12%|█▏        | 1225/10573 [00:05<00:37, 252.56it/s] 12%|█▏        | 1251/10573 [00:05<00:41, 227.21it/s] 12%|█▏        | 1275/10573 [00:06<00:42, 219.96it/s] 12%|█▏        | 1315/10573 [00:06<00:34, 265.99it/s] 13%|█▎        | 1343/10573 [00:06<00:42, 216.98it/s] 13%|█▎        | 1387/10573 [00:06<00:34, 269.19it/s] 13%|█▎        | 1417/10573 [00:06<00:34, 267.67it/s] 14%|█▎        | 1446/10573 [00:06<00:37, 241.42it/s] 14%|█▍        | 1476/10573 [00:06<00:36, 252.67it/s] 14%|█▍        | 1503/10573 [00:06<00:39, 232.23it/s] 14%|█▍        | 1531/10573 [00:07<00:38, 237.14it/s] 15%|█▍        | 1556/10573 [00:07<00:38, 235.11it/s] 15%|█▍        | 1585/10573 [00:07<00:37, 241.22it/s] 15%|█▌        | 1610/10573 [00:07<00:39, 227.11it/s] 15%|█▌        | 1634/10573 [00:07<00:40, 221.89it/s] 16%|█▌        | 1657/10573 [00:07<00:45, 196.75it/s] 16%|█▌        | 1686/10573 [00:07<00:41, 215.57it/s] 16%|█▌        | 1717/10573 [00:07<00:37, 237.78it/s] 16%|█▋        | 1742/10573 [00:08<00:44, 199.89it/s] 17%|█▋        | 1779/10573 [00:08<00:40, 219.79it/s] 17%|█▋        | 1802/10573 [00:08<00:41, 211.62it/s] 17%|█▋        | 1827/10573 [00:08<00:39, 219.22it/s] 18%|█▊        | 1856/10573 [00:08<00:36, 235.65it/s] 18%|█▊        | 1899/10573 [00:08<00:30, 287.10it/s] 18%|█▊        | 1929/10573 [00:08<00:33, 258.97it/s] 19%|█▊        | 1958/10573 [00:08<00:32, 262.07it/s] 19%|█▉        | 1986/10573 [00:09<00:34, 250.47it/s] 19%|█▉        | 2012/10573 [00:09<00:37, 230.70it/s] 19%|█▉        | 2036/10573 [00:09<00:36, 231.83it/s] 20%|█▉        | 2066/10573 [00:09<00:34, 244.17it/s] 20%|█▉        | 2091/10573 [00:09<00:35, 236.90it/s] 20%|██        | 2117/10573 [00:09<00:36, 232.54it/s] 20%|██        | 2142/10573 [00:09<00:37, 227.05it/s] 21%|██        | 2188/10573 [00:09<00:28, 289.48it/s] 21%|██        | 2218/10573 [00:10<00:34, 239.67it/s] 21%|██        | 2245/10573 [00:10<00:35, 232.43it/s] 21%|██▏       | 2270/10573 [00:10<00:36, 230.46it/s] 22%|██▏       | 2294/10573 [00:10<00:35, 231.08it/s] 22%|██▏       | 2318/10573 [00:10<00:38, 215.49it/s] 22%|██▏       | 2341/10573 [00:10<00:44, 183.56it/s] 22%|██▏       | 2361/10573 [00:10<00:44, 184.57it/s] 23%|██▎       | 2388/10573 [00:10<00:40, 202.29it/s] 23%|██▎       | 2434/10573 [00:10<00:30, 264.77it/s] 23%|██▎       | 2462/10573 [00:11<00:31, 259.27it/s] 24%|██▎       | 2489/10573 [00:11<00:30, 262.09it/s] 24%|██▍       | 2516/10573 [00:11<00:32, 249.71it/s] 24%|██▍       | 2542/10573 [00:11<00:32, 245.81it/s] 24%|██▍       | 2570/10573 [00:11<00:31, 250.55it/s] 25%|██▍       | 2596/10573 [00:11<00:31, 252.83it/s] 25%|██▍       | 2622/10573 [00:11<00:35, 225.60it/s] 25%|██▌       | 2659/10573 [00:11<00:30, 257.94it/s] 25%|██▌       | 2686/10573 [00:12<00:31, 249.53it/s] 26%|██▌       | 2712/10573 [00:12<00:33, 232.88it/s] 26%|██▌       | 2736/10573 [00:12<00:34, 229.36it/s] 26%|██▌       | 2771/10573 [00:12<00:29, 260.30it/s] 26%|██▋       | 2800/10573 [00:12<00:30, 257.12it/s] 27%|██▋       | 2827/10573 [00:12<00:34, 221.73it/s] 27%|██▋       | 2877/10573 [00:12<00:27, 284.58it/s] 28%|██▊       | 2908/10573 [00:12<00:27, 282.64it/s] 28%|██▊       | 2938/10573 [00:13<00:31, 240.80it/s] 28%|██▊       | 2986/10573 [00:13<00:26, 289.85it/s] 29%|██▊       | 3017/10573 [00:13<00:25, 293.22it/s] 29%|██▉       | 3048/10573 [00:13<00:28, 268.50it/s] 29%|██▉       | 3077/10573 [00:13<00:28, 258.70it/s] 29%|██▉       | 3104/10573 [00:13<00:29, 256.07it/s] 30%|██▉       | 3131/10573 [00:13<00:29, 252.69it/s] 30%|██▉       | 3160/10573 [00:13<00:29, 254.51it/s] 30%|███       | 3186/10573 [00:13<00:29, 248.93it/s] 30%|███       | 3212/10573 [00:14<00:31, 230.56it/s] 31%|███       | 3247/10573 [00:14<00:28, 255.05it/s] 31%|███       | 3273/10573 [00:14<00:33, 219.69it/s] 31%|███▏      | 3322/10573 [00:14<00:26, 277.16it/s] 32%|███▏      | 3351/10573 [00:14<00:28, 254.21it/s] 32%|███▏      | 3388/10573 [00:14<00:25, 279.81it/s] 32%|███▏      | 3418/10573 [00:14<00:26, 266.39it/s] 33%|███▎      | 3446/10573 [00:14<00:27, 259.72it/s] 33%|███▎      | 3473/10573 [00:15<00:27, 254.12it/s] 33%|███▎      | 3501/10573 [00:15<00:27, 257.02it/s] 33%|███▎      | 3527/10573 [00:15<00:30, 227.46it/s] 34%|███▎      | 3551/10573 [00:15<00:30, 229.12it/s] 34%|███▍      | 3580/10573 [00:15<00:29, 240.36it/s] 34%|███▍      | 3608/10573 [00:15<00:27, 248.87it/s] 34%|███▍      | 3634/10573 [00:15<00:30, 226.26it/s] 35%|███▍      | 3664/10573 [00:15<00:30, 224.54it/s] 35%|███▌      | 3705/10573 [00:16<00:25, 264.48it/s] 35%|███▌      | 3733/10573 [00:16<00:32, 207.65it/s] 36%|███▌      | 3760/10573 [00:16<00:30, 220.16it/s] 36%|███▌      | 3784/10573 [00:16<00:30, 223.18it/s] 36%|███▌      | 3822/10573 [00:16<00:25, 262.74it/s] 36%|███▋      | 3850/10573 [00:16<00:26, 252.28it/s] 37%|███▋      | 3881/10573 [00:16<00:25, 264.17it/s] 37%|███▋      | 3909/10573 [00:16<00:27, 246.28it/s] 37%|███▋      | 3942/10573 [00:17<00:24, 265.59it/s] 38%|███▊      | 3970/10573 [00:17<00:25, 254.43it/s] 38%|███▊      | 4000/10573 [00:17<00:25, 262.90it/s] 38%|███▊      | 4027/10573 [00:17<00:25, 256.02it/s] 38%|███▊      | 4053/10573 [00:17<00:27, 235.26it/s] 39%|███▊      | 4089/10573 [00:17<00:24, 265.68it/s] 39%|███▉      | 4117/10573 [00:17<00:28, 230.51it/s] 39%|███▉      | 4154/10573 [00:17<00:24, 259.72it/s] 40%|███▉      | 4187/10573 [00:17<00:23, 276.45it/s] 40%|███▉      | 4216/10573 [00:18<00:23, 268.64it/s] 40%|████      | 4248/10573 [00:18<00:22, 276.59it/s] 40%|████      | 4277/10573 [00:18<00:25, 245.24it/s] 41%|████      | 4303/10573 [00:18<00:25, 247.00it/s] 41%|████      | 4335/10573 [00:18<00:24, 256.94it/s] 41%|████▏     | 4362/10573 [00:18<00:27, 222.88it/s] 42%|████▏     | 4413/10573 [00:18<00:21, 285.35it/s] 42%|████▏     | 4444/10573 [00:18<00:22, 268.92it/s] 42%|████▏     | 4472/10573 [00:19<00:26, 232.71it/s] 43%|████▎     | 4497/10573 [00:19<00:25, 236.65it/s] 43%|████▎     | 4525/10573 [00:19<00:24, 245.49it/s] 43%|████▎     | 4568/10573 [00:19<00:20, 293.90it/s] 43%|████▎     | 4599/10573 [00:19<00:20, 297.67it/s] 44%|████▍     | 4630/10573 [00:19<00:20, 286.21it/s] 44%|████▍     | 4660/10573 [00:19<00:20, 286.64it/s] 44%|████▍     | 4690/10573 [00:19<00:23, 247.45it/s] 45%|████▍     | 4716/10573 [00:20<00:23, 244.94it/s] 45%|████▍     | 4754/10573 [00:20<00:21, 274.68it/s] 45%|████▌     | 4783/10573 [00:20<00:21, 269.77it/s] 46%|████▌     | 4821/10573 [00:20<00:19, 297.17it/s] 46%|████▌     | 4852/10573 [00:20<00:21, 266.24it/s] 46%|████▌     | 4880/10573 [00:20<00:21, 262.51it/s] 46%|████▋     | 4907/10573 [00:20<00:22, 254.92it/s] 47%|████▋     | 4933/10573 [00:20<00:22, 255.11it/s] 47%|████▋     | 4959/10573 [00:20<00:22, 247.68it/s] 47%|████▋     | 4985/10573 [00:21<00:22, 247.34it/s] 47%|████▋     | 5010/10573 [00:21<00:26, 212.53it/s] 48%|████▊     | 5038/10573 [00:21<00:24, 228.98it/s] 48%|████▊     | 5074/10573 [00:21<00:21, 261.30it/s] 48%|████▊     | 5102/10573 [00:21<00:24, 226.71it/s] 49%|████▊     | 5132/10573 [00:21<00:22, 244.39it/s] 49%|████▉     | 5174/10573 [00:21<00:18, 290.46it/s] 49%|████▉     | 5205/10573 [00:21<00:18, 292.70it/s] 50%|████▉     | 5236/10573 [00:22<00:19, 269.67it/s] 50%|████▉     | 5265/10573 [00:22<00:19, 265.78it/s] 50%|█████     | 5297/10573 [00:22<00:19, 268.74it/s] 50%|█████     | 5325/10573 [00:22<00:20, 260.06it/s] 51%|█████     | 5353/10573 [00:22<00:19, 261.13it/s] 51%|█████     | 5380/10573 [00:22<00:19, 261.11it/s] 51%|█████     | 5413/10573 [00:22<00:18, 274.59it/s] 51%|█████▏    | 5441/10573 [00:22<00:19, 264.77it/s] 52%|█████▏    | 5468/10573 [00:22<00:20, 251.71it/s] 52%|█████▏    | 5506/10573 [00:23<00:18, 267.55it/s] 52%|█████▏    | 5533/10573 [00:23<00:19, 255.43it/s] 53%|█████▎    | 5559/10573 [00:23<00:20, 241.83it/s] 53%|█████▎    | 5600/10573 [00:23<00:17, 283.05it/s] 53%|█████▎    | 5629/10573 [00:23<00:18, 262.23it/s] 54%|█████▎    | 5663/10573 [00:23<00:17, 281.92it/s] 54%|█████▍    | 5708/10573 [00:23<00:15, 319.80it/s] 54%|█████▍    | 5741/10573 [00:23<00:16, 284.70it/s] 55%|█████▍    | 5771/10573 [00:23<00:17, 280.95it/s] 55%|█████▍    | 5803/10573 [00:24<00:16, 286.15it/s] 55%|█████▌    | 5833/10573 [00:24<00:18, 254.79it/s] 55%|█████▌    | 5860/10573 [00:24<00:19, 240.39it/s] 56%|█████▌    | 5889/10573 [00:24<00:19, 245.96it/s] 56%|█████▌    | 5915/10573 [00:24<00:22, 209.77it/s] 56%|█████▋    | 5964/10573 [00:24<00:17, 268.91it/s] 57%|█████▋    | 5993/10573 [00:24<00:17, 263.10it/s] 57%|█████▋    | 6021/10573 [00:25<00:19, 239.40it/s] 57%|█████▋    | 6053/10573 [00:25<00:17, 254.12it/s] 58%|█████▊    | 6080/10573 [00:25<00:19, 227.41it/s] 58%|█████▊    | 6117/10573 [00:25<00:17, 261.27it/s] 58%|█████▊    | 6145/10573 [00:25<00:17, 251.22it/s] 59%|█████▊    | 6186/10573 [00:25<00:15, 284.29it/s] 59%|█████▉    | 6219/10573 [00:25<00:15, 288.77it/s] 59%|█████▉    | 6249/10573 [00:25<00:16, 259.07it/s] 59%|█████▉    | 6276/10573 [00:25<00:16, 257.78it/s] 60%|█████▉    | 6303/10573 [00:26<00:17, 248.78it/s] 60%|█████▉    | 6329/10573 [00:26<00:17, 245.74it/s] 60%|██████    | 6358/10573 [00:26<00:16, 252.77it/s] 60%|██████    | 6385/10573 [00:26<00:16, 254.92it/s] 61%|██████    | 6419/10573 [00:26<00:15, 260.40it/s] 61%|██████    | 6446/10573 [00:26<00:16, 253.99it/s] 61%|██████▏   | 6481/10573 [00:26<00:14, 274.11it/s] 62%|██████▏   | 6509/10573 [00:26<00:15, 257.28it/s] 62%|██████▏   | 6535/10573 [00:27<00:16, 247.06it/s] 62%|██████▏   | 6572/10573 [00:27<00:14, 277.78it/s] 62%|██████▏   | 6601/10573 [00:27<00:15, 259.51it/s] 63%|██████▎   | 6628/10573 [00:27<00:15, 252.73it/s] 63%|██████▎   | 6654/10573 [00:27<00:18, 212.99it/s] 63%|██████▎   | 6683/10573 [00:27<00:16, 229.05it/s] 63%|██████▎   | 6707/10573 [00:27<00:17, 225.42it/s] 64%|██████▎   | 6738/10573 [00:27<00:15, 243.82it/s] 64%|██████▍   | 6764/10573 [00:27<00:16, 227.62it/s] 64%|██████▍   | 6788/10573 [00:28<00:17, 218.10it/s] 65%|██████▍   | 6836/10573 [00:28<00:13, 282.89it/s] 65%|██████▍   | 6866/10573 [00:28<00:13, 283.30it/s] 65%|██████▌   | 6896/10573 [00:28<00:15, 238.24it/s] 66%|██████▌   | 6929/10573 [00:28<00:14, 256.35it/s] 66%|██████▌   | 6957/10573 [00:28<00:14, 249.72it/s] 66%|██████▌   | 6992/10573 [00:28<00:13, 265.03it/s] 66%|██████▋   | 7021/10573 [00:28<00:13, 268.90it/s] 67%|██████▋   | 7049/10573 [00:29<00:13, 263.41it/s] 67%|██████▋   | 7078/10573 [00:29<00:13, 262.48it/s] 67%|██████▋   | 7105/10573 [00:29<00:13, 261.29it/s] 67%|██████▋   | 7132/10573 [00:29<00:14, 240.72it/s] 68%|██████▊   | 7169/10573 [00:29<00:12, 264.70it/s] 68%|██████▊   | 7196/10573 [00:29<00:14, 228.54it/s] 68%|██████▊   | 7226/10573 [00:29<00:13, 241.09it/s] 69%|██████▊   | 7251/10573 [00:29<00:14, 234.77it/s] 69%|██████▉   | 7276/10573 [00:30<00:14, 235.26it/s] 69%|██████▉   | 7301/10573 [00:30<00:13, 236.58it/s] 69%|██████▉   | 7333/10573 [00:30<00:12, 258.96it/s] 70%|██████▉   | 7360/10573 [00:30<00:12, 251.97it/s] 70%|██████▉   | 7386/10573 [00:30<00:12, 246.78it/s] 70%|███████   | 7411/10573 [00:30<00:13, 227.33it/s] 70%|███████   | 7444/10573 [00:30<00:12, 245.47it/s] 71%|███████   | 7469/10573 [00:30<00:13, 233.09it/s] 71%|███████   | 7520/10573 [00:30<00:09, 306.70it/s] 71%|███████▏  | 7552/10573 [00:31<00:10, 281.56it/s] 72%|███████▏  | 7582/10573 [00:31<00:12, 242.14it/s] 72%|███████▏  | 7608/10573 [00:31<00:12, 229.74it/s] 72%|███████▏  | 7649/10573 [00:31<00:10, 271.96it/s] 73%|███████▎  | 7678/10573 [00:31<00:12, 239.66it/s] 73%|███████▎  | 7710/10573 [00:31<00:11, 245.69it/s] 73%|███████▎  | 7736/10573 [00:31<00:11, 244.94it/s] 73%|███████▎  | 7762/10573 [00:31<00:12, 224.67it/s] 74%|███████▎  | 7786/10573 [00:32<00:12, 228.34it/s] 74%|███████▍  | 7818/10573 [00:32<00:11, 246.56it/s] 74%|███████▍  | 7844/10573 [00:32<00:12, 226.56it/s] 74%|███████▍  | 7868/10573 [00:32<00:11, 229.88it/s] 75%|███████▍  | 7892/10573 [00:32<00:11, 229.17it/s] 75%|███████▍  | 7926/10573 [00:32<00:10, 258.10it/s] 75%|███████▌  | 7953/10573 [00:32<00:10, 253.66it/s] 75%|███████▌  | 7979/10573 [00:32<00:10, 237.44it/s] 76%|███████▌  | 8006/10573 [00:32<00:10, 246.22it/s] 76%|███████▌  | 8032/10573 [00:33<00:10, 248.56it/s] 76%|███████▌  | 8061/10573 [00:33<00:09, 258.59it/s] 76%|███████▋  | 8088/10573 [00:33<00:09, 259.14it/s] 77%|███████▋  | 8120/10573 [00:33<00:09, 269.63it/s] 77%|███████▋  | 8150/10573 [00:33<00:08, 274.37it/s] 77%|███████▋  | 8178/10573 [00:33<00:09, 242.86it/s] 78%|███████▊  | 8203/10573 [00:33<00:09, 237.37it/s] 78%|███████▊  | 8228/10573 [00:33<00:10, 225.29it/s] 78%|███████▊  | 8257/10573 [00:33<00:09, 233.24it/s] 78%|███████▊  | 8291/10573 [00:34<00:08, 256.87it/s] 79%|███████▊  | 8322/10573 [00:34<00:08, 267.47it/s] 79%|███████▉  | 8350/10573 [00:34<00:08, 247.10it/s] 79%|███████▉  | 8378/10573 [00:34<00:08, 248.38it/s] 79%|███████▉  | 8404/10573 [00:34<00:09, 236.83it/s] 80%|███████▉  | 8428/10573 [00:34<00:09, 231.77it/s] 80%|███████▉  | 8452/10573 [00:34<00:09, 232.57it/s] 80%|████████  | 8483/10573 [00:34<00:08, 250.52it/s] 80%|████████  | 8511/10573 [00:35<00:08, 251.01it/s] 81%|████████  | 8539/10573 [00:35<00:07, 254.42it/s] 81%|████████  | 8568/10573 [00:35<00:07, 254.96it/s] 81%|████████▏ | 8597/10573 [00:35<00:07, 258.16it/s] 82%|████████▏ | 8623/10573 [00:35<00:07, 258.13it/s] 82%|████████▏ | 8649/10573 [00:35<00:07, 257.26it/s] 82%|████████▏ | 8675/10573 [00:35<00:07, 246.72it/s] 82%|████████▏ | 8700/10573 [00:35<00:07, 241.00it/s] 83%|████████▎ | 8725/10573 [00:35<00:08, 211.18it/s] 83%|████████▎ | 8776/10573 [00:36<00:06, 273.69it/s] 83%|████████▎ | 8804/10573 [00:36<00:06, 253.95it/s] 84%|████████▎ | 8832/10573 [00:36<00:06, 255.35it/s] 84%|████████▍ | 8858/10573 [00:36<00:06, 251.70it/s] 84%|████████▍ | 8884/10573 [00:36<00:07, 230.85it/s] 84%|████████▍ | 8930/10573 [00:36<00:05, 290.06it/s] 85%|████████▍ | 8961/10573 [00:36<00:05, 291.69it/s] 85%|████████▌ | 8992/10573 [00:36<00:05, 275.27it/s] 85%|████████▌ | 9021/10573 [00:37<00:06, 243.49it/s] 86%|████████▌ | 9050/10573 [00:37<00:06, 250.21it/s] 86%|████████▌ | 9076/10573 [00:37<00:06, 248.71it/s] 86%|████████▌ | 9102/10573 [00:37<00:05, 247.55it/s] 86%|████████▋ | 9128/10573 [00:37<00:06, 231.57it/s] 87%|████████▋ | 9152/10573 [00:37<00:06, 227.98it/s] 87%|████████▋ | 9190/10573 [00:37<00:05, 259.42it/s] 87%|████████▋ | 9221/10573 [00:37<00:05, 263.39it/s] 87%|████████▋ | 9248/10573 [00:37<00:05, 252.32it/s] 88%|████████▊ | 9274/10573 [00:38<00:05, 219.87it/s] 88%|████████▊ | 9313/10573 [00:38<00:04, 259.75it/s] 88%|████████▊ | 9342/10573 [00:38<00:04, 266.13it/s] 89%|████████▊ | 9370/10573 [00:38<00:04, 259.74it/s] 89%|████████▉ | 9406/10573 [00:38<00:04, 280.22it/s] 89%|████████▉ | 9435/10573 [00:38<00:04, 268.49it/s] 90%|████████▉ | 9470/10573 [00:38<00:03, 285.87it/s] 90%|████████▉ | 9499/10573 [00:38<00:04, 251.45it/s] 90%|█████████ | 9527/10573 [00:38<00:04, 254.27it/s] 90%|█████████ | 9554/10573 [00:39<00:04, 249.56it/s] 91%|█████████ | 9580/10573 [00:39<00:04, 214.59it/s] 91%|█████████ | 9611/10573 [00:39<00:04, 235.77it/s] 91%|█████████ | 9644/10573 [00:39<00:03, 252.84it/s] 91%|█████████▏| 9671/10573 [00:39<00:03, 250.30it/s] 92%|█████████▏| 9705/10573 [00:39<00:03, 263.61it/s] 92%|█████████▏| 9734/10573 [00:39<00:03, 266.52it/s] 92%|█████████▏| 9762/10573 [00:39<00:03, 263.57it/s] 93%|█████████▎| 9789/10573 [00:40<00:03, 221.68it/s] 93%|█████████▎| 9823/10573 [00:40<00:03, 242.83it/s] 93%|█████████▎| 9849/10573 [00:40<00:03, 228.33it/s] 93%|█████████▎| 9873/10573 [00:40<00:03, 228.60it/s] 94%|█████████▎| 9899/10573 [00:40<00:02, 231.94it/s] 94%|█████████▍| 9927/10573 [00:40<00:02, 244.74it/s] 94%|█████████▍| 9953/10573 [00:40<00:02, 247.89it/s] 94%|█████████▍| 9983/10573 [00:40<00:02, 261.88it/s] 95%|█████████▍| 10018/10573 [00:40<00:02, 276.09it/s] 95%|█████████▌| 10046/10573 [00:41<00:01, 268.37it/s] 95%|█████████▌| 10073/10573 [00:41<00:01, 257.69it/s] 96%|█████████▌| 10099/10573 [00:41<00:01, 237.82it/s] 96%|█████████▌| 10131/10573 [00:41<00:01, 259.79it/s] 96%|█████████▌| 10158/10573 [00:41<00:01, 251.58it/s] 96%|█████████▋| 10184/10573 [00:41<00:01, 233.15it/s] 97%|█████████▋| 10215/10573 [00:41<00:01, 236.19it/s] 97%|█████████▋| 10239/10573 [00:41<00:01, 233.42it/s] 97%|█████████▋| 10272/10573 [00:42<00:01, 244.90it/s] 97%|█████████▋| 10298/10573 [00:42<00:01, 243.20it/s] 98%|█████████▊| 10323/10573 [00:42<00:01, 237.51it/s] 98%|█████████▊| 10349/10573 [00:42<00:00, 238.22it/s] 98%|█████████▊| 10373/10573 [00:42<00:00, 212.52it/s] 98%|█████████▊| 10403/10573 [00:42<00:00, 234.17it/s] 99%|█████████▊| 10428/10573 [00:42<00:00, 227.91it/s] 99%|█████████▉| 10452/10573 [00:42<00:00, 230.50it/s] 99%|█████████▉| 10478/10573 [00:42<00:00, 230.67it/s] 99%|█████████▉| 10502/10573 [00:43<00:00, 217.57it/s]100%|█████████▉| 10536/10573 [00:43<00:00, 249.71it/s]100%|█████████▉| 10562/10573 [00:43<00:00, 213.80it/s]100%|██████████| 10573/10573 [00:43<00:00, 243.91it/s]
test process done: X = (10573, 3, 2, 2000), y = (10573,)
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/train.py", line 77, in <module>
    train_X, train_y = data_processor.load_data(os.path.join(in_path, f"{args.train_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_aug_train.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/data_analysis/spatial_analysis.py", line 80, in <module>
    model.load_state_dict(torch.load(os.path.join(ckp_path, f"{args.save_name}.pth"), map_location="cpu"))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/serialization.py", line 997, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/serialization.py", line 444, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/torch/serialization.py", line 425, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/CW/Holmes/max_f1.pth'
Valid: X=torch.Size([28548, 3, 2, 2000]), y=torch.Size([28548])
num_classes: 95
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p20.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p20.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p20.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p30.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p30.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p30.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p40.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p40.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p40.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p50.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p50.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p50.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p60.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p60.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p60.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p70.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p70.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p70.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p80.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p80.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p80.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p90.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p90.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p90.npz
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/dataset_process/gen_taf.py", line 44, in <module>
    data = np.load(os.path.join(in_path, f"{args.in_file}.npz"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/test_p100.npz'
/home/kka151/venvs/python_11_5/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/exp/test.py", line 76, in <module>
    test_X, test_y = data_processor.load_data(os.path.join(in_path, f"{args.test_file}.npz"), args.feature, args.seq_len, args.num_tabs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre06/project/6058534/kka151/Website-Fingerprinting-Library/WFlib/tools/data_processor.py", line 37, in load_data
    data = np.load(data_path)
           ^^^^^^^^^^^^^^^^^^
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2024a/lib/python3.11/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/home/kka151/scratch/holmes/datasets/CW/taf_test_p100.npz'
loading test file:  /home/kka151/scratch/holmes/datasets/CW/taf_test_p100.npz
slurmstepd: error: Detected 1 oom_kill event in StepId=36684738.batch. Some of the step tasks have been OOM Killed.
